{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "csc0yqd5MKNc",
        "VGMKJU6WMNbH",
        "shzLd24gOUiS",
        "WlfY1ZTFLRV2",
        "Xj3Vq-IRLmp-",
        "QTPDU2AVL2Rn",
        "fyjB9Ba1E3xn",
        "g79rHllDgQ8H",
        "Iu8kk_em8Xjj",
        "3SVz6RKLDD4i",
        "wUdp4giVEJ58",
        "VBRn-o2bbPjA",
        "lo78jMVwbZjb",
        "ABpXBPPPe6UR",
        "yBvKXyPpEWmw",
        "pBZpVrIfn1iF",
        "KyBR60XxLOcj"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "997d801c1a254c26a5e4414fbcf96083": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_945cf93a2b6a4c16aa13f066dcf75b6f",
              "IPY_MODEL_7d8dab323aec4247a9c19037afd6b7c5",
              "IPY_MODEL_8e67329302ac47969200c3fd286f8dc2"
            ],
            "layout": "IPY_MODEL_47b42ea311cc44f092e2ecb9332bd665"
          }
        },
        "945cf93a2b6a4c16aa13f066dcf75b6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae6c9c56ca59451e857cb5b9bfbd18f0",
            "placeholder": "​",
            "style": "IPY_MODEL_93e71fea81af46da8a1fea802fec4fd1",
            "value": "model.safetensors: 100%"
          }
        },
        "7d8dab323aec4247a9c19037afd6b7c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd31f0aac1bd409a88457c5767f85533",
            "max": 1115567652,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5cc31b585f49415096f0d3c8f6cc2a54",
            "value": 1115567652
          }
        },
        "8e67329302ac47969200c3fd286f8dc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ddbfc26e018477c8b07fce1bf6f7d7b",
            "placeholder": "​",
            "style": "IPY_MODEL_5cc74437d49740e298f791ab8165626d",
            "value": " 1.12G/1.12G [00:20&lt;00:00, 107MB/s]"
          }
        },
        "47b42ea311cc44f092e2ecb9332bd665": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae6c9c56ca59451e857cb5b9bfbd18f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93e71fea81af46da8a1fea802fec4fd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd31f0aac1bd409a88457c5767f85533": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cc31b585f49415096f0d3c8f6cc2a54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ddbfc26e018477c8b07fce1bf6f7d7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cc74437d49740e298f791ab8165626d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96c94bc1cd7645e083a2ecf23467b161": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9408941d92394a57abf558b70defa1e4",
              "IPY_MODEL_b949ba66b41e469bbe502c47b45917cb",
              "IPY_MODEL_cf9cb44b3e824fddafce14b3bf88fb65"
            ],
            "layout": "IPY_MODEL_e3bda7677e214dafa113d908b5e31af8"
          }
        },
        "9408941d92394a57abf558b70defa1e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9c8b75421d44f53a834bbe6dde6eaff",
            "placeholder": "​",
            "style": "IPY_MODEL_f40cab9b54674b52ba5510ac60c5c2b2",
            "value": "Downloading builder script: "
          }
        },
        "b949ba66b41e469bbe502c47b45917cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1c78b11455d4629a2f8b4feb158fc92",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f7515cc2fd2405eba41652008538b38",
            "value": 1
          }
        },
        "cf9cb44b3e824fddafce14b3bf88fb65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bb86119faa84c4bba8d585fba5d6361",
            "placeholder": "​",
            "style": "IPY_MODEL_afdbf51791c442248088aee77314cd4f",
            "value": " 6.34k/? [00:00&lt;00:00, 346kB/s]"
          }
        },
        "e3bda7677e214dafa113d908b5e31af8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9c8b75421d44f53a834bbe6dde6eaff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f40cab9b54674b52ba5510ac60c5c2b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1c78b11455d4629a2f8b4feb158fc92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7f7515cc2fd2405eba41652008538b38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1bb86119faa84c4bba8d585fba5d6361": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afdbf51791c442248088aee77314cd4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae022b14f41d47949de8a341ee8cc763": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbfef43be2b54c1cb17fc7d712b49139",
              "IPY_MODEL_37ef7fab221043428fdce9bdcdc8f06b",
              "IPY_MODEL_679b8f10389044e8b1e9589c64a65929"
            ],
            "layout": "IPY_MODEL_fe31e34b4907456db80e003e16a92034"
          }
        },
        "dbfef43be2b54c1cb17fc7d712b49139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17845c9a62a142c9ab71c852ade0bd9e",
            "placeholder": "​",
            "style": "IPY_MODEL_b7dd1ab5adf3402e96450f8ab154b4e5",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "37ef7fab221043428fdce9bdcdc8f06b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4eec372bb2e84a72af86169c041b0205",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d70bd28d08441ffa6669e8b3a096af3",
            "value": 25
          }
        },
        "679b8f10389044e8b1e9589c64a65929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d43fd759cc734a57af728b8be2d65044",
            "placeholder": "​",
            "style": "IPY_MODEL_daaecafe75c94d7bad0d590d4e7ed280",
            "value": " 25.0/25.0 [00:00&lt;00:00, 2.55kB/s]"
          }
        },
        "fe31e34b4907456db80e003e16a92034": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17845c9a62a142c9ab71c852ade0bd9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7dd1ab5adf3402e96450f8ab154b4e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4eec372bb2e84a72af86169c041b0205": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d70bd28d08441ffa6669e8b3a096af3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d43fd759cc734a57af728b8be2d65044": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daaecafe75c94d7bad0d590d4e7ed280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1a9d42bc9184adc9f825d27b6165635": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a9b3ce24f0084f8399a8be27e68f201c",
              "IPY_MODEL_00cd5736fee2469e8b9b629ce3ebb01c",
              "IPY_MODEL_944995c3da6e4d94b9964f29177309eb"
            ],
            "layout": "IPY_MODEL_c258f41b12484169bb9779ae47dd2ebd"
          }
        },
        "a9b3ce24f0084f8399a8be27e68f201c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cdbf6c97748449ea5c0d45e3131667f",
            "placeholder": "​",
            "style": "IPY_MODEL_4c47459e17f648a69a38998a3b7f2e87",
            "value": "config.json: 100%"
          }
        },
        "00cd5736fee2469e8b9b629ce3ebb01c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d6572a43d28457097c446cef4043727",
            "max": 615,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d9e38dea76949b688f2b89c7d4d9913",
            "value": 615
          }
        },
        "944995c3da6e4d94b9964f29177309eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_966149eec5074d45846838774df4b353",
            "placeholder": "​",
            "style": "IPY_MODEL_40888dbfc46043f7854b4c5b68695e38",
            "value": " 615/615 [00:00&lt;00:00, 75.0kB/s]"
          }
        },
        "c258f41b12484169bb9779ae47dd2ebd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cdbf6c97748449ea5c0d45e3131667f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c47459e17f648a69a38998a3b7f2e87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d6572a43d28457097c446cef4043727": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d9e38dea76949b688f2b89c7d4d9913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "966149eec5074d45846838774df4b353": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40888dbfc46043f7854b4c5b68695e38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5fa212761344a94949d335ae6330de1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d5865940b61448fbbb42ec504f202d0c",
              "IPY_MODEL_a2e639b925b8450a8ee68cf73dda26db",
              "IPY_MODEL_1be77d01a11a4a35b8675f16151d0826"
            ],
            "layout": "IPY_MODEL_c11ad014a37441d5a434873eb9f374e1"
          }
        },
        "d5865940b61448fbbb42ec504f202d0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b606ca89a814a97bb09cd92300bcc64",
            "placeholder": "​",
            "style": "IPY_MODEL_75024a0e6c28402d993bc9acc4ac824e",
            "value": "sentencepiece.bpe.model: 100%"
          }
        },
        "a2e639b925b8450a8ee68cf73dda26db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69ba41a11be8452b9668b55234774bc6",
            "max": 5069051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0272e9abc7df4f9aa19e2e284678fe3f",
            "value": 5069051
          }
        },
        "1be77d01a11a4a35b8675f16151d0826": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_465f8be96d74493baf10d9a32fa3b9a8",
            "placeholder": "​",
            "style": "IPY_MODEL_01f507a275e346a9b930c6cefdad3555",
            "value": " 5.07M/5.07M [00:00&lt;00:00, 20.5MB/s]"
          }
        },
        "c11ad014a37441d5a434873eb9f374e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b606ca89a814a97bb09cd92300bcc64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75024a0e6c28402d993bc9acc4ac824e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69ba41a11be8452b9668b55234774bc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0272e9abc7df4f9aa19e2e284678fe3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "465f8be96d74493baf10d9a32fa3b9a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01f507a275e346a9b930c6cefdad3555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "beb065e3b5e14e719d0070d184c2ba01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94d5083a1c7e4a049daee091c1761db0",
              "IPY_MODEL_8a9ab6a950174ffa89d07f1048cb6c42",
              "IPY_MODEL_b092c23e4262461282c5daa0f93099a2"
            ],
            "layout": "IPY_MODEL_f2e8dafdf5874c5e97f25dd0838c1ddd"
          }
        },
        "94d5083a1c7e4a049daee091c1761db0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e67e1d7f55c476b9657294e8574740b",
            "placeholder": "​",
            "style": "IPY_MODEL_aacaad60c8c84b4aaf36e1c6163f045d",
            "value": "tokenizer.json: 100%"
          }
        },
        "8a9ab6a950174ffa89d07f1048cb6c42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11c2eaa7aec146c7ba2663703f9dda1a",
            "max": 9096718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_311a34502e054cf284663cb96a61c1e5",
            "value": 9096718
          }
        },
        "b092c23e4262461282c5daa0f93099a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_914a49df50f24cddbef0060cb79821a6",
            "placeholder": "​",
            "style": "IPY_MODEL_63789d87c8914da8acf4a9b56d0e91a8",
            "value": " 9.10M/9.10M [00:00&lt;00:00, 43.9MB/s]"
          }
        },
        "f2e8dafdf5874c5e97f25dd0838c1ddd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e67e1d7f55c476b9657294e8574740b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aacaad60c8c84b4aaf36e1c6163f045d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11c2eaa7aec146c7ba2663703f9dda1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "311a34502e054cf284663cb96a61c1e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "914a49df50f24cddbef0060cb79821a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63789d87c8914da8acf4a9b56d0e91a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f06730fc86424629af153c693497c990": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f264177f388045bc83d275117fe73afb",
              "IPY_MODEL_9330428cb5bc441d81ca343ef5ce2527",
              "IPY_MODEL_a4e97d83a69d46bf881e0b06cb042f86"
            ],
            "layout": "IPY_MODEL_d7db71d9be3349ba83e87515412d70f8"
          }
        },
        "f264177f388045bc83d275117fe73afb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d5239557c954eb9bb4c76d8e9adef9c",
            "placeholder": "​",
            "style": "IPY_MODEL_60f4f54c932345bbaf727a3a2726a890",
            "value": "model.safetensors: 100%"
          }
        },
        "9330428cb5bc441d81ca343ef5ce2527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf88f12857d64968909dd93fb7861dee",
            "max": 1115567652,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1cc5c4fcc0234292a7e80a27b5c23a1e",
            "value": 1115567652
          }
        },
        "a4e97d83a69d46bf881e0b06cb042f86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fbe665b8f094e659affc626027a8d9e",
            "placeholder": "​",
            "style": "IPY_MODEL_e8d6af4d5be040e3afff2ace2219f6f3",
            "value": " 1.12G/1.12G [00:08&lt;00:00, 178MB/s]"
          }
        },
        "d7db71d9be3349ba83e87515412d70f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d5239557c954eb9bb4c76d8e9adef9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60f4f54c932345bbaf727a3a2726a890": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf88f12857d64968909dd93fb7861dee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cc5c4fcc0234292a7e80a27b5c23a1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1fbe665b8f094e659affc626027a8d9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8d6af4d5be040e3afff2ace2219f6f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "507d41f535aa4d2fb1639c76bea3d83b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_163538dbd0d94a9baed654b70850292b",
              "IPY_MODEL_708667775997411bbe80b9bdafee5795",
              "IPY_MODEL_51a67709a78d49f397fd1b9398f43770"
            ],
            "layout": "IPY_MODEL_f2c6f30ced504786af185540a59ee7e3"
          }
        },
        "163538dbd0d94a9baed654b70850292b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb0d18fa410944bbb3432d5bee8d03ed",
            "placeholder": "​",
            "style": "IPY_MODEL_ab6efd65657b4585b16545bd7531f3f5",
            "value": "Downloading builder script: "
          }
        },
        "708667775997411bbe80b9bdafee5795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8249abc6a6654f4b9f6dcd20ae3b0a62",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5a4c111a95046e3a6af673329f5d338",
            "value": 1
          }
        },
        "51a67709a78d49f397fd1b9398f43770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c5779c75ed84616b290834cb59a401d",
            "placeholder": "​",
            "style": "IPY_MODEL_40f275fc5b214abbb8ee158f025cdac8",
            "value": " 6.34k/? [00:00&lt;00:00, 476kB/s]"
          }
        },
        "f2c6f30ced504786af185540a59ee7e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb0d18fa410944bbb3432d5bee8d03ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab6efd65657b4585b16545bd7531f3f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8249abc6a6654f4b9f6dcd20ae3b0a62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a5a4c111a95046e3a6af673329f5d338": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c5779c75ed84616b290834cb59a401d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40f275fc5b214abbb8ee158f025cdac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/417godmin/NER-/blob/main/NER%E6%97%B6%E9%97%B4%E5%9C%B0%E5%9D%80%E8%AF%86%E5%88%AB%E5%8A%A8%E6%80%81%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%B5%8B%E8%AF%95_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **谷歌云盘**\n",
        "挂载谷歌云盘，便于通过代码保存和加载模型文件"
      ],
      "metadata": {
        "id": "xwPrQBXzWXB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aSPzNQxHw29l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d8e1c5c-aefe-4f10-d4fb-d993cf3f420a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 构造数据集\n",
        "\n",
        "完善中......\n"
      ],
      "metadata": {
        "id": "scBISv0OEjAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 调用大模型接口生成自然语言模板\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bfexlBkGG0CM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 说明"
      ],
      "metadata": {
        "id": "csc0yqd5MKNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "每次运行为文件新增数据而非重写(已测试)\n",
        "\n",
        "输入输出说明：\n",
        "\n",
        "*  不需要输入\n",
        "\n",
        "*  输出：/content/drive/MyDrive/NER/address_templates/address_templates_{地址数*（1/2/3/4/5）}/{语言名（中文）}_address_templates.jsonl\n",
        "\n",
        "*  输出结构\n",
        "```\n",
        "/content\n",
        "└── drive\n",
        "    └── MyDrive\n",
        "        └── NER\n",
        "            ├── address_templates_0(整句纯负例)\n",
        "                ├── 简体中文_address_templates.jsonl\n",
        "                ├── 繁体中文_address_templates.jsonl\n",
        "                 ......\n",
        "                └—— 韩语_address_templates.jsonl\n",
        "            ├── address_templates_1\n",
        "                ├── 简体中文_address_templates.jsonl\n",
        "                ├── 繁体中文_address_templates.jsonl\n",
        "                 ......\n",
        "                └—— 韩语_address_templates.jsonl\n",
        "            ├── address_templates_2          \n",
        "            ├── address_templates_3\n",
        "            ├── address_templates_4\n",
        "            └── address_templates_5\n",
        "```"
      ],
      "metadata": {
        "id": "jjqJBdLTL_wB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 生成训练模版"
      ],
      "metadata": {
        "id": "VGMKJU6WMNbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "纯地址模版生成器\n"
      ],
      "metadata": {
        "id": "YQ9iNdykK6xL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {\"form-width\":\"35%\"}\n",
        "llm_api_key = \"\"\n",
        "\n",
        "# 依赖：pip install httpx\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import asyncio\n",
        "import platform\n",
        "from typing import List, Dict, Any, Optional\n",
        "import httpx\n",
        "from google.colab import drive\n",
        "\n",
        "# ======================================================可按需修改的配置 ======================================================\n",
        "\n",
        "# 生成目标\n",
        "ADDRESS_SLOT_LIST   = [0, 1, 2, 3, 4, 5]     # 需要生成的“地址占位符个数”集合（最多到 5）\n",
        "TARGET_PER_SLOT     = [1000, 2000, 2000, 1500, 500, 100]  # 与上面一一对应，每种占位数下，每语言目标条数\n",
        "#TARGET_PER_SLOT     = [2,2,2,2,2,2]  # 测试\n",
        "\n",
        "\n",
        "api_key = \"sk-013SeYkFFCjw1G3IIwDWfj2iXFqWhUSF6fmC0L7KdJjE\"  # @param {\"type\":\"string\",\"placeholder\":\"请输入大模型api key\"}\n",
        "\n",
        "TEMPLATE_BATCH_SIZE = 40            # @param {\"type\":\"integer\",\"placeholder\":\"一次让大模型产出n条\"}（建议50以下，过大会增大 JSON 不规范风险）\n",
        "TEMPERATURE = 0.8               # @param {\"type\":\"number\",\"placeholder\":\"模型温度\"} 越高输出越多样，越低越遵循指令\n",
        "MODEL_NAME = \"turing/gpt-4o\"         # @param {\"type\":\"number\",\"placeholder\":\"模型名\"}（如需更换请保持模型名和服务器平台上的一致）\n",
        "\n",
        "# =================================================================================================================================\n",
        "\n",
        "\n",
        "# 确保挂载google drive\n",
        "try:\n",
        "    import google.colab\n",
        "    from google.colab import drive  # 在 Colab 环境下可用\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"云盘挂载成功\")\n",
        "except Exception:\n",
        "    print(\"云盘挂载失败\")\n",
        "\n",
        "\n",
        "# 保存到drive\n",
        "OUTPUT_BASE_DIR = \"/content/drive/MyDrive/NER/address_templates\"\n",
        "SLOT2TARGET = dict(zip(ADDRESS_SLOT_LIST, TARGET_PER_SLOT))\n",
        "\n",
        "\n",
        "# ================== 接口配置 ==================\n",
        "BASE_URL = os.getenv(\"LLM_BASE_URL\", \"https://test-turing.cn.llm.tcljd.com\")\n",
        "ENDPOINT = os.getenv(\"LLM_ENDPOINT\", \"/api/v1/chat/completions\")\n",
        "API_KEY  = os.getenv(\"LLM_API_KEY\", api_key)\n",
        "\n",
        "HEADERS = {\n",
        "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}\n",
        "\n",
        "# ================== 生成配置 ==================\n",
        "LANGUAGES = [\n",
        "    \"简体中文\", \"繁体中文\", \"英语\", \"丹麦语\", \"俄语\", \"土耳其语\", \"德语\",\n",
        "    \"意大利语\", \"日语\", \"法语\", \"瑞典语\", \"荷兰语\", \"葡萄牙语\",\n",
        "    \"西班牙语\", \"韩语\"\n",
        "]\n",
        "\n",
        "\n",
        "TIMEOUT = 90                    # 单次请求超时\n",
        "RETRY_TIMES = 2                   # 每批重试次数\n",
        "\n",
        "#  HTTP 客户端（全局）\n",
        "client: Optional[httpx.AsyncClient] = None\n",
        "\n",
        "\n",
        "# 根据地址数量选择prompt中的示例\n",
        "prompt_example1 = [\n",
        "    '{\"template\": \"这里是一条模板文本，包含{address}。\"}',\n",
        "    '{\"template\": \"这里是一条模板文本，包含{address}和{address}。\"}',\n",
        "    '{\"template\": \"这里是一条模板文本，包含{address}和{address}，还包含{address}。\"}',\n",
        "    '{\"template\": \"这里是一条模板文本，包含{address}和{address}。以及{address}，还有{address}\"}',\n",
        "    '{\"template\": \"这里是一条模板文本，包含{address}和{address}以及{address}。还有{address}和{address}\"}',\n",
        "]\n",
        "\n",
        "prompt_example2 = [\n",
        "      \"\"\"\n",
        "      [\n",
        "        {{\"template\": \"请到{address}办理入场登记。\"}},\n",
        "        {{\"template\": \"Meet me at {address}, don't be late.\"}}\n",
        "      ]\n",
        "      \"\"\",\n",
        "      \"\"\"\n",
        "      [\n",
        "        {{\"template\": \"请到{address}，再到{address}领取胸卡。\"}},\n",
        "        {{\"template\": \"Meet me at {address}, don't be late. We can have lunch at {address} in the afternoon.\"}}\n",
        "      ]\n",
        "      \"\"\",\n",
        "      \"\"\"\n",
        "      [\n",
        "        {{\"template\": \"{address}或{address}办理入场登记，再到{address}领取胸卡。\"}},\n",
        "        {{\"template\": \"Meet me at {address}, don't be late. We can have lunch at {address} or {address}.\"}}\n",
        "      ]\n",
        "      \"\"\",\n",
        "      \"\"\"\n",
        "      [\n",
        "        {{\"template\": \"请到{address}办理入场登记。再到{address}领取胸卡，在{address}办公。如果还有问题可以来{address}找我。\"}},\n",
        "        {{\"template\": \"Meet me at {address}, don't be late. We can have lunch at {address} or {address} in the afternoon. In the evening, we might visit {address} for some fun.\"}}\n",
        "      ]\n",
        "      \"\"\",\n",
        "      \"\"\"\n",
        "      [\n",
        "        {{\"template\": \"请到{address}或{address}办理入场登记。再到{address}领取胸卡，在{address}办公。如果还有问题可以来{address}找我。\"}},\n",
        "        {{\"template\": \"Meet me at {address}, don't be late. We can have lunch at {address} or {address} in the afternoon. In the evening, we might visit {{address}} or {{address}}for some fun.\"}}\n",
        "      ]\n",
        "      \"\"\",\n",
        "\n",
        "]\n",
        "\n",
        "# ================== 工具函数 ==================\n",
        "def build_template_prompt(language: str, n: int, n_address: int) -> str:\n",
        "    if n_address == 0:\n",
        "      return f\"\"\"\n",
        "          你是一个不包含地址信息的剪贴板文本生成器。请用 **{language}** 语言生成 {n} 条“不包含地址的剪贴板文本”：\n",
        "              - 模板里严禁出现任何地址\n",
        "              - 风格多样：口语、正式通知、聊天记录、评论、邮件、日程卡片、社交帖子等，长度 1–3 句\n",
        "              - 必须输出“纯 JSON 数组”，不要代码块、不要多余说明\n",
        "              - JSON 文本中不要包含换行符 \\\\n\n",
        "\n",
        "              示例（注意：这是示意，不要照抄，也不要输出示例之外任何文字）：\n",
        "              [\n",
        "                {{\"template\": \"下午请找我办理入场登记。\"}},\n",
        "                {{\"template\": \"Let's play games together tomorrow morning.\"}}\n",
        "              ]\n",
        "\n",
        "              现在请直接输出 **{n} 条**该语言的剪贴板文本 JSON 数组。\"\"\"\n",
        "\n",
        "    else:\n",
        "      return f\"\"\"你是一个模板生成器。请用 **{language}** 语言生成 {n} 条“上下文模板”，\n",
        "              每条模板必须包含{n_address}个占位符：\n",
        "              - 占位符：{{address}}（必须原样出现，不加引号）\n",
        "              - 模板里严禁出现任何地址（只能用占位符）\n",
        "              - 风格多样：口语、正式通知、聊天记录、评论、邮件、日程卡片、社交帖子等，长度 1–3 句\n",
        "              - 必须输出“纯 JSON 数组”，不要代码块、不要多余说明，数组元素为：\n",
        "                {prompt_example1[n_address-1]}\n",
        "              - JSON 文本中不要包含换行符 \\\\n\n",
        "\n",
        "              示例（注意：这是示意，不要照抄，也不要输出示例之外任何文字）：\n",
        "              {prompt_example2[n_address-1]}\n",
        "              现在请直接输出 **{n} 条**该语言的模板 JSON 数组。\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _strip_code_fence(s: str) -> str:\n",
        "    \"\"\"去除可能的Markdown代码块围栏，并截取第一个看起来像JSON数组的片段\"\"\"\n",
        "    txt = s.strip()\n",
        "    # 如果有 ``` 包裹，尝试找到第一个 [ 开始位置\n",
        "    if txt.startswith(\"```\"):\n",
        "        i = txt.find(\"[\")\n",
        "        if i == -1:\n",
        "            i = txt.find(\"{\")\n",
        "        if i != -1:\n",
        "            txt = txt[i:]\n",
        "        if txt.endswith(\"```\"):\n",
        "            txt = txt[:-3].strip()\n",
        "    return txt\n",
        "\n",
        "def _parse_json_array(text: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"尽量宽松地把返回内容解析为 JSON 数组\"\"\"\n",
        "    txt = _strip_code_fence(text)\n",
        "\n",
        "    # 先尝试直接解析\n",
        "    try:\n",
        "        data = json.loads(txt)\n",
        "        if isinstance(data, list):\n",
        "            return data\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 再尝试正则抽取首个JSON数组\n",
        "    m = re.search(r\"\\[[\\s\\S]*\\]\", txt)\n",
        "    if m:\n",
        "        arr = m.group(0)\n",
        "        return json.loads(arr)\n",
        "\n",
        "    raise ValueError(f\"无法解析为 JSON 数组，片段：{txt[:200]}...\")\n",
        "\n",
        "# 过滤生成的模板\n",
        "def _post_process_templates(json_list: List[Dict[str, Any]], need: int, n_address: int) -> List[str]:\n",
        "    out, seen = [], set()\n",
        "    for item in json_list:\n",
        "        if not isinstance(item, dict):\n",
        "            continue\n",
        "        t = item.get(\"template\")\n",
        "        if not isinstance(t, str):\n",
        "            continue\n",
        "        t = \" \".join(t.strip().split())\n",
        "        if not t or t in seen:\n",
        "            continue\n",
        "\n",
        "        if n_address == 0:\n",
        "            if \"{address}\" in t:\n",
        "                continue\n",
        "        else:\n",
        "            # 占位符数量必须符合要求\n",
        "            if t.count(\"{address}\") != n_address:\n",
        "                continue\n",
        "\n",
        "        out.append(t)\n",
        "        seen.add(t)\n",
        "\n",
        "    # 返回\n",
        "    return out[:need]\n",
        "\n",
        "\n",
        "\n",
        "async def chat_stream(payload: dict, *, collect: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    流式接口封装：\n",
        "    - collect=True：收集完整响应文本并返回（适合后续 JSON 解析）\n",
        "    \"\"\"\n",
        "    assert client is not None, \"client 尚未初始化\"\n",
        "    text_buf: List[str] = []\n",
        "    async with client.stream(\"POST\", ENDPOINT, headers=HEADERS, content=json.dumps(payload)) as resp:\n",
        "        async for line in resp.aiter_lines():\n",
        "            if not line:\n",
        "                continue\n",
        "            line = line.strip()\n",
        "            # 你的服务返回格式若与 OpenAI SSE 一致，形如：data: {...}\n",
        "            if not line.startswith(\"data:\"):\n",
        "                continue\n",
        "            data = line[5:].strip()\n",
        "            if data == \"[DONE]\":\n",
        "                break\n",
        "            try:\n",
        "                delta = json.loads(data)[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n",
        "            except Exception:\n",
        "                continue\n",
        "            if collect:\n",
        "                text_buf.append(delta)\n",
        "    return \"\".join(text_buf)\n",
        "\n",
        "async def generate_templates_batch(language: str, n: int, n_address: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    生成该语言的 n 条模板；若 LLM 输出异常，抛出异常由上层处理重试。\n",
        "    \"\"\"\n",
        "    prompt = build_template_prompt(language, n, n_address)\n",
        "    payload = {\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"stream\": True,\n",
        "        \"temperature\": TEMPERATURE,\n",
        "    }\n",
        "    raw = await chat_stream(payload, collect=True)\n",
        "    data = _parse_json_array(raw)\n",
        "    return _post_process_templates(data, n, n_address)\n",
        "\n",
        "\n",
        "async def generate_templates_for_language(language: str, slot: int) -> str:\n",
        "    \"\"\"\n",
        "    为不同地址数模板生成子目录\n",
        "    为某个 language 生成 TEMPLATES_PER_LANG 条模板，\n",
        "    按 TEMPLATE_BATCH_SIZE 分批请求，并输出到 templates_both/<language>_template.jsonl\n",
        "    \"\"\"\n",
        "    target_per_lang = SLOT2TARGET.get(slot, 0)\n",
        "    if target_per_lang <= 0:\n",
        "        return \"\"\n",
        "\n",
        "    out_dir = os.path.join(OUTPUT_BASE_DIR, f\"address_templates_{slot}\")\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    out_path = os.path.join(out_dir, f\"{language}_address_templates.jsonl\")\n",
        "\n",
        "\n",
        "    # 统计现有条数，续写\n",
        "    existing = 0\n",
        "    if os.path.exists(out_path):\n",
        "        with open(out_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            existing = sum(1 for _ in f)\n",
        "\n",
        "\n",
        "    target = target_per_lang\n",
        "    if existing >= target:\n",
        "        print(f\"[{language}] 已有 {existing} 条，已达到目标 {target}，跳过。→ {out_path}\")\n",
        "        return out_path\n",
        "\n",
        "\n",
        "    bs = TEMPLATE_BATCH_SIZE\n",
        "    total = existing\n",
        "    print(f\"[{language}] 目标 {target} 条，已存在 {existing} 条，需新增 {target - total} 条；batch={bs} → {out_path}\")\n",
        "\n",
        "    ROUND_CAP = 50          # 安全上限：最多打 50 轮\n",
        "    EMPTY_STREAK_CAP = 3    # 连续空批最多 3 次就停，提示检查 prompt\n",
        "    rounds_done = 0\n",
        "    empty_streak = 0\n",
        "\n",
        "    while total < target and rounds_done < ROUND_CAP:\n",
        "        ask_n = min(bs, target - total)   # 也可以固定用 bs，看你偏好\n",
        "        last_err = None\n",
        "\n",
        "        for attempt in range(1, RETRY_TIMES + 2):\n",
        "            try:\n",
        "                batch = await generate_templates_batch(language, ask_n, slot)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                last_err = e\n",
        "                print(f\"[{language}] 第 {rounds_done+1} 轮，第 {attempt} 次尝试失败：{e}\")\n",
        "                await asyncio.sleep(1.2 * attempt)\n",
        "        else:\n",
        "            raise RuntimeError(f\"[{language}] 连续失败：{last_err}\")\n",
        "\n",
        "        got = len(batch)\n",
        "        if got == 0:\n",
        "            empty_streak += 1\n",
        "            print(f\"[{language}] 空批（第 {empty_streak} 次），总进度仍为 {total}/{target}\")\n",
        "            if empty_streak >= EMPTY_STREAK_CAP:\n",
        "                print(f\"[{language}] 连续空批达到 {EMPTY_STREAK_CAP}，提前停止，请检查提示词或模型输出。\")\n",
        "                break\n",
        "        else:\n",
        "            empty_streak = 0\n",
        "            with open(out_path, \"a\", encoding=\"utf-8\") as f:\n",
        "                for t in batch:\n",
        "                    rec = {\"language\": language, \"template\": t}\n",
        "                    f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "            total += got\n",
        "            print(f\"[{language}] 进度：{total}/{target} (+{got})\")\n",
        "\n",
        "        rounds_done += 1\n",
        "\n",
        "    if total < target:\n",
        "        print(f\"[{language}] 未达标：{total}/{target}。可提高 TEMPLATE_BATCH_SIZE 或改写提示词。\")\n",
        "    else:\n",
        "        print(f\"[{language}] 完成，合计 {total} 条 → {out_path}\")\n",
        "\n",
        "    return out_path\n",
        "\n",
        "async def generate_templates_for_all_languages():\n",
        "    \"\"\"\n",
        "    依次为不同地址数的每个 LANGUAGES 生成模板文件。\n",
        "    \"\"\"\n",
        "    for slot in ADDRESS_SLOT_LIST:\n",
        "        target = SLOT2TARGET.get(slot, 0)\n",
        "        if target <= 0:\n",
        "            print(f\"[addr={slot}] 目标条数为 0，跳过。\")\n",
        "            continue\n",
        "        print(\"=\" * 12, f\"开始：address数量： {slot}（每语言目标 {target} 条）\", \"=\" * 12)\n",
        "        for lang in LANGUAGES:\n",
        "            await generate_templates_for_language(lang, slot)\n",
        "\n",
        "\n",
        "\n",
        "# ================== 主程序 ==================\n",
        "async def main():\n",
        "    global client\n",
        "    client = httpx.AsyncClient(base_url=BASE_URL, timeout=TIMEOUT)\n",
        "    try:\n",
        "        print(f\"已连接到模型服务：{BASE_URL}{ENDPOINT} | model={MODEL_NAME}\")\n",
        "        print(f\"输出根目录：{OUTPUT_BASE_DIR}\")\n",
        "        await generate_templates_for_all_languages()\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"全部模板生成完成！目录按地址占位数区分：address_templates_<slot>\")\n",
        "        print(\"文件命名：<语言>_address_templates.jsonl（每行一条 JSON）\")\n",
        "        print(\"=\" * 50)\n",
        "    finally:\n",
        "        await client.aclose()\n",
        "\n",
        "\n",
        "await main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nelwYmmMEih9",
        "outputId": "62b868fe-8342-4250-a9da-3399f5e04b07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "云盘挂载成功\n",
            "已连接到模型服务：https://test-turing.cn.llm.tcljd.com/api/v1/chat/completions | model=turing/gpt-4o\n",
            "输出根目录：/content/drive/MyDrive/NER/address_templates\n",
            "============ 开始：address数量： 0（每语言目标 1000 条） ============\n",
            "[简体中文] 目标 1000 条，已存在 100 条，需新增 900 条；batch=40 → /content/drive/MyDrive/NER/address_templates/address_templates_0/简体中文_address_templates.jsonl\n",
            "[简体中文] 进度：140/1000 (+40)\n",
            "[简体中文] 进度：180/1000 (+40)\n",
            "[简体中文] 进度：220/1000 (+40)\n",
            "[简体中文] 进度：259/1000 (+39)\n",
            "[简体中文] 进度：299/1000 (+40)\n",
            "[简体中文] 进度：339/1000 (+40)\n",
            "[简体中文] 进度：379/1000 (+40)\n",
            "[简体中文] 进度：419/1000 (+40)\n",
            "[简体中文] 进度：459/1000 (+40)\n",
            "[简体中文] 进度：499/1000 (+40)\n",
            "[简体中文] 进度：539/1000 (+40)\n",
            "[简体中文] 进度：579/1000 (+40)\n",
            "[简体中文] 进度：619/1000 (+40)\n",
            "[简体中文] 进度：659/1000 (+40)\n",
            "[简体中文] 进度：699/1000 (+40)\n",
            "[简体中文] 进度：739/1000 (+40)\n",
            "[简体中文] 进度：779/1000 (+40)\n",
            "[简体中文] 进度：819/1000 (+40)\n",
            "[简体中文] 进度：859/1000 (+40)\n",
            "[简体中文] 进度：899/1000 (+40)\n",
            "[简体中文] 进度：939/1000 (+40)\n",
            "[简体中文] 进度：979/1000 (+40)\n",
            "[简体中文] 进度：1000/1000 (+21)\n",
            "[简体中文] 完成，合计 1000 条 → /content/drive/MyDrive/NER/address_templates/address_templates_0/简体中文_address_templates.jsonl\n",
            "[繁体中文] 目标 1000 条，已存在 100 条，需新增 900 条；batch=40 → /content/drive/MyDrive/NER/address_templates/address_templates_0/繁体中文_address_templates.jsonl\n",
            "[繁体中文] 进度：140/1000 (+40)\n",
            "[繁体中文] 进度：180/1000 (+40)\n",
            "[繁体中文] 进度：220/1000 (+40)\n",
            "[繁体中文] 进度：260/1000 (+40)\n",
            "[繁体中文] 进度：300/1000 (+40)\n",
            "[繁体中文] 进度：340/1000 (+40)\n",
            "[繁体中文] 进度：380/1000 (+40)\n",
            "[繁体中文] 进度：420/1000 (+40)\n",
            "[繁体中文] 进度：460/1000 (+40)\n",
            "[繁体中文] 进度：500/1000 (+40)\n",
            "[繁体中文] 进度：540/1000 (+40)\n",
            "[繁体中文] 进度：580/1000 (+40)\n",
            "[繁体中文] 进度：620/1000 (+40)\n",
            "[繁体中文] 进度：660/1000 (+40)\n",
            "[繁体中文] 进度：700/1000 (+40)\n",
            "[繁体中文] 进度：740/1000 (+40)\n",
            "[繁体中文] 进度：780/1000 (+40)\n",
            "[繁体中文] 进度：820/1000 (+40)\n",
            "[繁体中文] 进度：860/1000 (+40)\n",
            "[繁体中文] 进度：900/1000 (+40)\n",
            "[繁体中文] 进度：940/1000 (+40)\n",
            "[繁体中文] 进度：979/1000 (+39)\n",
            "[繁体中文] 进度：1000/1000 (+21)\n",
            "[繁体中文] 完成，合计 1000 条 → /content/drive/MyDrive/NER/address_templates/address_templates_0/繁体中文_address_templates.jsonl\n",
            "[英语] 目标 1000 条，已存在 100 条，需新增 900 条；batch=40 → /content/drive/MyDrive/NER/address_templates/address_templates_0/英语_address_templates.jsonl\n",
            "[英语] 进度：140/1000 (+40)\n",
            "[英语] 进度：180/1000 (+40)\n",
            "[英语] 进度：220/1000 (+40)\n",
            "[英语] 进度：260/1000 (+40)\n",
            "[英语] 进度：300/1000 (+40)\n",
            "[英语] 进度：340/1000 (+40)\n",
            "[英语] 进度：380/1000 (+40)\n",
            "[英语] 进度：420/1000 (+40)\n",
            "[英语] 进度：460/1000 (+40)\n",
            "[英语] 进度：500/1000 (+40)\n",
            "[英语] 进度：540/1000 (+40)\n",
            "[英语] 进度：580/1000 (+40)\n",
            "[英语] 进度：620/1000 (+40)\n",
            "[英语] 进度：660/1000 (+40)\n",
            "[英语] 进度：700/1000 (+40)\n",
            "[英语] 进度：740/1000 (+40)\n",
            "[英语] 进度：780/1000 (+40)\n",
            "[英语] 进度：820/1000 (+40)\n",
            "[英语] 进度：860/1000 (+40)\n",
            "[英语] 进度：900/1000 (+40)\n",
            "[英语] 进度：940/1000 (+40)\n",
            "[英语] 进度：980/1000 (+40)\n",
            "[英语] 进度：1000/1000 (+20)\n",
            "[英语] 完成，合计 1000 条 → /content/drive/MyDrive/NER/address_templates/address_templates_0/英语_address_templates.jsonl\n",
            "[丹麦语] 目标 1000 条，已存在 100 条，需新增 900 条；batch=40 → /content/drive/MyDrive/NER/address_templates/address_templates_0/丹麦语_address_templates.jsonl\n",
            "[丹麦语] 进度：140/1000 (+40)\n",
            "[丹麦语] 进度：180/1000 (+40)\n",
            "[丹麦语] 进度：220/1000 (+40)\n",
            "[丹麦语] 进度：260/1000 (+40)\n",
            "[丹麦语] 进度：300/1000 (+40)\n",
            "[丹麦语] 进度：340/1000 (+40)\n",
            "[丹麦语] 进度：380/1000 (+40)\n",
            "[丹麦语] 进度：420/1000 (+40)\n",
            "[丹麦语] 进度：460/1000 (+40)\n",
            "[丹麦语] 进度：499/1000 (+39)\n",
            "[丹麦语] 进度：539/1000 (+40)\n",
            "[丹麦语] 进度：579/1000 (+40)\n",
            "[丹麦语] 进度：619/1000 (+40)\n",
            "[丹麦语] 进度：659/1000 (+40)\n",
            "[丹麦语] 进度：699/1000 (+40)\n",
            "[丹麦语] 进度：739/1000 (+40)\n",
            "[丹麦语] 进度：779/1000 (+40)\n",
            "[丹麦语] 进度：819/1000 (+40)\n",
            "[丹麦语] 进度：859/1000 (+40)\n",
            "[丹麦语] 进度：899/1000 (+40)\n",
            "[丹麦语] 进度：939/1000 (+40)\n",
            "[丹麦语] 进度：979/1000 (+40)\n",
            "[丹麦语] 进度：1000/1000 (+21)\n",
            "[丹麦语] 完成，合计 1000 条 → /content/drive/MyDrive/NER/address_templates/address_templates_0/丹麦语_address_templates.jsonl\n",
            "[俄语] 目标 1000 条，已存在 100 条，需新增 900 条；batch=40 → /content/drive/MyDrive/NER/address_templates/address_templates_0/俄语_address_templates.jsonl\n",
            "[俄语] 进度：140/1000 (+40)\n",
            "[俄语] 进度：180/1000 (+40)\n",
            "[俄语] 进度：220/1000 (+40)\n",
            "[俄语] 进度：260/1000 (+40)\n",
            "[俄语] 进度：300/1000 (+40)\n",
            "[俄语] 进度：340/1000 (+40)\n",
            "[俄语] 进度：380/1000 (+40)\n",
            "[俄语] 进度：420/1000 (+40)\n",
            "[俄语] 进度：460/1000 (+40)\n",
            "[俄语] 进度：500/1000 (+40)\n",
            "[俄语] 进度：540/1000 (+40)\n",
            "[俄语] 进度：580/1000 (+40)\n",
            "[俄语] 进度：620/1000 (+40)\n",
            "[俄语] 进度：660/1000 (+40)\n",
            "[俄语] 进度：700/1000 (+40)\n",
            "[俄语] 进度：740/1000 (+40)\n",
            "[俄语] 进度：780/1000 (+40)\n",
            "[俄语] 进度：820/1000 (+40)\n",
            "[俄语] 进度：860/1000 (+40)\n",
            "[俄语] 进度：900/1000 (+40)\n",
            "[俄语] 进度：940/1000 (+40)\n",
            "[俄语] 进度：980/1000 (+40)\n",
            "[俄语] 进度：1000/1000 (+20)\n",
            "[俄语] 完成，合计 1000 条 → /content/drive/MyDrive/NER/address_templates/address_templates_0/俄语_address_templates.jsonl\n",
            "[土耳其语] 目标 1000 条，已存在 100 条，需新增 900 条；batch=40 → /content/drive/MyDrive/NER/address_templates/address_templates_0/土耳其语_address_templates.jsonl\n",
            "[土耳其语] 进度：140/1000 (+40)\n",
            "[土耳其语] 进度：180/1000 (+40)\n",
            "[土耳其语] 进度：220/1000 (+40)\n",
            "[土耳其语] 进度：260/1000 (+40)\n",
            "[土耳其语] 进度：299/1000 (+39)\n",
            "[土耳其语] 进度：339/1000 (+40)\n",
            "[土耳其语] 进度：379/1000 (+40)\n",
            "[土耳其语] 进度：419/1000 (+40)\n",
            "[土耳其语] 进度：459/1000 (+40)\n",
            "[土耳其语] 进度：499/1000 (+40)\n",
            "[土耳其语] 进度：539/1000 (+40)\n",
            "[土耳其语] 进度：578/1000 (+39)\n",
            "[土耳其语] 进度：618/1000 (+40)\n",
            "[土耳其语] 进度：658/1000 (+40)\n",
            "[土耳其语] 进度：698/1000 (+40)\n",
            "[土耳其语] 进度：737/1000 (+39)\n",
            "[土耳其语] 进度：777/1000 (+40)\n",
            "[土耳其语] 进度：817/1000 (+40)\n",
            "[土耳其语] 进度：857/1000 (+40)\n",
            "[土耳其语] 进度：897/1000 (+40)\n",
            "[土耳其语] 进度：937/1000 (+40)\n",
            "[土耳其语] 进度：977/1000 (+40)\n",
            "[土耳其语] 进度：1000/1000 (+23)\n",
            "[土耳其语] 完成，合计 1000 条 → /content/drive/MyDrive/NER/address_templates/address_templates_0/土耳其语_address_templates.jsonl\n",
            "[德语] 目标 1000 条，已存在 100 条，需新增 900 条；batch=40 → /content/drive/MyDrive/NER/address_templates/address_templates_0/德语_address_templates.jsonl\n",
            "[德语] 进度：140/1000 (+40)\n",
            "[德语] 进度：180/1000 (+40)\n",
            "[德语] 进度：220/1000 (+40)\n",
            "[德语] 进度：260/1000 (+40)\n",
            "[德语] 进度：300/1000 (+40)\n",
            "[德语] 进度：340/1000 (+40)\n",
            "[德语] 进度：380/1000 (+40)\n",
            "[德语] 进度：420/1000 (+40)\n",
            "[德语] 进度：460/1000 (+40)\n",
            "[德语] 进度：500/1000 (+40)\n",
            "[德语] 进度：540/1000 (+40)\n",
            "[德语] 进度：580/1000 (+40)\n",
            "[德语] 进度：620/1000 (+40)\n",
            "[德语] 进度：660/1000 (+40)\n",
            "[德语] 进度：700/1000 (+40)\n",
            "[德语] 进度：740/1000 (+40)\n",
            "[德语] 进度：780/1000 (+40)\n",
            "[德语] 进度：820/1000 (+40)\n",
            "[德语] 进度：860/1000 (+40)\n",
            "[德语] 进度：900/1000 (+40)\n",
            "[德语] 进度：940/1000 (+40)\n",
            "[德语] 进度：980/1000 (+40)\n",
            "[德语] 进度：1000/1000 (+20)\n",
            "[德语] 完成，合计 1000 条 → /content/drive/MyDrive/NER/address_templates/address_templates_0/德语_address_templates.jsonl\n",
            "[意大利语] 目标 1000 条，已存在 100 条，需新增 900 条；batch=40 → /content/drive/MyDrive/NER/address_templates/address_templates_0/意大利语_address_templates.jsonl\n",
            "[意大利语] 进度：140/1000 (+40)\n",
            "[意大利语] 进度：180/1000 (+40)\n",
            "[意大利语] 进度：220/1000 (+40)\n",
            "[意大利语] 进度：260/1000 (+40)\n",
            "[意大利语] 进度：300/1000 (+40)\n",
            "[意大利语] 进度：340/1000 (+40)\n",
            "[意大利语] 进度：380/1000 (+40)\n",
            "[意大利语] 进度：420/1000 (+40)\n",
            "[意大利语] 进度：460/1000 (+40)\n",
            "[意大利语] 进度：500/1000 (+40)\n",
            "[意大利语] 进度：540/1000 (+40)\n",
            "[意大利语] 进度：580/1000 (+40)\n",
            "[意大利语] 进度：620/1000 (+40)\n",
            "[意大利语] 进度：660/1000 (+40)\n",
            "[意大利语] 进度：700/1000 (+40)\n",
            "[意大利语] 进度：740/1000 (+40)\n",
            "[意大利语] 进度：780/1000 (+40)\n",
            "[意大利语] 进度：820/1000 (+40)\n",
            "[意大利语] 进度：860/1000 (+40)\n",
            "[意大利语] 进度：900/1000 (+40)\n",
            "[意大利语] 进度：940/1000 (+40)\n",
            "[意大利语] 进度：980/1000 (+40)\n",
            "[意大利语] 进度：1000/1000 (+20)\n",
            "[意大利语] 完成，合计 1000 条 → /content/drive/MyDrive/NER/address_templates/address_templates_0/意大利语_address_templates.jsonl\n",
            "[日语] 目标 1000 条，已存在 100 条，需新增 900 条；batch=40 → /content/drive/MyDrive/NER/address_templates/address_templates_0/日语_address_templates.jsonl\n",
            "[日语] 进度：140/1000 (+40)\n",
            "[日语] 进度：180/1000 (+40)\n",
            "[日语] 进度：220/1000 (+40)\n",
            "[日语] 进度：260/1000 (+40)\n",
            "[日语] 进度：300/1000 (+40)\n",
            "[日语] 进度：340/1000 (+40)\n",
            "[日语] 进度：380/1000 (+40)\n",
            "[日语] 进度：420/1000 (+40)\n",
            "[日语] 进度：460/1000 (+40)\n",
            "[日语] 进度：500/1000 (+40)\n",
            "[日语] 进度：540/1000 (+40)\n",
            "[日语] 进度：580/1000 (+40)\n",
            "[日语] 进度：620/1000 (+40)\n",
            "[日语] 进度：660/1000 (+40)\n",
            "[日语] 进度：700/1000 (+40)\n",
            "[日语] 进度：740/1000 (+40)\n",
            "[日语] 进度：780/1000 (+40)\n",
            "[日语] 进度：820/1000 (+40)\n",
            "[日语] 进度：860/1000 (+40)\n",
            "[日语] 进度：900/1000 (+40)\n",
            "[日语] 进度：940/1000 (+40)\n",
            "[日语] 进度：980/1000 (+40)\n",
            "[日语] 进度：1000/1000 (+20)\n",
            "[日语] 完成，合计 1000 条 → /content/drive/MyDrive/NER/address_templates/address_templates_0/日语_address_templates.jsonl\n",
            "[法语] 目标 1000 条，已存在 100 条，需新增 900 条；batch=40 → /content/drive/MyDrive/NER/address_templates/address_templates_0/法语_address_templates.jsonl\n",
            "[法语] 进度：140/1000 (+40)\n",
            "[法语] 进度：180/1000 (+40)\n",
            "[法语] 进度：220/1000 (+40)\n",
            "[法语] 进度：260/1000 (+40)\n",
            "[法语] 进度：300/1000 (+40)\n",
            "[法语] 进度：340/1000 (+40)\n",
            "[法语] 进度：380/1000 (+40)\n",
            "[法语] 进度：420/1000 (+40)\n",
            "[法语] 进度：460/1000 (+40)\n",
            "[法语] 进度：500/1000 (+40)\n",
            "[法语] 进度：540/1000 (+40)\n",
            "[法语] 进度：580/1000 (+40)\n",
            "[法语] 进度：620/1000 (+40)\n",
            "[法语] 进度：660/1000 (+40)\n",
            "[法语] 进度：700/1000 (+40)\n",
            "[法语] 进度：740/1000 (+40)\n",
            "[法语] 进度：780/1000 (+40)\n",
            "[法语] 进度：820/1000 (+40)\n",
            "[法语] 进度：860/1000 (+40)\n",
            "[法语] 进度：900/1000 (+40)\n",
            "[法语] 进度：940/1000 (+40)\n",
            "[法语] 进度：980/1000 (+40)\n",
            "[法语] 进度：1000/1000 (+20)\n",
            "[法语] 完成，合计 1000 条 → /content/drive/MyDrive/NER/address_templates/address_templates_0/法语_address_templates.jsonl\n",
            "[瑞典语] 目标 1000 条，已存在 100 条，需新增 900 条；batch=40 → /content/drive/MyDrive/NER/address_templates/address_templates_0/瑞典语_address_templates.jsonl\n",
            "[瑞典语] 进度：140/1000 (+40)\n",
            "[瑞典语] 进度：180/1000 (+40)\n",
            "[瑞典语] 进度：220/1000 (+40)\n",
            "[瑞典语] 进度：260/1000 (+40)\n",
            "[瑞典语] 进度：300/1000 (+40)\n",
            "[瑞典语] 进度：340/1000 (+40)\n",
            "[瑞典语] 进度：380/1000 (+40)\n",
            "[瑞典语] 进度：420/1000 (+40)\n",
            "[瑞典语] 进度：460/1000 (+40)\n",
            "[瑞典语] 进度：500/1000 (+40)\n",
            "[瑞典语] 进度：540/1000 (+40)\n",
            "[瑞典语] 进度：580/1000 (+40)\n",
            "[瑞典语] 进度：620/1000 (+40)\n",
            "[瑞典语] 进度：660/1000 (+40)\n",
            "[瑞典语] 进度：700/1000 (+40)\n",
            "[瑞典语] 进度：740/1000 (+40)\n",
            "[瑞典语] 进度：780/1000 (+40)\n",
            "[瑞典语] 进度：820/1000 (+40)\n",
            "[瑞典语] 进度：860/1000 (+40)\n",
            "[瑞典语] 进度：900/1000 (+40)\n",
            "[瑞典语] 进度：940/1000 (+40)\n",
            "[瑞典语] 进度：980/1000 (+40)\n",
            "[瑞典语] 进度：1000/1000 (+20)\n",
            "[瑞典语] 完成，合计 1000 条 → /content/drive/MyDrive/NER/address_templates/address_templates_0/瑞典语_address_templates.jsonl\n",
            "[荷兰语] 目标 1000 条，已存在 100 条，需新增 900 条；batch=40 → /content/drive/MyDrive/NER/address_templates/address_templates_0/荷兰语_address_templates.jsonl\n",
            "[荷兰语] 进度：140/1000 (+40)\n",
            "[荷兰语] 进度：180/1000 (+40)\n",
            "[荷兰语] 进度：220/1000 (+40)\n",
            "[荷兰语] 进度：260/1000 (+40)\n",
            "[荷兰语] 进度：300/1000 (+40)\n",
            "[荷兰语] 进度：340/1000 (+40)\n",
            "[荷兰语] 进度：380/1000 (+40)\n",
            "[荷兰语] 进度：420/1000 (+40)\n",
            "[荷兰语] 进度：460/1000 (+40)\n",
            "[荷兰语] 进度：500/1000 (+40)\n",
            "[荷兰语] 进度：540/1000 (+40)\n",
            "[荷兰语] 进度：580/1000 (+40)\n",
            "[荷兰语] 进度：620/1000 (+40)\n",
            "[荷兰语] 进度：659/1000 (+39)\n",
            "[荷兰语] 进度：699/1000 (+40)\n",
            "[荷兰语] 进度：739/1000 (+40)\n",
            "[荷兰语] 进度：779/1000 (+40)\n",
            "[荷兰语] 进度：819/1000 (+40)\n",
            "[荷兰语] 进度：859/1000 (+40)\n",
            "[荷兰语] 进度：899/1000 (+40)\n",
            "[荷兰语] 进度：939/1000 (+40)\n",
            "[荷兰语] 进度：979/1000 (+40)\n",
            "[荷兰语] 进度：1000/1000 (+21)\n",
            "[荷兰语] 完成，合计 1000 条 → /content/drive/MyDrive/NER/address_templates/address_templates_0/荷兰语_address_templates.jsonl\n",
            "[葡萄牙语] 目标 1000 条，已存在 100 条，需新增 900 条；batch=40 → /content/drive/MyDrive/NER/address_templates/address_templates_0/葡萄牙语_address_templates.jsonl\n",
            "[葡萄牙语] 进度：140/1000 (+40)\n",
            "[葡萄牙语] 进度：180/1000 (+40)\n",
            "[葡萄牙语] 进度：220/1000 (+40)\n",
            "[葡萄牙语] 进度：260/1000 (+40)\n",
            "[葡萄牙语] 进度：300/1000 (+40)\n",
            "[葡萄牙语] 进度：340/1000 (+40)\n",
            "[葡萄牙语] 进度：380/1000 (+40)\n",
            "[葡萄牙语] 进度：420/1000 (+40)\n",
            "[葡萄牙语] 进度：460/1000 (+40)\n",
            "[葡萄牙语] 进度：500/1000 (+40)\n",
            "[葡萄牙语] 进度：540/1000 (+40)\n",
            "[葡萄牙语] 进度：580/1000 (+40)\n",
            "[葡萄牙语] 进度：620/1000 (+40)\n",
            "[葡萄牙语] 进度：660/1000 (+40)\n",
            "[葡萄牙语] 进度：700/1000 (+40)\n",
            "[葡萄牙语] 进度：740/1000 (+40)\n",
            "[葡萄牙语] 进度：780/1000 (+40)\n",
            "[葡萄牙语] 进度：820/1000 (+40)\n",
            "[葡萄牙语] 进度：860/1000 (+40)\n",
            "[葡萄牙语] 进度：900/1000 (+40)\n",
            "[葡萄牙语] 进度：940/1000 (+40)\n",
            "[葡萄牙语] 进度：980/1000 (+40)\n",
            "[葡萄牙语] 进度：1000/1000 (+20)\n",
            "[葡萄牙语] 完成，合计 1000 条 → /content/drive/MyDrive/NER/address_templates/address_templates_0/葡萄牙语_address_templates.jsonl\n",
            "[西班牙语] 目标 1000 条，已存在 100 条，需新增 900 条；batch=40 → /content/drive/MyDrive/NER/address_templates/address_templates_0/西班牙语_address_templates.jsonl\n",
            "[西班牙语] 进度：140/1000 (+40)\n",
            "[西班牙语] 进度：180/1000 (+40)\n",
            "[西班牙语] 进度：220/1000 (+40)\n",
            "[西班牙语] 进度：260/1000 (+40)\n",
            "[西班牙语] 进度：300/1000 (+40)\n",
            "[西班牙语] 进度：340/1000 (+40)\n",
            "[西班牙语] 进度：380/1000 (+40)\n",
            "[西班牙语] 进度：420/1000 (+40)\n",
            "[西班牙语] 进度：460/1000 (+40)\n",
            "[西班牙语] 进度：500/1000 (+40)\n",
            "[西班牙语] 进度：540/1000 (+40)\n",
            "[西班牙语] 进度：580/1000 (+40)\n",
            "[西班牙语] 进度：620/1000 (+40)\n",
            "[西班牙语] 进度：660/1000 (+40)\n",
            "[西班牙语] 进度：700/1000 (+40)\n",
            "[西班牙语] 进度：740/1000 (+40)\n",
            "[西班牙语] 进度：780/1000 (+40)\n",
            "[西班牙语] 进度：820/1000 (+40)\n",
            "[西班牙语] 进度：860/1000 (+40)\n",
            "[西班牙语] 进度：900/1000 (+40)\n",
            "[西班牙语] 进度：940/1000 (+40)\n",
            "[西班牙语] 进度：980/1000 (+40)\n",
            "[西班牙语] 进度：1000/1000 (+20)\n",
            "[西班牙语] 完成，合计 1000 条 → /content/drive/MyDrive/NER/address_templates/address_templates_0/西班牙语_address_templates.jsonl\n",
            "[韩语] 目标 1000 条，已存在 100 条，需新增 900 条；batch=40 → /content/drive/MyDrive/NER/address_templates/address_templates_0/韩语_address_templates.jsonl\n",
            "[韩语] 进度：140/1000 (+40)\n",
            "[韩语] 进度：180/1000 (+40)\n",
            "[韩语] 进度：220/1000 (+40)\n",
            "[韩语] 进度：260/1000 (+40)\n",
            "[韩语] 进度：300/1000 (+40)\n",
            "[韩语] 进度：340/1000 (+40)\n",
            "[韩语] 进度：380/1000 (+40)\n",
            "[韩语] 进度：420/1000 (+40)\n",
            "[韩语] 进度：460/1000 (+40)\n",
            "[韩语] 进度：500/1000 (+40)\n",
            "[韩语] 进度：540/1000 (+40)\n",
            "[韩语] 进度：580/1000 (+40)\n",
            "[韩语] 进度：620/1000 (+40)\n",
            "[韩语] 进度：660/1000 (+40)\n",
            "[韩语] 进度：700/1000 (+40)\n",
            "[韩语] 进度：740/1000 (+40)\n",
            "[韩语] 进度：780/1000 (+40)\n",
            "[韩语] 进度：820/1000 (+40)\n",
            "[韩语] 进度：860/1000 (+40)\n",
            "[韩语] 进度：900/1000 (+40)\n",
            "[韩语] 进度：940/1000 (+40)\n",
            "[韩语] 进度：980/1000 (+40)\n",
            "[韩语] 进度：1000/1000 (+20)\n",
            "[韩语] 完成，合计 1000 条 → /content/drive/MyDrive/NER/address_templates/address_templates_0/韩语_address_templates.jsonl\n",
            "============ 开始：address数量： 1（每语言目标 2000 条） ============\n",
            "[简体中文] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_1/简体中文_address_templates.jsonl\n",
            "[繁体中文] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_1/繁体中文_address_templates.jsonl\n",
            "[英语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_1/英语_address_templates.jsonl\n",
            "[丹麦语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_1/丹麦语_address_templates.jsonl\n",
            "[俄语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_1/俄语_address_templates.jsonl\n",
            "[土耳其语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_1/土耳其语_address_templates.jsonl\n",
            "[德语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_1/德语_address_templates.jsonl\n",
            "[意大利语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_1/意大利语_address_templates.jsonl\n",
            "[日语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_1/日语_address_templates.jsonl\n",
            "[法语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_1/法语_address_templates.jsonl\n",
            "[瑞典语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_1/瑞典语_address_templates.jsonl\n",
            "[荷兰语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_1/荷兰语_address_templates.jsonl\n",
            "[葡萄牙语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_1/葡萄牙语_address_templates.jsonl\n",
            "[西班牙语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_1/西班牙语_address_templates.jsonl\n",
            "[韩语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_1/韩语_address_templates.jsonl\n",
            "============ 开始：address数量： 2（每语言目标 2000 条） ============\n",
            "[简体中文] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_2/简体中文_address_templates.jsonl\n",
            "[繁体中文] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_2/繁体中文_address_templates.jsonl\n",
            "[英语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_2/英语_address_templates.jsonl\n",
            "[丹麦语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_2/丹麦语_address_templates.jsonl\n",
            "[俄语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_2/俄语_address_templates.jsonl\n",
            "[土耳其语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_2/土耳其语_address_templates.jsonl\n",
            "[德语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_2/德语_address_templates.jsonl\n",
            "[意大利语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_2/意大利语_address_templates.jsonl\n",
            "[日语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_2/日语_address_templates.jsonl\n",
            "[法语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_2/法语_address_templates.jsonl\n",
            "[瑞典语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_2/瑞典语_address_templates.jsonl\n",
            "[荷兰语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_2/荷兰语_address_templates.jsonl\n",
            "[葡萄牙语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_2/葡萄牙语_address_templates.jsonl\n",
            "[西班牙语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_2/西班牙语_address_templates.jsonl\n",
            "[韩语] 已有 2000 条，已达到目标 2000，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_2/韩语_address_templates.jsonl\n",
            "============ 开始：address数量： 3（每语言目标 1500 条） ============\n",
            "[简体中文] 已有 1500 条，已达到目标 1500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_3/简体中文_address_templates.jsonl\n",
            "[繁体中文] 已有 1500 条，已达到目标 1500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_3/繁体中文_address_templates.jsonl\n",
            "[英语] 已有 1500 条，已达到目标 1500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_3/英语_address_templates.jsonl\n",
            "[丹麦语] 已有 1500 条，已达到目标 1500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_3/丹麦语_address_templates.jsonl\n",
            "[俄语] 已有 1500 条，已达到目标 1500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_3/俄语_address_templates.jsonl\n",
            "[土耳其语] 已有 1500 条，已达到目标 1500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_3/土耳其语_address_templates.jsonl\n",
            "[德语] 已有 1500 条，已达到目标 1500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_3/德语_address_templates.jsonl\n",
            "[意大利语] 已有 1500 条，已达到目标 1500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_3/意大利语_address_templates.jsonl\n",
            "[日语] 已有 1500 条，已达到目标 1500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_3/日语_address_templates.jsonl\n",
            "[法语] 已有 1500 条，已达到目标 1500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_3/法语_address_templates.jsonl\n",
            "[瑞典语] 已有 1500 条，已达到目标 1500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_3/瑞典语_address_templates.jsonl\n",
            "[荷兰语] 已有 1500 条，已达到目标 1500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_3/荷兰语_address_templates.jsonl\n",
            "[葡萄牙语] 已有 1500 条，已达到目标 1500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_3/葡萄牙语_address_templates.jsonl\n",
            "[西班牙语] 已有 1500 条，已达到目标 1500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_3/西班牙语_address_templates.jsonl\n",
            "[韩语] 已有 1500 条，已达到目标 1500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_3/韩语_address_templates.jsonl\n",
            "============ 开始：address数量： 4（每语言目标 500 条） ============\n",
            "[简体中文] 已有 500 条，已达到目标 500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_4/简体中文_address_templates.jsonl\n",
            "[繁体中文] 已有 500 条，已达到目标 500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_4/繁体中文_address_templates.jsonl\n",
            "[英语] 已有 500 条，已达到目标 500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_4/英语_address_templates.jsonl\n",
            "[丹麦语] 已有 500 条，已达到目标 500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_4/丹麦语_address_templates.jsonl\n",
            "[俄语] 已有 500 条，已达到目标 500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_4/俄语_address_templates.jsonl\n",
            "[土耳其语] 已有 500 条，已达到目标 500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_4/土耳其语_address_templates.jsonl\n",
            "[德语] 已有 500 条，已达到目标 500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_4/德语_address_templates.jsonl\n",
            "[意大利语] 已有 500 条，已达到目标 500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_4/意大利语_address_templates.jsonl\n",
            "[日语] 已有 500 条，已达到目标 500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_4/日语_address_templates.jsonl\n",
            "[法语] 已有 500 条，已达到目标 500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_4/法语_address_templates.jsonl\n",
            "[瑞典语] 已有 500 条，已达到目标 500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_4/瑞典语_address_templates.jsonl\n",
            "[荷兰语] 已有 500 条，已达到目标 500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_4/荷兰语_address_templates.jsonl\n",
            "[葡萄牙语] 已有 500 条，已达到目标 500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_4/葡萄牙语_address_templates.jsonl\n",
            "[西班牙语] 已有 500 条，已达到目标 500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_4/西班牙语_address_templates.jsonl\n",
            "[韩语] 已有 500 条，已达到目标 500，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_4/韩语_address_templates.jsonl\n",
            "============ 开始：address数量： 5（每语言目标 100 条） ============\n",
            "[简体中文] 已有 100 条，已达到目标 100，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_5/简体中文_address_templates.jsonl\n",
            "[繁体中文] 已有 100 条，已达到目标 100，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_5/繁体中文_address_templates.jsonl\n",
            "[英语] 已有 100 条，已达到目标 100，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_5/英语_address_templates.jsonl\n",
            "[丹麦语] 已有 100 条，已达到目标 100，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_5/丹麦语_address_templates.jsonl\n",
            "[俄语] 已有 100 条，已达到目标 100，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_5/俄语_address_templates.jsonl\n",
            "[土耳其语] 已有 100 条，已达到目标 100，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_5/土耳其语_address_templates.jsonl\n",
            "[德语] 已有 100 条，已达到目标 100，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_5/德语_address_templates.jsonl\n",
            "[意大利语] 已有 100 条，已达到目标 100，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_5/意大利语_address_templates.jsonl\n",
            "[日语] 已有 100 条，已达到目标 100，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_5/日语_address_templates.jsonl\n",
            "[法语] 已有 100 条，已达到目标 100，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_5/法语_address_templates.jsonl\n",
            "[瑞典语] 已有 100 条，已达到目标 100，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_5/瑞典语_address_templates.jsonl\n",
            "[荷兰语] 已有 100 条，已达到目标 100，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_5/荷兰语_address_templates.jsonl\n",
            "[葡萄牙语] 已有 100 条，已达到目标 100，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_5/葡萄牙语_address_templates.jsonl\n",
            "[西班牙语] 已有 100 条，已达到目标 100，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_5/西班牙语_address_templates.jsonl\n",
            "[韩语] 已有 100 条，已达到目标 100，跳过。→ /content/drive/MyDrive/NER/address_templates/address_templates_5/韩语_address_templates.jsonl\n",
            "\n",
            "==================================================\n",
            "全部模板生成完成！目录按地址占位数区分：address_templates_<slot>\n",
            "文件命名：<语言>_address_templates.jsonl（每行一条 JSON）\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 生成测试用负例（纯地址）"
      ],
      "metadata": {
        "id": "shzLd24gOUiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 输出目录\n",
        "output_dir = \"/content/drive/MyDrive/NER/non_address\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# 各语言常见楼栋表达\n",
        "translations = {\n",
        "    \"简体中文\": \"xx栋\",\n",
        "    \"繁体中文\": \"xx棟\",\n",
        "    \"英语\": \"Building xx\",\n",
        "    \"丹麦语\": \"Bygning xx\",\n",
        "    \"俄语\": \"Здание xx\",\n",
        "    \"土耳其语\": \"xx Blok\",\n",
        "    \"德语\": \"Gebäude xx\",\n",
        "    \"意大利语\": \"Edificio xx\",\n",
        "    \"日语\": \"xx棟\",\n",
        "    \"法语\": \"Bâtiment xx\",\n",
        "    \"瑞典语\": \"Byggnad xx\",\n",
        "    \"荷兰语\": \"Gebouw xx\",\n",
        "    \"葡萄牙语\": \"Edifício xx\",\n",
        "    \"西班牙语\": \"Edificio xx\",\n",
        "    \"韩语\": \"xx동\",\n",
        "}\n",
        "\n",
        "def generate_files():\n",
        "    for lang, phrase in translations.items():\n",
        "        filepath = os.path.join(output_dir, f\"{lang}.txt\")\n",
        "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "            # 原有楼栋负例\n",
        "            for i in range(1, 50):\n",
        "                f.write(phrase.replace(\"xx\", str(i)) + \"\\n\")\n",
        "\n",
        "            # 新增 xx# 样式（专门避免 16#、23# 这种被识别）\n",
        "            for i in range(1, 50):\n",
        "                f.write(f\"{i}#\\n\")\n",
        "        print(f\"生成文件: {filepath}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qqi5inJOX_u",
        "outputId": "91467d3c-aa80-4871-c1d5-fccfcbbe77f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "生成文件: /content/drive/MyDrive/NER/non_address/简体中文.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_address/繁体中文.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_address/英语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_address/丹麦语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_address/俄语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_address/土耳其语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_address/德语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_address/意大利语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_address/日语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_address/法语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_address/瑞典语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_address/荷兰语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_address/葡萄牙语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_address/西班牙语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_address/韩语.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 纯时间模版生成器\n",
        "参照纯地址模版生成器编写的时间模版生成代码，在prompt中示例内容进行了结构性优化，同时降采样到地址模板的四分之一（考虑到时间模板自身的结构多样性有限）"
      ],
      "metadata": {
        "id": "WlfY1ZTFLRV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 纯时间模板生成器 {\"form-width\":\"35%\"}\n",
        "# 依赖：pip install httpx\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import asyncio\n",
        "from typing import List, Dict, Any, Optional\n",
        "import httpx\n",
        "\n",
        "# ======================================================可按需修改的配置 ======================================================\n",
        "\n",
        "# 原地址模板目标\n",
        "ADDRESS_SLOT_LIST   = [0, 1, 2, 3, 4, 5]\n",
        "TARGET_PER_SLOT     = [1000, 2000, 2000, 1500, 500, 100]\n",
        "\n",
        "# 时间模板目标 = 地址目标的 1/4，只保留 0-3 slot\n",
        "TIME_SLOT_LIST      = [0, 1, 2, 3]\n",
        "TARGET_PER_SLOT_TIME = [int(x/4) for x in TARGET_PER_SLOT[:len(TIME_SLOT_LIST)]]\n",
        "\n",
        "api_key = \"sk-013SeYkFFCjw1G3IIwDWfj2iXFqWhUSF6fmC0L7KdJjE\"  # @param {\"type\":\"string\",\"placeholder\":\"请输入大模型api key\"}\n",
        "\n",
        "TEMPLATE_BATCH_SIZE = 40\n",
        "TEMPERATURE = 0.8\n",
        "MODEL_NAME = \"turing/gpt-4o\"   # 你的模型名称\n",
        "\n",
        "# 输出路径（独立于地址模板）\n",
        "OUTPUT_BASE_DIR = \"/content/drive/MyDrive/NER/time_templates\"\n",
        "SLOT2TARGET = dict(zip(TIME_SLOT_LIST, TARGET_PER_SLOT_TIME))\n",
        "\n",
        "# ================== 接口配置 ==================\n",
        "BASE_URL = os.getenv(\"LLM_BASE_URL\", \"https://test-turing.cn.llm.tcljd.com\")\n",
        "ENDPOINT = os.getenv(\"LLM_ENDPOINT\", \"/api/v1/chat/completions\")\n",
        "API_KEY  = os.getenv(\"LLM_API_KEY\", api_key)\n",
        "\n",
        "HEADERS = {\n",
        "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}\n",
        "\n",
        "LANGUAGES = [\n",
        "    \"简体中文\", \"繁体中文\", \"英语\", \"丹麦语\", \"俄语\", \"土耳其语\", \"德语\",\n",
        "    \"意大利语\", \"日语\", \"法语\", \"瑞典语\", \"荷兰语\", \"葡萄牙语\",\n",
        "    \"西班牙语\", \"韩语\"\n",
        "]\n",
        "\n",
        "TIMEOUT = 90\n",
        "RETRY_TIMES = 2\n",
        "client: Optional[httpx.AsyncClient] = None\n",
        "\n",
        "# ================== 示例（重写高质量时间模板示例） ==================\n",
        "# prompt_example1_time 用于告诉模型“每条模板只有占位符，风格多样”\n",
        "# 索引 n_time-1，对应 slot = 1, 2, 3\n",
        "prompt_example1_time = [\n",
        "    '{\"template\": \"这里是一条模板文本，包含{time}。\"}',\n",
        "    '{\"template\": \"这里是一条模板文本，包含{time}和{time}。\"}',\n",
        "    '{\"template\": \"这里是一条模板文本，包含{time}、{time}和{time}。\"}',\n",
        "]\n",
        "\n",
        "# prompt_example2_time：每个slot对应一个中英示例数组，全部时间用占位符，不出现真实时间词\n",
        "prompt_example2_time = [\n",
        "    # slot=0（纯负例，不含任何时间概念词）\n",
        "    \"\"\"\n",
        "    [\n",
        "      {\"template\": \"这个方案我们需要进一步完善后再提交。\"},\n",
        "      {\"template\": \"Your proposal looks promising, let's polish it more.\"}\n",
        "    ]\n",
        "    \"\"\",\n",
        "    # slot=1（一个 {time} 占位符，中英母语化风格）\n",
        "    \"\"\"\n",
        "    [\n",
        "      {\"template\": \"请在{time}参加培训会，不要迟到。\"},\n",
        "      {\"template\": \"Let’s meet at {time}, I’ll bring the new designs.\"}\n",
        "    ]\n",
        "    \"\"\",\n",
        "    # slot=2（两个 {time} 占位符，中英母语化风格）\n",
        "    \"\"\"\n",
        "    [\n",
        "      {\"template\": \"从{time}开始，一直到{time}结束，请全程参与。\"},\n",
        "      {\"template\": \"We’ll start at {time} and finish by {time}, be prepared.\"}\n",
        "    ]\n",
        "    \"\"\",\n",
        "    # slot=3（三个 {time} 占位符，分时段安排）\n",
        "    \"\"\"\n",
        "    [\n",
        "      {\"template\": \"在{time}召开会议，{time}午餐，{time}继续讨论方案。\"},\n",
        "      {\"template\": \"Kick-off at {time}, lunch at {time}, resume work by {time}.\"}\n",
        "    ]\n",
        "    \"\"\"\n",
        "]\n",
        "\n",
        "# ================== 工具函数（重写后的 build_template_prompt） ==================\n",
        "def build_template_prompt(language: str, n: int, n_time: int) -> str:\n",
        "    \"\"\"\n",
        "    构造生成时间模板的 Prompt。\n",
        "    规则：\n",
        "      - slot=0: 不允许出现任何时间相关的词汇或暗示\n",
        "      - slot>0: 所有时间类词汇用 {time} 占位符代替\n",
        "    \"\"\"\n",
        "    if n_time == 0:\n",
        "        # 纯负例，不含时间\n",
        "        return f\"\"\"\n",
        "            你是一个不包含时间信息的剪贴板文本生成器。请用 **{language}** 语言生成 {n} 条“不包含时间的剪贴板文本”：\n",
        "                - 模板里严禁出现任何时间相关词汇（包括日期、时刻、星期、上午、下午、中午、夜晚、昨天、今天、明天、稍后、之后等）\n",
        "                - 风格多样：口语、正式通知、聊天记录、评论、邮件、日程卡片、社交帖子等，长度 1–3 句\n",
        "                - 必须输出“纯 JSON 数组”，不要代码块、不要多余说明\n",
        "                - JSON 文本中不要包含换行符 \\\\n\n",
        "            示例：\n",
        "            {prompt_example2_time[0]}\n",
        "            现在请直接输出 **{n} 条**该语言的剪贴板文本 JSON 数组。\n",
        "        \"\"\"\n",
        "    else:\n",
        "        # 时间占位符模板\n",
        "        return f\"\"\"\n",
        "            你是一个模板生成器。请用 **{language}** 语言生成 {n} 条“上下文模板”，\n",
        "            每条模板必须包含 {n_time} 个占位符：\n",
        "                - 占位符：{{time}}（必须原样出现，不加引号）\n",
        "                - 模板里严禁出现任何真实时间相关词汇（上午、下午、中午、夜晚、昨天、今天、明天、日期、时刻等），一律用 {{time}} 占位符替代\n",
        "                - 风格多样：口语、正式通知、聊天记录、评论、邮件、日程卡片、社交帖子等，长度 1–3 句\n",
        "                - 必须输出“纯 JSON 数组”，不要代码块、不要多余说明，数组元素为：\n",
        "                  {prompt_example1_time[n_time-1]}\n",
        "                - JSON 文本中不要包含换行符 \\\\n\n",
        "            示例：\n",
        "            {prompt_example2_time[n_time]}\n",
        "            现在请直接输出 **{n} 条**该语言的模板 JSON 数组。\n",
        "        \"\"\"\n",
        "\n",
        "def _strip_code_fence(s: str) -> str:\n",
        "    txt = s.strip()\n",
        "    if txt.startswith(\"```\"):\n",
        "        i = txt.find(\"[\")\n",
        "        if i == -1: i = txt.find(\"{\")\n",
        "        if i != -1: txt = txt[i:]\n",
        "        if txt.endswith(\"```\"):\n",
        "            txt = txt[:-3].strip()\n",
        "    return txt\n",
        "\n",
        "def _parse_json_array(text: str) -> List[Dict[str, Any]]:\n",
        "    txt = _strip_code_fence(text)\n",
        "    try:\n",
        "        data = json.loads(txt)\n",
        "        if isinstance(data, list):\n",
        "            return data\n",
        "    except Exception:\n",
        "        pass\n",
        "    m = re.search(r\"$$[\\s\\S]*$$\", txt)\n",
        "    if m:\n",
        "        arr = m.group(0)\n",
        "        return json.loads(arr)\n",
        "    raise ValueError(f\"无法解析为 JSON 数组，片段：{txt[:200]}...\")\n",
        "\n",
        "def _post_process_templates(json_list: List[Dict[str, Any]], need: int, n_time: int) -> List[str]:\n",
        "    out, seen = [], set()\n",
        "    for item in json_list:\n",
        "        if not isinstance(item, dict): continue\n",
        "        t = item.get(\"template\")\n",
        "        if not isinstance(t, str): continue\n",
        "        t = \" \".join(t.strip().split())\n",
        "        if not t or t in seen: continue\n",
        "        if n_time == 0:\n",
        "            if \"{time}\" in t:\n",
        "                continue\n",
        "        else:\n",
        "            if t.count(\"{time}\") != n_time:\n",
        "                continue\n",
        "        out.append(t)\n",
        "        seen.add(t)\n",
        "    return out[:need]\n",
        "\n",
        "async def chat_stream(payload: dict, *, collect: bool = True) -> str:\n",
        "    assert client is not None\n",
        "    text_buf: List[str] = []\n",
        "    async with client.stream(\"POST\", ENDPOINT, headers=HEADERS, content=json.dumps(payload)) as resp:\n",
        "        async for line in resp.aiter_lines():\n",
        "            if not line: continue\n",
        "            line = line.strip()\n",
        "            if not line.startswith(\"data:\"): continue\n",
        "            data = line[5:].strip()\n",
        "            if data == \"[DONE]\": break\n",
        "            try:\n",
        "                delta = json.loads(data)[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n",
        "            except Exception:\n",
        "                continue\n",
        "            if collect:\n",
        "                text_buf.append(delta)\n",
        "    return \"\".join(text_buf)\n",
        "\n",
        "async def generate_templates_batch(language: str, n: int, n_time: int) -> List[str]:\n",
        "    prompt = build_template_prompt(language, n, n_time)\n",
        "    payload = {\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"stream\": True,\n",
        "        \"temperature\": TEMPERATURE,\n",
        "    }\n",
        "    raw = await chat_stream(payload, collect=True)\n",
        "    data = _parse_json_array(raw)\n",
        "    return _post_process_templates(data, n, n_time)\n",
        "\n",
        "async def generate_templates_for_language(language: str, slot: int) -> str:\n",
        "    target_per_lang = SLOT2TARGET.get(slot, 0)\n",
        "    if target_per_lang <= 0: return \"\"\n",
        "    out_dir = os.path.join(OUTPUT_BASE_DIR, f\"time_templates_{slot}\")\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    out_path = os.path.join(out_dir, f\"{language}_time_templates.jsonl\")\n",
        "\n",
        "    existing = 0\n",
        "    if os.path.exists(out_path):\n",
        "        with open(out_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            existing = sum(1 for _ in f)\n",
        "    target = target_per_lang\n",
        "    if existing >= target:\n",
        "        print(f\"[{language}] 已有 {existing} 条，已达到目标 {target}，跳过。→ {out_path}\")\n",
        "        return out_path\n",
        "\n",
        "    bs, total = TEMPLATE_BATCH_SIZE, existing\n",
        "    print(f\"[{language}] 目标 {target} 条，已有 {existing} 条 → {out_path}\")\n",
        "    ROUND_CAP, EMPTY_STREAK_CAP = 50, 3\n",
        "    rounds_done, empty_streak = 0, 0\n",
        "\n",
        "    while total < target and rounds_done < ROUND_CAP:\n",
        "        ask_n = min(bs, target - total)\n",
        "        for attempt in range(1, RETRY_TIMES + 2):\n",
        "            try:\n",
        "                batch = await generate_templates_batch(language, ask_n, slot)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"[{language}] 第{rounds_done+1}轮，第{attempt}次失败：{e}\")\n",
        "                await asyncio.sleep(1.2 * attempt)\n",
        "        else:\n",
        "            raise RuntimeError(f\"[{language}] 连续失败\")\n",
        "\n",
        "        got = len(batch)\n",
        "        if got == 0:\n",
        "            empty_streak += 1\n",
        "            if empty_streak >= EMPTY_STREAK_CAP:\n",
        "                break\n",
        "        else:\n",
        "            empty_streak = 0\n",
        "            with open(out_path, \"a\", encoding=\"utf-8\") as f:\n",
        "                for t in batch:\n",
        "                    rec = {\"language\": language, \"template\": t}\n",
        "                    f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "            total += got\n",
        "            print(f\"[{language}] 进度：{total}/{target} (+{got})\")\n",
        "        rounds_done += 1\n",
        "\n",
        "    return out_path\n",
        "\n",
        "async def generate_templates_for_all_languages():\n",
        "    for slot in TIME_SLOT_LIST:\n",
        "        target = SLOT2TARGET.get(slot, 0)\n",
        "        if target <= 0: continue\n",
        "        print(\"=\"*12, f\"开始：time数量 {slot}（每语言目标 {target} 条）\", \"=\"*12)\n",
        "        for lang in LANGUAGES:\n",
        "            await generate_templates_for_language(lang, slot)\n",
        "\n",
        "# ================== 主程序 ==================\n",
        "async def main():\n",
        "    global client\n",
        "    client = httpx.AsyncClient(base_url=BASE_URL, timeout=TIMEOUT)\n",
        "    try:\n",
        "        await generate_templates_for_all_languages()\n",
        "        print(\"全部时间模板生成完成！\")\n",
        "    finally:\n",
        "        await client.aclose()\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "id": "DPS_TnU2Lgh4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "d94beec8-8502-4ee4-8cc8-b07bcfc9526a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============ 开始：time数量 0（每语言目标 250 条） ============\n",
            "[简体中文] 目标 250 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_0/简体中文_time_templates.jsonl\n",
            "[简体中文] 进度：40/250 (+40)\n",
            "[简体中文] 进度：80/250 (+40)\n",
            "[简体中文] 进度：120/250 (+40)\n",
            "[简体中文] 进度：160/250 (+40)\n",
            "[简体中文] 进度：200/250 (+40)\n",
            "[简体中文] 进度：240/250 (+40)\n",
            "[简体中文] 进度：250/250 (+10)\n",
            "[繁体中文] 目标 250 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_0/繁体中文_time_templates.jsonl\n",
            "[繁体中文] 进度：40/250 (+40)\n",
            "[繁体中文] 进度：80/250 (+40)\n",
            "[繁体中文] 进度：120/250 (+40)\n",
            "[繁体中文] 进度：160/250 (+40)\n",
            "[繁体中文] 进度：200/250 (+40)\n",
            "[繁体中文] 进度：240/250 (+40)\n",
            "[繁体中文] 进度：250/250 (+10)\n",
            "[英语] 目标 250 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_0/英语_time_templates.jsonl\n",
            "[英语] 进度：40/250 (+40)\n",
            "[英语] 进度：80/250 (+40)\n",
            "[英语] 进度：120/250 (+40)\n",
            "[英语] 进度：160/250 (+40)\n",
            "[英语] 进度：200/250 (+40)\n",
            "[英语] 进度：240/250 (+40)\n",
            "[英语] 进度：250/250 (+10)\n",
            "[丹麦语] 目标 250 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_0/丹麦语_time_templates.jsonl\n",
            "[丹麦语] 进度：39/250 (+39)\n",
            "[丹麦语] 进度：79/250 (+40)\n",
            "[丹麦语] 进度：119/250 (+40)\n",
            "[丹麦语] 进度：159/250 (+40)\n",
            "[丹麦语] 进度：199/250 (+40)\n",
            "[丹麦语] 进度：239/250 (+40)\n",
            "[丹麦语] 进度：250/250 (+11)\n",
            "[俄语] 目标 250 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_0/俄语_time_templates.jsonl\n",
            "[俄语] 进度：40/250 (+40)\n",
            "[俄语] 进度：80/250 (+40)\n",
            "[俄语] 进度：119/250 (+39)\n",
            "[俄语] 进度：159/250 (+40)\n",
            "[俄语] 进度：198/250 (+39)\n",
            "[俄语] 进度：238/250 (+40)\n",
            "[俄语] 进度：250/250 (+12)\n",
            "[土耳其语] 目标 250 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_0/土耳其语_time_templates.jsonl\n",
            "[土耳其语] 进度：40/250 (+40)\n",
            "[土耳其语] 进度：80/250 (+40)\n",
            "[土耳其语] 进度：120/250 (+40)\n",
            "[土耳其语] 进度：159/250 (+39)\n",
            "[土耳其语] 进度：199/250 (+40)\n",
            "[土耳其语] 进度：239/250 (+40)\n",
            "[土耳其语] 进度：250/250 (+11)\n",
            "[德语] 目标 250 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_0/德语_time_templates.jsonl\n",
            "[德语] 进度：40/250 (+40)\n",
            "[德语] 进度：80/250 (+40)\n",
            "[德语] 进度：120/250 (+40)\n",
            "[德语] 进度：160/250 (+40)\n",
            "[德语] 进度：200/250 (+40)\n",
            "[德语] 进度：240/250 (+40)\n",
            "[德语] 进度：250/250 (+10)\n",
            "[意大利语] 目标 250 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_0/意大利语_time_templates.jsonl\n",
            "[意大利语] 进度：40/250 (+40)\n",
            "[意大利语] 进度：80/250 (+40)\n",
            "[意大利语] 进度：120/250 (+40)\n",
            "[意大利语] 进度：160/250 (+40)\n",
            "[意大利语] 进度：200/250 (+40)\n",
            "[意大利语] 进度：240/250 (+40)\n",
            "[意大利语] 进度：250/250 (+10)\n",
            "[日语] 目标 250 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_0/日语_time_templates.jsonl\n",
            "[日语] 进度：40/250 (+40)\n",
            "[日语] 进度：80/250 (+40)\n",
            "[日语] 进度：120/250 (+40)\n",
            "[日语] 进度：160/250 (+40)\n",
            "[日语] 进度：200/250 (+40)\n",
            "[日语] 进度：240/250 (+40)\n",
            "[日语] 进度：250/250 (+10)\n",
            "[法语] 目标 250 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_0/法语_time_templates.jsonl\n",
            "[法语] 进度：40/250 (+40)\n",
            "[法语] 进度：80/250 (+40)\n",
            "[法语] 进度：120/250 (+40)\n",
            "[法语] 进度：160/250 (+40)\n",
            "[法语] 进度：200/250 (+40)\n",
            "[法语] 进度：240/250 (+40)\n",
            "[法语] 进度：250/250 (+10)\n",
            "[瑞典语] 目标 250 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_0/瑞典语_time_templates.jsonl\n",
            "[瑞典语] 进度：39/250 (+39)\n",
            "[瑞典语] 进度：79/250 (+40)\n",
            "[瑞典语] 进度：119/250 (+40)\n",
            "[瑞典语] 进度：158/250 (+39)\n",
            "[瑞典语] 进度：198/250 (+40)\n",
            "[瑞典语] 进度：238/250 (+40)\n",
            "[瑞典语] 进度：250/250 (+12)\n",
            "[荷兰语] 目标 250 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_0/荷兰语_time_templates.jsonl\n",
            "[荷兰语] 进度：40/250 (+40)\n",
            "[荷兰语] 进度：80/250 (+40)\n",
            "[荷兰语] 进度：120/250 (+40)\n",
            "[荷兰语] 进度：160/250 (+40)\n",
            "[荷兰语] 进度：200/250 (+40)\n",
            "[荷兰语] 进度：240/250 (+40)\n",
            "[荷兰语] 进度：250/250 (+10)\n",
            "[葡萄牙语] 目标 250 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_0/葡萄牙语_time_templates.jsonl\n",
            "[葡萄牙语] 进度：40/250 (+40)\n",
            "[葡萄牙语] 进度：80/250 (+40)\n",
            "[葡萄牙语] 进度：120/250 (+40)\n",
            "[葡萄牙语] 进度：160/250 (+40)\n",
            "[葡萄牙语] 进度：200/250 (+40)\n",
            "[葡萄牙语] 进度：240/250 (+40)\n",
            "[葡萄牙语] 进度：250/250 (+10)\n",
            "[西班牙语] 目标 250 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_0/西班牙语_time_templates.jsonl\n",
            "[西班牙语] 进度：40/250 (+40)\n",
            "[西班牙语] 进度：80/250 (+40)\n",
            "[西班牙语] 进度：120/250 (+40)\n",
            "[西班牙语] 进度：160/250 (+40)\n",
            "[西班牙语] 进度：200/250 (+40)\n",
            "[西班牙语] 进度：240/250 (+40)\n",
            "[西班牙语] 进度：250/250 (+10)\n",
            "[韩语] 目标 250 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_0/韩语_time_templates.jsonl\n",
            "[韩语] 进度：40/250 (+40)\n",
            "[韩语] 进度：80/250 (+40)\n",
            "[韩语] 进度：120/250 (+40)\n",
            "[韩语] 进度：160/250 (+40)\n",
            "[韩语] 进度：200/250 (+40)\n",
            "[韩语] 进度：240/250 (+40)\n",
            "[韩语] 进度：250/250 (+10)\n",
            "============ 开始：time数量 1（每语言目标 500 条） ============\n",
            "[简体中文] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_1/简体中文_time_templates.jsonl\n",
            "[简体中文] 进度：40/500 (+40)\n",
            "[简体中文] 进度：80/500 (+40)\n",
            "[简体中文] 进度：120/500 (+40)\n",
            "[简体中文] 进度：160/500 (+40)\n",
            "[简体中文] 进度：200/500 (+40)\n",
            "[简体中文] 进度：240/500 (+40)\n",
            "[简体中文] 进度：280/500 (+40)\n",
            "[简体中文] 进度：320/500 (+40)\n",
            "[简体中文] 进度：360/500 (+40)\n",
            "[简体中文] 进度：400/500 (+40)\n",
            "[简体中文] 进度：440/500 (+40)\n",
            "[简体中文] 进度：480/500 (+40)\n",
            "[简体中文] 进度：500/500 (+20)\n",
            "[繁体中文] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_1/繁体中文_time_templates.jsonl\n",
            "[繁体中文] 进度：40/500 (+40)\n",
            "[繁体中文] 进度：80/500 (+40)\n",
            "[繁体中文] 进度：120/500 (+40)\n",
            "[繁体中文] 进度：160/500 (+40)\n",
            "[繁体中文] 进度：200/500 (+40)\n",
            "[繁体中文] 进度：240/500 (+40)\n",
            "[繁体中文] 进度：280/500 (+40)\n",
            "[繁体中文] 进度：320/500 (+40)\n",
            "[繁体中文] 进度：360/500 (+40)\n",
            "[繁体中文] 进度：400/500 (+40)\n",
            "[繁体中文] 进度：440/500 (+40)\n",
            "[繁体中文] 进度：480/500 (+40)\n",
            "[繁体中文] 进度：500/500 (+20)\n",
            "[英语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_1/英语_time_templates.jsonl\n",
            "[英语] 进度：40/500 (+40)\n",
            "[英语] 进度：80/500 (+40)\n",
            "[英语] 进度：120/500 (+40)\n",
            "[英语] 进度：159/500 (+39)\n",
            "[英语] 进度：198/500 (+39)\n",
            "[英语] 进度：234/500 (+36)\n",
            "[英语] 进度：274/500 (+40)\n",
            "[英语] 进度：314/500 (+40)\n",
            "[英语] 进度：354/500 (+40)\n",
            "[英语] 进度：394/500 (+40)\n",
            "[英语] 进度：434/500 (+40)\n",
            "[英语] 进度：474/500 (+40)\n",
            "[英语] 进度：500/500 (+26)\n",
            "[丹麦语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_1/丹麦语_time_templates.jsonl\n",
            "[丹麦语] 进度：39/500 (+39)\n",
            "[丹麦语] 进度：79/500 (+40)\n",
            "[丹麦语] 进度：118/500 (+39)\n",
            "[丹麦语] 进度：157/500 (+39)\n",
            "[丹麦语] 进度：197/500 (+40)\n",
            "[丹麦语] 进度：236/500 (+39)\n",
            "[丹麦语] 进度：275/500 (+39)\n",
            "[丹麦语] 进度：315/500 (+40)\n",
            "[丹麦语] 进度：355/500 (+40)\n",
            "[丹麦语] 进度：395/500 (+40)\n",
            "[丹麦语] 进度：434/500 (+39)\n",
            "[丹麦语] 进度：474/500 (+40)\n",
            "[丹麦语] 进度：500/500 (+26)\n",
            "[俄语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_1/俄语_time_templates.jsonl\n",
            "[俄语] 进度：40/500 (+40)\n",
            "[俄语] 进度：80/500 (+40)\n",
            "[俄语] 进度：119/500 (+39)\n",
            "[俄语] 进度：159/500 (+40)\n",
            "[俄语] 进度：198/500 (+39)\n",
            "[俄语] 进度：238/500 (+40)\n",
            "[俄语] 进度：278/500 (+40)\n",
            "[俄语] 进度：318/500 (+40)\n",
            "[俄语] 进度：358/500 (+40)\n",
            "[俄语] 进度：398/500 (+40)\n",
            "[俄语] 进度：437/500 (+39)\n",
            "[俄语] 进度：477/500 (+40)\n",
            "[俄语] 进度：500/500 (+23)\n",
            "[土耳其语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_1/土耳其语_time_templates.jsonl\n",
            "[土耳其语] 进度：40/500 (+40)\n",
            "[土耳其语] 进度：80/500 (+40)\n",
            "[土耳其语] 进度：120/500 (+40)\n",
            "[土耳其语] 进度：159/500 (+39)\n",
            "[土耳其语] 进度：199/500 (+40)\n",
            "[土耳其语] 进度：239/500 (+40)\n",
            "[土耳其语] 进度：279/500 (+40)\n",
            "[土耳其语] 进度：319/500 (+40)\n",
            "[土耳其语] 进度：359/500 (+40)\n",
            "[土耳其语] 进度：398/500 (+39)\n",
            "[土耳其语] 进度：438/500 (+40)\n",
            "[土耳其语] 进度：478/500 (+40)\n",
            "[土耳其语] 进度：500/500 (+22)\n",
            "[德语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_1/德语_time_templates.jsonl\n",
            "[德语] 进度：40/500 (+40)\n",
            "[德语] 进度：79/500 (+39)\n",
            "[德语] 进度：119/500 (+40)\n",
            "[德语] 进度：159/500 (+40)\n",
            "[德语] 进度：199/500 (+40)\n",
            "[德语] 进度：238/500 (+39)\n",
            "[德语] 进度：278/500 (+40)\n",
            "[德语] 进度：318/500 (+40)\n",
            "[德语] 进度：358/500 (+40)\n",
            "[德语] 进度：398/500 (+40)\n",
            "[德语] 进度：438/500 (+40)\n",
            "[德语] 进度：477/500 (+39)\n",
            "[德语] 进度：500/500 (+23)\n",
            "[意大利语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_1/意大利语_time_templates.jsonl\n",
            "[意大利语] 进度：40/500 (+40)\n",
            "[意大利语] 进度：80/500 (+40)\n",
            "[意大利语] 进度：120/500 (+40)\n",
            "[意大利语] 进度：160/500 (+40)\n",
            "[意大利语] 进度：200/500 (+40)\n",
            "[意大利语] 进度：240/500 (+40)\n",
            "[意大利语] 进度：280/500 (+40)\n",
            "[意大利语] 进度：320/500 (+40)\n",
            "[意大利语] 进度：360/500 (+40)\n",
            "[意大利语] 进度：399/500 (+39)\n",
            "[意大利语] 进度：439/500 (+40)\n",
            "[意大利语] 进度：479/500 (+40)\n",
            "[意大利语] 进度：499/500 (+20)\n",
            "[意大利语] 进度：500/500 (+1)\n",
            "[日语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_1/日语_time_templates.jsonl\n",
            "[日语] 进度：40/500 (+40)\n",
            "[日语] 进度：80/500 (+40)\n",
            "[日语] 进度：120/500 (+40)\n",
            "[日语] 进度：159/500 (+39)\n",
            "[日语] 进度：199/500 (+40)\n",
            "[日语] 进度：239/500 (+40)\n",
            "[日语] 进度：279/500 (+40)\n",
            "[日语] 进度：319/500 (+40)\n",
            "[日语] 进度：358/500 (+39)\n",
            "[日语] 进度：398/500 (+40)\n",
            "[日语] 进度：438/500 (+40)\n",
            "[日语] 进度：478/500 (+40)\n",
            "[日语] 进度：500/500 (+22)\n",
            "[法语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_1/法语_time_templates.jsonl\n",
            "[法语] 进度：40/500 (+40)\n",
            "[法语] 进度：80/500 (+40)\n",
            "[法语] 进度：120/500 (+40)\n",
            "[法语] 进度：160/500 (+40)\n",
            "[法语] 进度：200/500 (+40)\n",
            "[法语] 进度：240/500 (+40)\n",
            "[法语] 进度：280/500 (+40)\n",
            "[法语] 进度：320/500 (+40)\n",
            "[法语] 进度：360/500 (+40)\n",
            "[法语] 进度：399/500 (+39)\n",
            "[法语] 进度：439/500 (+40)\n",
            "[法语] 进度：479/500 (+40)\n",
            "[法语] 进度：500/500 (+21)\n",
            "[瑞典语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_1/瑞典语_time_templates.jsonl\n",
            "[瑞典语] 进度：40/500 (+40)\n",
            "[瑞典语] 进度：79/500 (+39)\n",
            "[瑞典语] 进度：119/500 (+40)\n",
            "[瑞典语] 进度：159/500 (+40)\n",
            "[瑞典语] 进度：199/500 (+40)\n",
            "[瑞典语] 进度：238/500 (+39)\n",
            "[瑞典语] 进度：278/500 (+40)\n",
            "[瑞典语] 进度：318/500 (+40)\n",
            "[瑞典语] 进度：358/500 (+40)\n",
            "[瑞典语] 进度：398/500 (+40)\n",
            "[瑞典语] 进度：437/500 (+39)\n",
            "[瑞典语] 进度：476/500 (+39)\n",
            "[瑞典语] 进度：500/500 (+24)\n",
            "[荷兰语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_1/荷兰语_time_templates.jsonl\n",
            "[荷兰语] 进度：39/500 (+39)\n",
            "[荷兰语] 进度：78/500 (+39)\n",
            "[荷兰语] 进度：118/500 (+40)\n",
            "[荷兰语] 进度：158/500 (+40)\n",
            "[荷兰语] 进度：198/500 (+40)\n",
            "[荷兰语] 进度：238/500 (+40)\n",
            "[荷兰语] 进度：278/500 (+40)\n",
            "[荷兰语] 进度：318/500 (+40)\n",
            "[荷兰语] 进度：358/500 (+40)\n",
            "[荷兰语] 进度：398/500 (+40)\n",
            "[荷兰语] 进度：438/500 (+40)\n",
            "[荷兰语] 进度：478/500 (+40)\n",
            "[荷兰语] 进度：500/500 (+22)\n",
            "[葡萄牙语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_1/葡萄牙语_time_templates.jsonl\n",
            "[葡萄牙语] 进度：40/500 (+40)\n",
            "[葡萄牙语] 进度：80/500 (+40)\n",
            "[葡萄牙语] 进度：120/500 (+40)\n",
            "[葡萄牙语] 进度：160/500 (+40)\n",
            "[葡萄牙语] 进度：200/500 (+40)\n",
            "[葡萄牙语] 进度：240/500 (+40)\n",
            "[葡萄牙语] 进度：280/500 (+40)\n",
            "[葡萄牙语] 进度：320/500 (+40)\n",
            "[葡萄牙语] 进度：359/500 (+39)\n",
            "[葡萄牙语] 进度：399/500 (+40)\n",
            "[葡萄牙语] 进度：439/500 (+40)\n",
            "[葡萄牙语] 进度：479/500 (+40)\n",
            "[葡萄牙语] 进度：500/500 (+21)\n",
            "[西班牙语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_1/西班牙语_time_templates.jsonl\n",
            "[西班牙语] 进度：39/500 (+39)\n",
            "[西班牙语] 进度：79/500 (+40)\n",
            "[西班牙语] 进度：119/500 (+40)\n",
            "[西班牙语] 进度：157/500 (+38)\n",
            "[西班牙语] 进度：197/500 (+40)\n",
            "[西班牙语] 进度：237/500 (+40)\n",
            "[西班牙语] 进度：277/500 (+40)\n",
            "[西班牙语] 进度：317/500 (+40)\n",
            "[西班牙语] 进度：357/500 (+40)\n",
            "[西班牙语] 进度：396/500 (+39)\n",
            "[西班牙语] 进度：436/500 (+40)\n",
            "[西班牙语] 进度：476/500 (+40)\n",
            "[西班牙语] 进度：500/500 (+24)\n",
            "[韩语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_1/韩语_time_templates.jsonl\n",
            "[韩语] 进度：40/500 (+40)\n",
            "[韩语] 进度：80/500 (+40)\n",
            "[韩语] 进度：120/500 (+40)\n",
            "[韩语] 进度：160/500 (+40)\n",
            "[韩语] 进度：200/500 (+40)\n",
            "[韩语] 进度：240/500 (+40)\n",
            "[韩语] 进度：280/500 (+40)\n",
            "[韩语] 进度：319/500 (+39)\n",
            "[韩语] 进度：359/500 (+40)\n",
            "[韩语] 进度：399/500 (+40)\n",
            "[韩语] 进度：438/500 (+39)\n",
            "[韩语] 进度：478/500 (+40)\n",
            "[韩语] 进度：500/500 (+22)\n",
            "============ 开始：time数量 2（每语言目标 500 条） ============\n",
            "[简体中文] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_2/简体中文_time_templates.jsonl\n",
            "[简体中文] 进度：35/500 (+35)\n",
            "[简体中文] 进度：75/500 (+40)\n",
            "[简体中文] 进度：114/500 (+39)\n",
            "[简体中文] 进度：154/500 (+40)\n",
            "[简体中文] 进度：193/500 (+39)\n",
            "[简体中文] 进度：232/500 (+39)\n",
            "[简体中文] 进度：272/500 (+40)\n",
            "[简体中文] 进度：312/500 (+40)\n",
            "[简体中文] 进度：352/500 (+40)\n",
            "[简体中文] 进度：391/500 (+39)\n",
            "[简体中文] 进度：431/500 (+40)\n",
            "[简体中文] 进度：471/500 (+40)\n",
            "[简体中文] 进度：500/500 (+29)\n",
            "[繁体中文] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_2/繁体中文_time_templates.jsonl\n",
            "[繁体中文] 进度：40/500 (+40)\n",
            "[繁体中文] 进度：79/500 (+39)\n",
            "[繁体中文] 进度：119/500 (+40)\n",
            "[繁体中文] 进度：159/500 (+40)\n",
            "[繁体中文] 进度：199/500 (+40)\n",
            "[繁体中文] 进度：238/500 (+39)\n",
            "[繁体中文] 进度：277/500 (+39)\n",
            "[繁体中文] 进度：317/500 (+40)\n",
            "[繁体中文] 进度：357/500 (+40)\n",
            "[繁体中文] 进度：397/500 (+40)\n",
            "[繁体中文] 进度：437/500 (+40)\n",
            "[繁体中文] 进度：477/500 (+40)\n",
            "[繁体中文] 进度：500/500 (+23)\n",
            "[英语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_2/英语_time_templates.jsonl\n",
            "[英语] 进度：40/500 (+40)\n",
            "[英语] 进度：80/500 (+40)\n",
            "[英语] 进度：120/500 (+40)\n",
            "[英语] 进度：160/500 (+40)\n",
            "[英语] 进度：200/500 (+40)\n",
            "[英语] 进度：240/500 (+40)\n",
            "[英语] 进度：280/500 (+40)\n",
            "[英语] 进度：320/500 (+40)\n",
            "[英语] 进度：360/500 (+40)\n",
            "[英语] 进度：400/500 (+40)\n",
            "[英语] 进度：440/500 (+40)\n",
            "[英语] 进度：480/500 (+40)\n",
            "[英语] 进度：500/500 (+20)\n",
            "[丹麦语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_2/丹麦语_time_templates.jsonl\n",
            "[丹麦语] 进度：40/500 (+40)\n",
            "[丹麦语] 进度：80/500 (+40)\n",
            "[丹麦语] 进度：119/500 (+39)\n",
            "[丹麦语] 进度：159/500 (+40)\n",
            "[丹麦语] 进度：199/500 (+40)\n",
            "[丹麦语] 进度：239/500 (+40)\n",
            "[丹麦语] 进度：279/500 (+40)\n",
            "[丹麦语] 进度：319/500 (+40)\n",
            "[丹麦语] 进度：358/500 (+39)\n",
            "[丹麦语] 进度：397/500 (+39)\n",
            "[丹麦语] 进度：437/500 (+40)\n",
            "[丹麦语] 进度：477/500 (+40)\n",
            "[丹麦语] 进度：500/500 (+23)\n",
            "[俄语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_2/俄语_time_templates.jsonl\n",
            "[俄语] 进度：39/500 (+39)\n",
            "[俄语] 进度：79/500 (+40)\n",
            "[俄语] 进度：118/500 (+39)\n",
            "[俄语] 进度：157/500 (+39)\n",
            "[俄语] 进度：197/500 (+40)\n",
            "[俄语] 进度：237/500 (+40)\n",
            "[俄语] 进度：276/500 (+39)\n",
            "[俄语] 进度：315/500 (+39)\n",
            "[俄语] 进度：355/500 (+40)\n",
            "[俄语] 进度：395/500 (+40)\n",
            "[俄语] 进度：435/500 (+40)\n",
            "[俄语] 进度：475/500 (+40)\n",
            "[俄语] 进度：500/500 (+25)\n",
            "[土耳其语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_2/土耳其语_time_templates.jsonl\n",
            "[土耳其语] 进度：40/500 (+40)\n",
            "[土耳其语] 进度：79/500 (+39)\n",
            "[土耳其语] 进度：118/500 (+39)\n",
            "[土耳其语] 进度：158/500 (+40)\n",
            "[土耳其语] 进度：198/500 (+40)\n",
            "[土耳其语] 进度：238/500 (+40)\n",
            "[土耳其语] 进度：278/500 (+40)\n",
            "[土耳其语] 进度：318/500 (+40)\n",
            "[土耳其语] 进度：356/500 (+38)\n",
            "[土耳其语] 进度：395/500 (+39)\n",
            "[土耳其语] 进度：434/500 (+39)\n",
            "[土耳其语] 进度：474/500 (+40)\n",
            "[土耳其语] 进度：500/500 (+26)\n",
            "[德语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_2/德语_time_templates.jsonl\n",
            "[德语] 进度：39/500 (+39)\n",
            "[德语] 进度：79/500 (+40)\n",
            "[德语] 进度：119/500 (+40)\n",
            "[德语] 进度：158/500 (+39)\n",
            "[德语] 进度：198/500 (+40)\n",
            "[德语] 进度：238/500 (+40)\n",
            "[德语] 进度：278/500 (+40)\n",
            "[德语] 进度：317/500 (+39)\n",
            "[德语] 进度：357/500 (+40)\n",
            "[德语] 进度：397/500 (+40)\n",
            "[德语] 进度：437/500 (+40)\n",
            "[德语] 进度：477/500 (+40)\n",
            "[德语] 进度：500/500 (+23)\n",
            "[意大利语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_2/意大利语_time_templates.jsonl\n",
            "[意大利语] 进度：40/500 (+40)\n",
            "[意大利语] 进度：80/500 (+40)\n",
            "[意大利语] 进度：119/500 (+39)\n",
            "[意大利语] 进度：159/500 (+40)\n",
            "[意大利语] 进度：198/500 (+39)\n",
            "[意大利语] 进度：237/500 (+39)\n",
            "[意大利语] 进度：277/500 (+40)\n",
            "[意大利语] 进度：316/500 (+39)\n",
            "[意大利语] 进度：349/500 (+33)\n",
            "[意大利语] 进度：387/500 (+38)\n",
            "[意大利语] 进度：427/500 (+40)\n",
            "[意大利语] 进度：466/500 (+39)\n",
            "[意大利语] 进度：499/500 (+33)\n",
            "[意大利语] 进度：500/500 (+1)\n",
            "[日语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_2/日语_time_templates.jsonl\n",
            "[日语] 进度：38/500 (+38)\n",
            "[日语] 进度：44/500 (+6)\n",
            "[日语] 进度：84/500 (+40)\n",
            "[日语] 进度：123/500 (+39)\n",
            "[日语] 进度：160/500 (+37)\n",
            "[日语] 进度：183/500 (+23)\n",
            "[日语] 进度：223/500 (+40)\n",
            "[日语] 进度：261/500 (+38)\n",
            "[日语] 进度：276/500 (+15)\n",
            "[日语] 进度：315/500 (+39)\n",
            "[日语] 进度：355/500 (+40)\n",
            "[日语] 进度：369/500 (+14)\n",
            "[日语] 进度：409/500 (+40)\n",
            "[日语] 进度：448/500 (+39)\n",
            "[日语] 进度：486/500 (+38)\n",
            "[日语] 进度：500/500 (+14)\n",
            "[法语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_2/法语_time_templates.jsonl\n",
            "[法语] 进度：40/500 (+40)\n",
            "[法语] 进度：80/500 (+40)\n",
            "[法语] 进度：120/500 (+40)\n",
            "[法语] 进度：160/500 (+40)\n",
            "[法语] 进度：200/500 (+40)\n",
            "[法语] 进度：240/500 (+40)\n",
            "[法语] 进度：280/500 (+40)\n",
            "[法语] 进度：320/500 (+40)\n",
            "[法语] 进度：360/500 (+40)\n",
            "[法语] 进度：399/500 (+39)\n",
            "[法语] 进度：439/500 (+40)\n",
            "[法语] 进度：479/500 (+40)\n",
            "[法语] 进度：500/500 (+21)\n",
            "[瑞典语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_2/瑞典语_time_templates.jsonl\n",
            "[瑞典语] 进度：40/500 (+40)\n",
            "[瑞典语] 进度：80/500 (+40)\n",
            "[瑞典语] 进度：120/500 (+40)\n",
            "[瑞典语] 进度：160/500 (+40)\n",
            "[瑞典语] 进度：200/500 (+40)\n",
            "[瑞典语] 进度：240/500 (+40)\n",
            "[瑞典语] 进度：280/500 (+40)\n",
            "[瑞典语] 进度：320/500 (+40)\n",
            "[瑞典语] 进度：359/500 (+39)\n",
            "[瑞典语] 进度：398/500 (+39)\n",
            "[瑞典语] 进度：438/500 (+40)\n",
            "[瑞典语] 进度：477/500 (+39)\n",
            "[瑞典语] 进度：500/500 (+23)\n",
            "[荷兰语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_2/荷兰语_time_templates.jsonl\n",
            "[荷兰语] 进度：40/500 (+40)\n",
            "[荷兰语] 进度：80/500 (+40)\n",
            "[荷兰语] 进度：120/500 (+40)\n",
            "[荷兰语] 进度：160/500 (+40)\n",
            "[荷兰语] 进度：199/500 (+39)\n",
            "[荷兰语] 进度：238/500 (+39)\n",
            "[荷兰语] 进度：276/500 (+38)\n",
            "[荷兰语] 进度：316/500 (+40)\n",
            "[荷兰语] 进度：356/500 (+40)\n",
            "[荷兰语] 进度：396/500 (+40)\n",
            "[荷兰语] 进度：436/500 (+40)\n",
            "[荷兰语] 进度：475/500 (+39)\n",
            "[荷兰语] 进度：500/500 (+25)\n",
            "[葡萄牙语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_2/葡萄牙语_time_templates.jsonl\n",
            "[葡萄牙语] 进度：40/500 (+40)\n",
            "[葡萄牙语] 进度：79/500 (+39)\n",
            "[葡萄牙语] 进度：119/500 (+40)\n",
            "[葡萄牙语] 进度：159/500 (+40)\n",
            "[葡萄牙语] 进度：199/500 (+40)\n",
            "[葡萄牙语] 进度：238/500 (+39)\n",
            "[葡萄牙语] 进度：278/500 (+40)\n",
            "[葡萄牙语] 进度：318/500 (+40)\n",
            "[葡萄牙语] 进度：358/500 (+40)\n",
            "[葡萄牙语] 进度：398/500 (+40)\n",
            "[葡萄牙语] 进度：438/500 (+40)\n",
            "[葡萄牙语] 进度：478/500 (+40)\n",
            "[葡萄牙语] 进度：500/500 (+22)\n",
            "[西班牙语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_2/西班牙语_time_templates.jsonl\n",
            "[西班牙语] 进度：40/500 (+40)\n",
            "[西班牙语] 进度：80/500 (+40)\n",
            "[西班牙语] 进度：120/500 (+40)\n",
            "[西班牙语] 进度：160/500 (+40)\n",
            "[西班牙语] 进度：200/500 (+40)\n",
            "[西班牙语] 进度：240/500 (+40)\n",
            "[西班牙语] 进度：280/500 (+40)\n",
            "[西班牙语] 进度：320/500 (+40)\n",
            "[西班牙语] 进度：360/500 (+40)\n",
            "[西班牙语] 进度：400/500 (+40)\n",
            "[西班牙语] 进度：440/500 (+40)\n",
            "[西班牙语] 进度：480/500 (+40)\n",
            "[西班牙语] 进度：500/500 (+20)\n",
            "[韩语] 目标 500 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_2/韩语_time_templates.jsonl\n",
            "[韩语] 进度：40/500 (+40)\n",
            "[韩语] 进度：80/500 (+40)\n",
            "[韩语] 进度：119/500 (+39)\n",
            "[韩语] 进度：159/500 (+40)\n",
            "[韩语] 进度：199/500 (+40)\n",
            "[韩语] 进度：238/500 (+39)\n",
            "[韩语] 进度：278/500 (+40)\n",
            "[韩语] 进度：317/500 (+39)\n",
            "[韩语] 进度：357/500 (+40)\n",
            "[韩语] 进度：397/500 (+40)\n",
            "[韩语] 进度：437/500 (+40)\n",
            "[韩语] 进度：475/500 (+38)\n",
            "[韩语] 进度：500/500 (+25)\n",
            "============ 开始：time数量 3（每语言目标 375 条） ============\n",
            "[简体中文] 目标 375 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_3/简体中文_time_templates.jsonl\n",
            "[简体中文] 进度：38/375 (+38)\n",
            "[简体中文] 进度：78/375 (+40)\n",
            "[简体中文] 进度：117/375 (+39)\n",
            "[简体中文] 进度：157/375 (+40)\n",
            "[简体中文] 进度：197/375 (+40)\n",
            "[简体中文] 进度：237/375 (+40)\n",
            "[简体中文] 进度：277/375 (+40)\n",
            "[简体中文] 进度：316/375 (+39)\n",
            "[简体中文] 进度：356/375 (+40)\n",
            "[简体中文] 进度：375/375 (+19)\n",
            "[繁体中文] 目标 375 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_3/繁体中文_time_templates.jsonl\n",
            "[繁体中文] 进度：40/375 (+40)\n",
            "[繁体中文] 进度：80/375 (+40)\n",
            "[繁体中文] 进度：120/375 (+40)\n",
            "[繁体中文] 进度：157/375 (+37)\n",
            "[繁体中文] 进度：196/375 (+39)\n",
            "[繁体中文] 进度：231/375 (+35)\n",
            "[繁体中文] 进度：270/375 (+39)\n",
            "[繁体中文] 进度：310/375 (+40)\n",
            "[繁体中文] 进度：348/375 (+38)\n",
            "[繁体中文] 进度：375/375 (+27)\n",
            "[英语] 目标 375 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_3/英语_time_templates.jsonl\n",
            "[英语] 进度：1/375 (+1)\n",
            "[英语] 进度：41/375 (+40)\n",
            "[英语] 进度：81/375 (+40)\n",
            "[英语] 进度：121/375 (+40)\n",
            "[英语] 进度：161/375 (+40)\n",
            "[英语] 进度：201/375 (+40)\n",
            "[英语] 进度：240/375 (+39)\n",
            "[英语] 进度：242/375 (+2)\n",
            "[英语] 进度：282/375 (+40)\n",
            "[英语] 进度：322/375 (+40)\n",
            "[英语] 进度：362/375 (+40)\n",
            "[英语] 进度：374/375 (+12)\n",
            "[英语] 进度：375/375 (+1)\n",
            "[丹麦语] 目标 375 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_3/丹麦语_time_templates.jsonl\n",
            "[丹麦语] 进度：40/375 (+40)\n",
            "[丹麦语] 进度：45/375 (+5)\n",
            "[丹麦语] 进度：47/375 (+2)\n",
            "[丹麦语] 进度：49/375 (+2)\n",
            "[丹麦语] 进度：52/375 (+3)\n",
            "[丹麦语] 进度：53/375 (+1)\n",
            "[丹麦语] 进度：93/375 (+40)\n",
            "[丹麦语] 进度：100/375 (+7)\n",
            "[丹麦语] 进度：140/375 (+40)\n",
            "[丹麦语] 进度：147/375 (+7)\n",
            "[丹麦语] 进度：187/375 (+40)\n",
            "[丹麦语] 进度：227/375 (+40)\n",
            "[丹麦语] 进度：229/375 (+2)\n",
            "[丹麦语] 进度：231/375 (+2)\n",
            "[丹麦语] 进度：270/375 (+39)\n",
            "[丹麦语] 进度：310/375 (+40)\n",
            "[丹麦语] 进度：315/375 (+5)\n",
            "[丹麦语] 进度：322/375 (+7)\n",
            "[丹麦语] 进度：361/375 (+39)\n",
            "[丹麦语] 进度：375/375 (+14)\n",
            "[俄语] 目标 375 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_3/俄语_time_templates.jsonl\n",
            "[俄语] 进度：40/375 (+40)\n",
            "[俄语] 进度：80/375 (+40)\n",
            "[俄语] 进度：120/375 (+40)\n",
            "[俄语] 进度：160/375 (+40)\n",
            "[俄语] 进度：200/375 (+40)\n",
            "[俄语] 进度：240/375 (+40)\n",
            "[俄语] 进度：241/375 (+1)\n",
            "[俄语] 进度：281/375 (+40)\n",
            "[俄语] 进度：317/375 (+36)\n",
            "[俄语] 进度：357/375 (+40)\n",
            "[俄语] 进度：375/375 (+18)\n",
            "[土耳其语] 目标 375 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_3/土耳其语_time_templates.jsonl\n",
            "[土耳其语] 进度：40/375 (+40)\n",
            "[土耳其语] 进度：80/375 (+40)\n",
            "[土耳其语] 进度：119/375 (+39)\n",
            "[土耳其语] 进度：159/375 (+40)\n",
            "[土耳其语] 进度：199/375 (+40)\n",
            "[土耳其语] 进度：239/375 (+40)\n",
            "[土耳其语] 进度：279/375 (+40)\n",
            "[土耳其语] 进度：315/375 (+36)\n",
            "[土耳其语] 进度：355/375 (+40)\n",
            "[土耳其语] 进度：375/375 (+20)\n",
            "[德语] 目标 375 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_3/德语_time_templates.jsonl\n",
            "[德语] 进度：16/375 (+16)\n",
            "[德语] 进度：56/375 (+40)\n",
            "[德语] 进度：95/375 (+39)\n",
            "[德语] 进度：135/375 (+40)\n",
            "[德语] 进度：175/375 (+40)\n",
            "[德语] 进度：214/375 (+39)\n",
            "[德语] 进度：254/375 (+40)\n",
            "[德语] 进度：261/375 (+7)\n",
            "[德语] 进度：301/375 (+40)\n",
            "[德语] 进度：307/375 (+6)\n",
            "[德语] 进度：311/375 (+4)\n",
            "[德语] 进度：351/375 (+40)\n",
            "[德语] 进度：367/375 (+16)\n",
            "[德语] 进度：373/375 (+6)\n",
            "[德语] 进度：375/375 (+2)\n",
            "[意大利语] 目标 375 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_3/意大利语_time_templates.jsonl\n",
            "[意大利语] 进度：40/375 (+40)\n",
            "[意大利语] 进度：41/375 (+1)\n",
            "[意大利语] 进度：81/375 (+40)\n",
            "[意大利语] 进度：86/375 (+5)\n",
            "[意大利语] 进度：126/375 (+40)\n",
            "[意大利语] 进度：142/375 (+16)\n",
            "[意大利语] 进度：182/375 (+40)\n",
            "[意大利语] 进度：222/375 (+40)\n",
            "[意大利语] 进度：262/375 (+40)\n",
            "[意大利语] 进度：302/375 (+40)\n",
            "[意大利语] 进度：342/375 (+40)\n",
            "[意大利语] 进度：375/375 (+33)\n",
            "[日语] 目标 375 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_3/日语_time_templates.jsonl\n",
            "[日语] 进度：38/375 (+38)\n",
            "[日语] 进度：77/375 (+39)\n",
            "[日语] 进度：82/375 (+5)\n",
            "[日语] 进度：84/375 (+2)\n",
            "[日语] 进度：87/375 (+3)\n",
            "[日语] 进度：88/375 (+1)\n",
            "[日语] 进度：101/375 (+13)\n",
            "[日语] 进度：116/375 (+15)\n",
            "[日语] 进度：133/375 (+17)\n",
            "[日语] 进度：134/375 (+1)\n",
            "[日语] 进度：173/375 (+39)\n",
            "[法语] 目标 375 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_3/法语_time_templates.jsonl\n",
            "[法语] 进度：40/375 (+40)\n",
            "[法语] 进度：80/375 (+40)\n",
            "[法语] 进度：120/375 (+40)\n",
            "[法语] 进度：160/375 (+40)\n",
            "[法语] 进度：162/375 (+2)\n",
            "[法语] 进度：202/375 (+40)\n",
            "[法语] 进度：242/375 (+40)\n",
            "[法语] 进度：282/375 (+40)\n",
            "[法语] 进度：322/375 (+40)\n",
            "[法语] 进度：362/375 (+40)\n",
            "[法语] 进度：375/375 (+13)\n",
            "[瑞典语] 目标 375 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_3/瑞典语_time_templates.jsonl\n",
            "[瑞典语] 进度：39/375 (+39)\n",
            "[瑞典语] 进度：79/375 (+40)\n",
            "[瑞典语] 进度：118/375 (+39)\n",
            "[瑞典语] 进度：157/375 (+39)\n",
            "[瑞典语] 进度：158/375 (+1)\n",
            "[瑞典语] 进度：197/375 (+39)\n",
            "[瑞典语] 进度：214/375 (+17)\n",
            "[瑞典语] 进度：253/375 (+39)\n",
            "[瑞典语] 进度：292/375 (+39)\n",
            "[瑞典语] 进度：332/375 (+40)\n",
            "[瑞典语] 进度：372/375 (+40)\n",
            "[瑞典语] 进度：375/375 (+3)\n",
            "[荷兰语] 目标 375 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_3/荷兰语_time_templates.jsonl\n",
            "[荷兰语] 进度：40/375 (+40)\n",
            "[荷兰语] 进度：80/375 (+40)\n",
            "[荷兰语] 进度：120/375 (+40)\n",
            "[荷兰语] 进度：159/375 (+39)\n",
            "[荷兰语] 进度：199/375 (+40)\n",
            "[荷兰语] 进度：239/375 (+40)\n",
            "[荷兰语] 进度：277/375 (+38)\n",
            "[荷兰语] 进度：317/375 (+40)\n",
            "[荷兰语] 进度：357/375 (+40)\n",
            "[荷兰语] 进度：375/375 (+18)\n",
            "[葡萄牙语] 目标 375 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_3/葡萄牙语_time_templates.jsonl\n",
            "[葡萄牙语] 进度：33/375 (+33)\n",
            "[葡萄牙语] 进度：73/375 (+40)\n",
            "[葡萄牙语] 进度：113/375 (+40)\n",
            "[葡萄牙语] 进度：153/375 (+40)\n",
            "[葡萄牙语] 进度：193/375 (+40)\n",
            "[葡萄牙语] 进度：200/375 (+7)\n",
            "[葡萄牙语] 进度：240/375 (+40)\n",
            "[葡萄牙语] 进度：280/375 (+40)\n",
            "[葡萄牙语] 进度：320/375 (+40)\n",
            "[葡萄牙语] 进度：360/375 (+40)\n",
            "[葡萄牙语] 进度：375/375 (+15)\n",
            "[西班牙语] 目标 375 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_3/西班牙语_time_templates.jsonl\n",
            "[西班牙语] 进度：40/375 (+40)\n",
            "[西班牙语] 进度：80/375 (+40)\n",
            "[西班牙语] 进度：120/375 (+40)\n",
            "[西班牙语] 进度：158/375 (+38)\n",
            "[西班牙语] 进度：198/375 (+40)\n",
            "[西班牙语] 进度：238/375 (+40)\n",
            "[西班牙语] 进度：278/375 (+40)\n",
            "[西班牙语] 进度：318/375 (+40)\n",
            "[西班牙语] 进度：358/375 (+40)\n",
            "[西班牙语] 进度：375/375 (+17)\n",
            "[韩语] 目标 375 条，已有 0 条 → /content/drive/MyDrive/NER/time_templates/time_templates_3/韩语_time_templates.jsonl\n",
            "[韩语] 进度：40/375 (+40)\n",
            "[韩语] 进度：80/375 (+40)\n",
            "[韩语] 进度：120/375 (+40)\n",
            "[韩语] 进度：140/375 (+20)\n",
            "[韩语] 进度：180/375 (+40)\n",
            "[韩语] 进度：220/375 (+40)\n",
            "[韩语] 进度：260/375 (+40)\n",
            "[韩语] 进度：285/375 (+25)\n",
            "[韩语] 进度：325/375 (+40)\n",
            "[韩语] 进度：365/375 (+40)\n",
            "[韩语] 进度：375/375 (+10)\n",
            "全部时间模板生成完成！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 生成测试用负例（纯时间）"
      ],
      "metadata": {
        "id": "Xj3Vq-IRLmp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 输出目录\n",
        "output_dir = \"/content/drive/MyDrive/NER/non_time\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# 各语言模糊时间词\n",
        "translations = {\n",
        "    \"简体中文\": \"最近\",\n",
        "    \"繁体中文\": \"最近\",\n",
        "    \"英语\": \"soon\",\n",
        "    \"丹麦语\": \"snart\",\n",
        "    \"俄语\": \"скоро\",\n",
        "    \"土耳其语\": \"yakında\",\n",
        "    \"德语\": \"bald\",\n",
        "    \"意大利语\": \"presto\",\n",
        "    \"日语\": \"近いうちに\",\n",
        "    \"法语\": \"bientôt\",\n",
        "    \"瑞典语\": \"snart\",\n",
        "    \"荷兰语\": \"binnenkort\",\n",
        "    \"葡萄牙语\": \"em breve\",\n",
        "    \"西班牙语\": \"pronto\",\n",
        "    \"韩语\": \"곧\",\n",
        "}\n",
        "\n",
        "# 其他一些模糊时间词（每语言10条可以变化）\n",
        "extra_fuzzy = {\n",
        "    \"简体中文\": [\"稍后\", \"过一阵子\", \"以后\", \"不久之后\", \"近来\", \"早晚\", \"迟早\", \"随后\", \"日后\", \"以后再说\"],\n",
        "    \"繁体中文\": [\"稍後\", \"過一陣子\", \"以後\", \"不久之後\", \"近來\", \"早晚\", \"遲早\", \"隨後\", \"日後\", \"以後再說\"],\n",
        "    \"英语\": [\"later\", \"after a while\", \"eventually\", \"someday\", \"in the near future\", \"soon enough\", \"at some point\", \"in due time\", \"before long\", \"down the road\"],\n",
        "    \"丹麦语\": [\"senere\", \"efter et stykke tid\", \"på et tidspunkt\", \"snart nok\", \"inden længe\", \"engang\", \"i den nærmeste fremtid\", \"før eller siden\", \"på sigt\", \"en dag\"],\n",
        "    \"俄语\": [\"позже\", \"через какое-то время\", \"рано или поздно\", \"в ближайшем будущем\", \"когда-нибудь\", \"в скоро времени\", \"на днях\", \"со временем\", \"в будущем\", \"не сразу\"],\n",
        "    \"土耳其语\": [\"daha sonra\", \"bir süre sonra\", \"yakın zamanda\", \"eninde sonunda\", \"ileride\", \"bir gün\", \"vaktinde\", \"çok geçmeden\", \"bir zamanlar\", \"sonunda\"],\n",
        "    \"德语\": [\"später\", \"nach einer Weile\", \"irgendwann\", \"bald genug\", \"in naher Zukunft\", \"früher oder später\", \"eines Tages\", \"mit der Zeit\", \"kurz darauf\", \"demnächst\"],\n",
        "    \"意大利语\": [\"più tardi\", \"fra un po'\", \"alla fine\", \"un giorno\", \"in futuro\", \"presto o tardi\", \"col tempo\", \"entro breve\", \"appena possibile\", \"successivamente\"],\n",
        "    \"日语\": [\"後で\", \"しばらくして\", \"いずれ\", \"近いうちに\", \"やがて\", \"そのうち\", \"今後\", \"後ほど\", \"ほどなく\", \"いつか\"],\n",
        "    \"法语\": [\"plus tard\", \"après un certain temps\", \"un jour\", \"dans un avenir proche\", \"tôt ou tard\", \"à la longue\", \"au fil du temps\", \"bientôt\", \"ultérieurement\", \"à un moment donné\"],\n",
        "    \"瑞典语\": [\"senare\", \"efter ett tag\", \"någon gång\", \"förr eller senare\", \"snart nog\", \"i framtiden\", \"en dag\", \"så småningom\", \"i sinom tid\", \"inom kort\"],\n",
        "    \"荷兰语\": [\"later\", \"na een tijdje\", \"ooit\", \"vroeg of laat\", \"binnenkort\", \"in de nabije toekomst\", \"mettertijd\", \"tegen die tijd\", \"op termijn\", \"eens\"],\n",
        "    \"葡萄牙语\": [\"mais tarde\", \"daqui a pouco\", \"algum dia\", \"no futuro\", \"em breve\", \"cedo ou tarde\", \"com o tempo\", \"logo depois\", \"no devido tempo\", \"eventualmente\"],\n",
        "    \"西班牙语\": [\"más tarde\", \"al cabo de un rato\", \"algún día\", \"en un futuro cercano\", \"pronto\", \"tarde o temprano\", \"con el tiempo\", \"dentro de poco\", \"en su momento\", \"eventualmente\"],\n",
        "    \"韩语\": [\"나중에\", \"잠시 후\", \"언젠가\", \"곧\", \"조만간\", \"서서히\", \"머지않아\", \"후에\", \"차후에\", \"훗날\"],\n",
        "}\n",
        "\n",
        "def generate_files():\n",
        "    for lang, phrase in translations.items():\n",
        "        filepath = os.path.join(output_dir, f\"{lang}.txt\")\n",
        "        # 前一个固定词 + 后面 extra_fuzzy 结合\n",
        "        fuzzy_list = [phrase] + extra_fuzzy.get(lang, [])\n",
        "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "            for word in fuzzy_list:\n",
        "                f.write(word + \"\\n\")\n",
        "        print(f\"生成文件: {filepath}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate_files()"
      ],
      "metadata": {
        "id": "-pw5OpVEL0q2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c0f5432-017e-409e-fc1b-3cb8a188c4c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "生成文件: /content/drive/MyDrive/NER/non_time/简体中文.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_time/繁体中文.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_time/英语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_time/丹麦语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_time/俄语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_time/土耳其语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_time/德语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_time/意大利语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_time/日语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_time/法语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_time/瑞典语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_time/荷兰语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_time/葡萄牙语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_time/西班牙语.txt\n",
            "生成文件: /content/drive/MyDrive/NER/non_time/韩语.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 混合模版生成器（时间0—3/地点0—3）（时间插入地点）"
      ],
      "metadata": {
        "id": "QTPDU2AVL2Rn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**本段落主要用于从最小元数据结构开始**以{address}和{time}为主要元素向大模型发送请求，做组词造句式模板，生成结构与之前两段模板生成器完全一致。但本段代码采用：**“创建正确格式文件夹来选择性生成你需要数据”**的数据控制方式。第一次运行时数据分布安排如下（尾号分别代表address个数和time个数，如01代表0个address，1个time的句子）：\n",
        "\n",
        "\n",
        "---\n",
        "分布矩阵如下：   \n",
        "*   0 1  2  3\n",
        "*   0 1600 500 500 500\n",
        "*   1 500 4000 500 500\n",
        "*   2 500 500 500 500\n",
        "*   3 500 500 500 500\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ECLDpaBT2Eli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import asyncio\n",
        "from typing import List, Dict, Any, Optional\n",
        "import httpx\n",
        "from google.colab import userdata\n",
        "\n",
        "# API Key和接口配置\n",
        "api_key = userdata.get('api_key')\n",
        "BASE_URL   = os.getenv(\"LLM_BASE_URL\", \"https://test-turing.cn.llm.tcljd.com\")\n",
        "ENDPOINT   = os.getenv(\"LLM_ENDPOINT\", \"/api/v1/chat/completions\")\n",
        "API_KEY    = os.getenv(\"LLM_API_KEY\", api_key)\n",
        "HEADERS    = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
        "\n",
        "# 输出根目录\n",
        "OUTPUT_BASE_DIR  = \"/content/drive/MyDrive/NER/combine_templates\"\n",
        "\n",
        "LANGUAGES = [\n",
        "    \"简体中文\", \"繁体中文\", \"英语\", \"丹麦语\", \"俄语\", \"土耳其语\", \"德语\",\n",
        "    \"意大利语\", \"日语\", \"法语\", \"瑞典语\", \"荷兰语\", \"葡萄牙语\", \"西班牙语\", \"韩语\"\n",
        "]\n",
        "\n",
        "TEMPLATE_BATCH_SIZE = 40\n",
        "TEMPERATURE = 0.8\n",
        "MODEL_NAME = \"turing/gpt-4o\"\n",
        "TIMEOUT = 90\n",
        "\n",
        "client: Optional[httpx.AsyncClient] = None\n",
        "\n",
        "# 西文介词黑名单（禁止出现在 {time} 前）\n",
        "PREPOSITION_BLACKLIST = {\n",
        "    \"英语\": [\"at\", \"by\", \"on\", \"before\", \"after\"],\n",
        "    \"西班牙语\": [\"a\", \"para\", \"en\", \"por\", \"antes de\", \"después de\", \"hacia\", \"sobre\"],\n",
        "    \"葡萄牙语\": [\"a\", \"para\", \"em\", \"por\", \"antes de\", \"depois de\", \"às\", \"à\"],\n",
        "    \"意大利语\": [\"a\", \"per\", \"in\", \"su\", \"prima di\", \"dopo\"]\n",
        "}\n",
        "\n",
        "# 演示样例（多语言）\n",
        "SAMPLE_PAIRS = {\n",
        "    \"简体中文\": [\n",
        "        \"我{time}打算去{address}看看，听说那边的{address}景色很迷人。\",\n",
        "        \"{time}我们将在{address}集合，并一同前往{address}参加活动。\",\n",
        "        \"传闻{address}与{address}相连的步道在{time}最适合拍照。\",\n",
        "        \"我喜欢在{time}去{address}散步，然后顺道去{address}吃晚餐。\",\n",
        "        \"{address}位于{address}，在{time}举办了盛大的灯会。\",\n",
        "        \"{time}刚吃完晚饭就去了{address}散步，到了{time}又和朋友在{address}看电影。\"\n",
        "    ],\n",
        "    \"繁体中文\": [\n",
        "        \"我{time}會去{address}走走，聽朋友說{address}的風景非常漂亮。\",\n",
        "        \"{time}我們約在{address}碰面，再一起去{address}逛逛。\",\n",
        "        \"聽說位於{address}的{address}在{time}非常熱鬧。\",\n",
        "        \"我喜歡{time}到{address}喝茶，順便去{address}買點小吃。\",\n",
        "        \"{address}鄰近{address}，並於{time}舉辦花卉展覽。\",\n",
        "        \"{time}剛下班路過{address}買麵包，{time}又與家人到{address}散步。\"\n",
        "    ],\n",
        "    \"英语\": [\n",
        "        \"I'm heading to {address} {time} to catch the view, then stopping by {address} for coffee.\",\n",
        "        \"We’re meeting at {address} {time} before walking to {address} for the exhibition.\",\n",
        "        \"{time} is the perfect moment to explore {address} and then visit {address} nearby.\",\n",
        "        \"Locals say {address}, next to {address}, shines best {time}.\",\n",
        "        \"{address} was the venue for a major cultural fair {time}.\",\n",
        "        \"{time} I left {address}, {time} I arrived at {address} for dinner.\"\n",
        "    ],\n",
        "    \"丹麦语\": [\n",
        "        \"Jeg vil tage til {address} {time} for at nyde udsigten og derefter besøge {address}.\",\n",
        "        \"Vi mødes ved {address} {time} og går sammen til {address} for arrangementet.\",\n",
        "        \"{time} er det bedste tidspunkt at gå en tur fra {address} til {address}.\",\n",
        "        \"{address} ligger tæt på {address} og havde en stor festival {time}.\",\n",
        "        \"Der er jazzkoncerter i {address}, nær {address}, {time}.\",\n",
        "        \"{time} jeg gik til {address}, {time} jeg kom til {address} for koncerten.\"\n",
        "    ],\n",
        "    \"俄语\": [\n",
        "        \"Я пойду в {address} {time}, а потом зайду в {address} на обед.\",\n",
        "        \"Мы встречаемся у {address} {time} и идём вместе в {address} на выставку.\",\n",
        "        \"{time} идеально подходит для прогулки из {address} в {address}.\",\n",
        "        \"Местные говорят, что {address} рядом с {address} особенно красив {time}.\",\n",
        "        \"В {address}, недалеко от {address}, {time} прошёл ярмарка.\",\n",
        "        \"{time} я вышел из {address}, {time} я пришёл в {address} к встрече.\"\n",
        "    ],\n",
        "    \"土耳其语\": [\n",
        "        \"{time} {address}'te buluşup ardından {address}'e geçeceğiz.\",\n",
        "        \"En güzel manzara {time} {address} ve {address} arasında görülür.\",\n",
        "        \"{address}, {address} yakınında {time} düzenlenen festivalin merkeziydi.\",\n",
        "        \"{time} günü {address}'e gidip oradan {address}'e alışverişe geçmeyi planlıyorum.\",\n",
        "        \"{address}'teki kahve molasından sonra {address}'e yürüyüş yapacağız.\",\n",
        "        \"{time} {address}'ten çıktım, {time} {address}'e geldim.\"\n",
        "    ],\n",
        "    \"德语\": [\n",
        "        \"Ich gehe {time} zum {address} und danach in den {address} spazieren.\",\n",
        "        \"Wir treffen uns {time} bei {address}, dann besuchen wir den {address}.\",\n",
        "        \"Von {address} zum {address} wandert man am besten {time}.\",\n",
        "        \"{address} in der Nähe von {address} war {time} Austragungsort eines Festes.\",\n",
        "        \"Ein Nachmittag {time} im {address} und anschließend im {address} ist perfekt.\",\n",
        "        \"{time} ich verließ {address}, {time} ich ging zum {address} für das Treffen.\"\n",
        "    ],\n",
        "    \"意大利语\": [\n",
        "        \"Visito {address} {time} e poi passo al famoso {address} per un gelato.\",\n",
        "        \"{time} ci incontriamo a {address} e andiamo insieme a {address} per lo spettacolo.\",\n",
        "        \"{address} vicino a {address} ospita una fiera {time}.\",\n",
        "        \"{time} è il momento ideale per camminare da {address} a {address}.\",\n",
        "        \"{address} è stato luogo di un grande concerto {time}.\",\n",
        "        \"{time} sono uscito da {address}, {time} sono arrivato a {address}.\"\n",
        "    ],\n",
        "    \"日语\": [\n",
        "        \"{time}に{address}へ行って、その後{address}でお茶をします。\",\n",
        "        \"{address}の近くの{address}では{time}に花祭りが開催されます。\",\n",
        "        \"{time}は{address}から{address}まで散歩するのに最高です。\",\n",
        "        \"{address}と{address}の間の小道が{time}にとても綺麗です。\",\n",
        "        \"{address}にある博物館は{time}に特別展を開きました。\",\n",
        "        \"{time}{address}を出て、{time}{address}に着いた。\"\n",
        "    ],\n",
        "    \"法语\": [\n",
        "        \"Je vais à {address} {time} puis prendre un café près de {address}.\",\n",
        "        \"Nous nous retrouvons {time} à {address} avant d’aller à {address} pour le marché.\",\n",
        "        \"Entre {address} et {address}, la balade {time} est magnifique.\",\n",
        "        \"{address} près de {address} a accueilli un festival {time}.\",\n",
        "        \"Un après-midi {time} dans {address} et ensuite {address} est parfait.\",\n",
        "        \"{time} je suis parti de {address}, {time} je suis arrivé à {address}.\"\n",
        "    ],\n",
        "    \"瑞典语\": [\n",
        "        \"Jag besöker {address} {time} och går sedan till {address} för middag.\",\n",
        "        \"{time} träffas vi vid {address} och går till {address} tillsammans.\",\n",
        "        \"Vandringen mellan {address} och {address} är bäst {time}.\",\n",
        "        \"{address} nära {address} höll en stor marknad {time}.\",\n",
        "        \"En kväll {time} i {address} följt av {address} är underbar.\",\n",
        "        \"{time} jag lämnade {address}, {time} jag kom till {address}.\"\n",
        "    ],\n",
        "    \"荷兰语\": [\n",
        "        \"Ik ga {time} naar {address} en daarna naar {address} om te winkelen.\",\n",
        "        \"We ontmoeten elkaar {time} bij {address} en wandelen naar {address}.\",\n",
        "        \"Tussen {address} en {address} lopen is prachtig {time}.\",\n",
        "        \"{address} vlakbij {address} had {time} een muziekfestival.\",\n",
        "        \"Een ochtend {time} in {address} en daarna naar {address} is perfect.\",\n",
        "        \"{time} ik vertrok uit {address}, {time} ik kwam aan bij {address}.\"\n",
        "    ],\n",
        "    \"葡萄牙语\": [\n",
        "         \"{time} visitei {address} e depois caminhei pelas ruas históricas de {address}.\",\n",
        "         \"Durante {time} participamos de um festival gastronômico no {address}.\",\n",
        "         \"No {address}, {time} assistimos a um concerto ao ar livre.\",\n",
        "         \"Passei {time} todo no {address} tirando fotos e aproveitando a vista.\",\n",
        "         \"{time} fui ao {address} para devolver livros e depois ao {address} para comprar pão.\",\n",
        "          \"Entre {time} e {time} viajamos por {address}, {address} e {address}.\"\n",
        "    ],\n",
        "    \"西班牙语\": [\n",
        "      \"{time} visité {address} y luego paseé por el casco antiguo de {address}.\",\n",
        "      \"Durante {time} asistimos a una feria de arte en {address}.\",\n",
        "      \"En {address}, {time} escuchamos un concierto de música clásica.\",\n",
        "      \"Pasé todo {time} en {address} tomando fotos y disfrutando la vista.\",\n",
        "      \"{time} fui a {address} para devolver unos libros y luego a {address} para comprar pan.\",\n",
        "      \"Entre {time} y {time} recorrimos {address}, {address} y {address}.\",\n",
        "      \"Anoche cenamos en {address} junto al {address}.\",\n",
        "    ],\n",
        "    \"韩语\": [\n",
        "        \"{time}에 {address}에 들른 후에 {address}에서 저녁을 먹을 예정입니다.\",\n",
        "        \"{time} {address}에서 만나 {address}로 함께 갈 거예요.\",\n",
        "        \"{address} 근처의 {address}에서는 {time}에 축제가 열립니다.\",\n",
        "        \"{time}에는 {address}와 {address} 사이 길이 가장 아름답습니다.\",\n",
        "        \"{address}는 {address}에 위치하며 {time}에 문화 행사를 개최했습니다.\",\n",
        "        \"{time}{address}에서 나왔고, {time}{address}에 도착했다.\"\n",
        "    ]\n",
        "}\n",
        "extra_samples = {\n",
        "    \"简体中文\": [\n",
        "        \"{time}我从{address}坐车出发，大约{time}到达{address}开始排队，等到{time}才轮到我们进去。\",\n",
        "        \"我在{time}抵达{address}喝咖啡，{time}前往{address}与朋友共进晚餐。\",\n",
        "        \"{time}我从{address}启程，在{time}抵达{address}短暂休息，接着{time}又前往{address}参加晚宴。\",\n",
        "        \"我{time}出发去{address}，{time}到达后开始拍摄，{time}离开时已近黄昏。\",\n",
        "        \"刚到{time}我已在{address}与同事会面，{time}驱车去{address}处理事务，直到{time}才返回{address}。\"\n",
        "    ],\n",
        "    \"繁体中文\": [\n",
        "        \"在{time}與{time}之間，我們走訪了{address}、{address}和{address}的文化中心。\",\n",
        "        \"{time}到達{address}拍攝，然後前往{address}參展，最後{time}返回{address}整理相片。\",\n",
        "        \"聽說{address}與{address}相連的小路在{time}與{time}都有不同的花景，所以我們特意安排{address}之旅。\",\n",
        "        \"{time}於{address}辦研討會，{time}移師{address}，{time}結束並返回{address}。\"\n",
        "    ],\n",
        "    \"英语\": [\n",
        "        \"Between {time} and {time}, we visited {address}, {address}, and {address} for sightseeing.\",\n",
        "        \"{time} at {address} for the meeting, {time} moved to {address} for lunch, then {time} back at {address} to wrap up.\",\n",
        "        \"{time} reporters gathered at {address}, then headed to {address} for interviews, finally at {time} returned to {address} for editing.\",\n",
        "        \"\\\"Meet me at {address} {time}, then we'll go to {address} and {address},\\\" she texted.\"\n",
        "    ],\n",
        "    \"丹麦语\": [\n",
        "        \"Mellem {time} og {time} besøgte vi {address}, {address} og {address}.\",\n",
        "        \"{time} startede vi ved {address}, {time} tog til {address} for at se udstillingen, {time} gik vi til {address} for aftensmad.\",\n",
        "        \"{time} ankom fotografer til {address}, {time} fortsatte mod {address}, og {time} vendte tilbage til {address}.\",\n",
        "        \"Vi planlægger at tage fra {address} {time}, besøge {address}, og derefter {address} inden aften.\"\n",
        "    ],\n",
        "    \"俄语\": [\n",
        "        \"Между {time} и {time} мы посетили {address}, {address} и {address}.\",\n",
        "        \"Между {time} и {time} мы встретились в {address}, обсудили детали в {address}, а к {time} вернулись в {address} для подготовки отчёта.\",\n",
        "        \"{time} журналисты прибыли в {address}, {time} провели интервью, а к {time} вернулись в {address} для монтажа материала.\",\n",
        "        \"В {time} началась эвакуация из {address}, и только {time} люди добрались до {address} для получения помощи.\",\n",
        "    ],\n",
        "    \"土耳其语\": [\n",
        "        \"{time} ile {time} arasında {address}, {address} ve {address} ziyaret ettik.\",\n",
        "        \"{time} {address}'te başladık, {time} {address}'e geçtik, {time} günü {address}'te bitirdik.\",\n",
        "        \"{time} gazeteciler {address}'te toplandı, {address}'e gitti, {time} tekrar {address}'e döndü.\",\n",
        "        \"{time} {address}'ten yola çıkıp {address}'e uğradım, sonra {address}'e devam ettim.\"\n",
        "    ],\n",
        "    \"德语\": [\n",
        "        \"Zwischen {time} und {time} besuchten wir {address}, {address} und {address}.\",\n",
        "        \"{time} starteten wir am {address}, {time} gingen wir zum {address}, und {time} beendeten wir den Tag im {address}.\",\n",
        "        \"{time} versammelten sich Reporter im {address}, fuhren zum {address} und kehrten {time} ins {address} zurück.\",\n",
        "        \"Ich plane, den {address} {time} zu verlassen, zum {address} zu fahren und dann zum {address}.\"\n",
        "    ],\n",
        "    \"意大利语\": [\n",
        "        \"Tra {time} e {time} abbiamo visitato {address}, {address} e {address}.\",\n",
        "        \"{time} siamo partiti da {address}, {time} abbiamo pranzato al {address}, {time} abbiamo finito al {address}.\",\n",
        "        \"{time} i giornalisti si sono riuniti al {address}, poi al {address}, e {time} erano di nuovo al {address}.\",\n",
        "        \"Parto da {address} {time}, passo al {address} e poi al {address}.\"\n",
        "    ],\n",
        "    \"日语\": [\n",
        "        \"{time}と{time}の間に{address}、{address}、{address}を巡った。\",\n",
        "        \"{time}{address}を出発し、{time}{address}へ移動して展示会を見学、{time}{address}で解散した。\",\n",
        "        \"{time}{address}で記者会見、その後{address}へ移動し、{time}{address}に戻った。\",\n",
        "        \"{time}{address}を出て{address}を訪問し、その後{address}へ向かった。\"\n",
        "    ],\n",
        "    \"法语\": [\n",
        "        \"Entre {time} et {time}, nous avons visité {address}, {address} et {address}.\",\n",
        "        \"{time} nous avons commencé à {address}, {time} avons déjeuné à {address}, puis {time} avons terminé au {address}.\",\n",
        "        \"{time} les journalistes se sont rassemblés à {address}, puis sont allés à {address}, et à {time} sont revenus à {address}.\",\n",
        "        \"Je pars de {address} {time}, vais à {address}, puis à {address}.\"\n",
        "    ],\n",
        "    \"瑞典语\": [\n",
        "        \"Mellan {time} och {time} besökte vi {address}, {address} och {address}.\",\n",
        "        \"{time} började vi vid {address}, {time} fortsatte till {address}, {time} avslutade vid {address}.\",\n",
        "        \"{time} samlades reportrar vid {address}, fortsatte till {address}, och {time} tillbaka till {address}.\",\n",
        "        \"Jag lämnar {address} {time}, besöker {address} och {address}.\"\n",
        "    ],\n",
        "    \"荷兰语\": [\n",
        "        \"Tussen {time} en {time} bezochten we {address}, {address} en {address}.\",\n",
        "        \"{time} begonnen we bij {address}, {time} gingen naar {address}, en {time} eindigden bij {address}.\",\n",
        "        \"{time} verzamelden journalisten zich bij {address}, gingen naar {address}, en {time} keerden terug naar {address}.\",\n",
        "        \"Ik vertrek uit {address} {time}, ga langs {address} en {address}.\"\n",
        "    ],\n",
        "    \"葡萄牙语\": [\n",
        "        \"Ontem à noite jantamos no {address} perto da {address}.\",\n",
        "        \"No verão passado acampamos nas montanhas de {address}.\",\n",
        "        \"Sem hora marcada, encontrei amigos no {address}.\",\n",
        "        \"Partimos de {address} {time} e chegamos ao {address} {time}.\",\n",
        "        \"Ontem comprei flores no {address}.\",\n",
        "        \"{time} não saí de casa.\"\n",
        "    ],\n",
        "    \"西班牙语\": [\n",
        "          \"En invierno esquiamos en {address}.\",\n",
        "         \"Sin hora fija, me encontré con amigos en {address}.\",\n",
        "         \"Salimos de {address} {time} y llegamos a {address} {time}.\",\n",
        "        \"Ayer compré flores en {address}.\",\n",
        "        \"{time} no salí de casa.\"\n",
        "    ],\n",
        "    \"韩语\": [\n",
        "        \"{time}과 {time} 사이에 {address}, {address}, {address}를 방문했다.\",\n",
        "        \"{time}{address}에서 출발해, {time}{address}로 이동 후, {time}{address}에서 마무리했다.\",\n",
        "        \"{time}{address}에서 기자회견을 열고, {address}로 이동해, {time}{address}로 돌아왔다.\",\n",
        "        \"{time}{address}를 떠나 {address}를 거쳐 {address}로 향했다.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "for lang, sentences in extra_samples.items():\n",
        "    SAMPLE_PAIRS[lang].extend(sentences)\n",
        "def build_prompt(language: str, addr_count: int, time_count: int) -> str:\n",
        "    # 构造演示文本\n",
        "    examples = SAMPLE_PAIRS.get(language, [])\n",
        "    examples_text = \"\\n\".join(json.dumps({\"language\": language, \"template\": ex}, ensure_ascii=False) for ex in examples)\n",
        "    return f\"\"\"\n",
        "你是一名多语言模板设计专家，要在所给语言中造句。\n",
        "\n",
        "### 规则：\n",
        "1. 必须使用固定数量的{{address}}和{{time}}占位符：\n",
        "   - {{address}}出现次数 = {addr_count}\n",
        "   - {{time}}出现次数 = {time_count}\n",
        "2. 不得出现任何真实地址或真实时间用词，唯一的地址类词只能是{{address}}，唯一的时间类词只能是{{time}}。\n",
        "3. 句子必须自然流畅，符合同语言的表达习惯，可带有修饰成分，要求生成的句子要尽可能针对各种语言进行母语化出路，符合各语种日常生活特征。\n",
        "4. 尽可能丰富的多种语境（旅游、工作、活动、新闻、故事、论文、小说、日常聊天等）混合生成。\n",
        "5. 西文语言（英语、西班牙语、葡萄牙语、意大利语）中{{time}}前禁止加介词。\n",
        "6. 输出为JSONL，每行一个JSON对象：{{\"language\": \"<语言>\", \"template\": \"<句子>\"}}\n",
        "7. 禁止输出JSONL以外内容。\n",
        "8. 当要求出现的address和time出现次数都为0时，要求你生成不含任何时间地址（包括“今天\"，“昨天\"，“这个周末\"等）且无占位符的整句负例。\n",
        "9. 重点审查：演示样例中的占位符数量可能与规则1.中声明的不同，因此需要你严格依赖规则1.中声明的占位符数量生成模板，从样例中请你尽量学习句式即可。\n",
        "10. 演示样例仅供参考，请你从网络上大量学习该语种表达习惯以丰富句式，严格禁止过度模仿重复样例,可以采用多种人称叙述方式，不要过度使用第一人称。\n",
        "\n",
        "=== 演示样例 ===\n",
        "{examples_text}\n",
        "\n",
        "请用{language}生成{TEMPLATE_BATCH_SIZE}条符合以上规则的句子。\n",
        "\"\"\".strip()\n",
        "\n",
        "async def chat_stream(payload: dict) -> str:\n",
        "    assert client is not None\n",
        "    text_buf: List[str] = []\n",
        "    async with client.stream(\"POST\", ENDPOINT, headers=HEADERS, content=json.dumps(payload)) as resp:\n",
        "        async for line in resp.aiter_lines():\n",
        "            if not line.startswith(\"data:\"):\n",
        "                continue\n",
        "            data = line[5:].strip()\n",
        "            if data == \"[DONE]\":\n",
        "                break\n",
        "            try:\n",
        "                delta = json.loads(data)[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n",
        "            except Exception:\n",
        "                continue\n",
        "            text_buf.append(delta)\n",
        "    return \"\".join(text_buf)\n",
        "\n",
        "def parse_jsonl(text: str) -> List[Dict[str, Any]]:\n",
        "    lines = [ln.strip() for ln in text.strip().splitlines() if ln.strip()]\n",
        "    out = []\n",
        "    for ln in lines:\n",
        "        try:\n",
        "            obj = json.loads(ln)\n",
        "            if isinstance(obj, dict):\n",
        "                out.append(obj)\n",
        "        except:\n",
        "            pass\n",
        "    return out\n",
        "\n",
        "def post_process(data: List[Dict[str, Any]], addr_count: int, time_count: int, language: str):\n",
        "    out, seen = [], set()\n",
        "    rejected = []\n",
        "    blacklist = PREPOSITION_BLACKLIST.get(language, [])\n",
        "\n",
        "    for item in data:\n",
        "        t = str(item.get(\"template\", \"\")).strip()\n",
        "        if not t or t in seen:\n",
        "            rejected.append((t, \"空或重复\"))\n",
        "            continue\n",
        "        if t.count(\"{address}\") != addr_count:\n",
        "            rejected.append((t, f\"{t.count('{address}')}个{{address}}占位符\"))\n",
        "            continue\n",
        "        if t.count(\"{time}\") != time_count:\n",
        "            rejected.append((t, f\"{t.count('{time}')}个{{time}}占位符\"))\n",
        "            continue\n",
        "        # 检查西文介词黑名单\n",
        "        if any(re.search(rf\"\\b{prep}\\s*\\{{time\\}}\", t, re.IGNORECASE) for prep in blacklist):\n",
        "            rejected.append((t, \"时间前出现介词\"))\n",
        "            continue\n",
        "\n",
        "        out.append(t)\n",
        "        seen.add(t)\n",
        "    return out, rejected\n",
        "\n",
        "async def generate_batch(language: str, addr_count: int, time_count: int):\n",
        "    prompt = build_prompt(language, addr_count, time_count)\n",
        "    payload = {\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"stream\": True,\n",
        "        \"temperature\": TEMPERATURE\n",
        "    }\n",
        "    raw = await chat_stream(payload)\n",
        "    parsed = parse_jsonl(raw)\n",
        "    valid, rejected = post_process(parsed, addr_count, time_count, language)\n",
        "    return valid, rejected\n",
        "\n",
        "def parse_counts_from_dir(dirname: str):\n",
        "    match = re.match(r\"combine_templates_(\\d)(\\d)\", dirname)\n",
        "    if not match:\n",
        "        raise ValueError(f\"目录名不合法: {dirname}\")\n",
        "    return int(match.group(1)), int(match.group(2))\n",
        "\n",
        "async def generate_for_dir(dirname: str):\n",
        "    addr_count, time_count = parse_counts_from_dir(dirname)\n",
        "    dir_path = os.path.join(OUTPUT_BASE_DIR, dirname)\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "    # === 根据文件夹名末尾自动调整 target ===\n",
        "    suffix = dirname.split('_')[-1]  # 比如 combine_templates_11 => '11'\n",
        "    if suffix == \"11\":\n",
        "        target = 4000\n",
        "    elif suffix == \"00\":\n",
        "        target = 1000\n",
        "    else:\n",
        "        target = 500  # 你可以随时改这个默认值\n",
        "\n",
        "    print(f\"[Info] {dirname} -> 每语言产出目标:{target}\")\n",
        "\n",
        "    for lang in LANGUAGES:\n",
        "        out_path = os.path.join(dir_path, f\"{lang}_combine_templates.jsonl\")\n",
        "        existing = sum(1 for _ in open(out_path, \"r\", encoding=\"utf-8\")) if os.path.exists(out_path) else 0\n",
        "        total = existing\n",
        "        while total < target:\n",
        "            try:\n",
        "                batch, rejected = await generate_batch(lang, addr_count, time_count)\n",
        "            except Exception as e:\n",
        "                print(f\"[{lang}] 批次失败: {e}\")\n",
        "                await asyncio.sleep(1)\n",
        "                continue\n",
        "            if batch:\n",
        "                with open(out_path, \"a\", encoding=\"utf-8\") as f:\n",
        "                    for t in batch:\n",
        "                        f.write(json.dumps({\"language\": lang, \"template\": t}, ensure_ascii=False) + \"\\n\")\n",
        "                total += len(batch)\n",
        "                print(f\"[{lang}] 生成进度: {total}/{target}\")\n",
        "            else:\n",
        "                print(f\"[{lang}] 无可用模板，跳过一次\")\n",
        "                if rejected:\n",
        "                    print(f\"[{lang}] 被淘汰模板示例:\")\n",
        "                    for sample, reason in rejected[:5]:  # 显示前 5 个\n",
        "                        print(f\"    ❌ 原文: {sample} | 原因: {reason}\")\n",
        "                await asyncio.sleep(1)\n",
        "\n",
        "async def main():\n",
        "    global client\n",
        "    client = httpx.AsyncClient(base_url=BASE_URL, timeout=TIMEOUT)\n",
        "    try:\n",
        "        for dirname in os.listdir(OUTPUT_BASE_DIR):\n",
        "            if dirname.startswith(\"combine_templates_\"):\n",
        "                await generate_for_dir(dirname)\n",
        "    finally:\n",
        "        await client.aclose()\n",
        "\n",
        "# 执行\n",
        "await main()"
      ],
      "metadata": {
        "id": "jnhV24IfMDNf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "74ddacfc-680d-4ead-bb39-c2bc9777e8a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] combine_templates_11 -> 每语言产出目标:4000\n",
            "[繁体中文] 生成进度: 2511/4000\n",
            "[繁体中文] 生成进度: 2551/4000\n",
            "[繁体中文] 生成进度: 2579/4000\n",
            "[繁体中文] 生成进度: 2619/4000\n",
            "[繁体中文] 生成进度: 2659/4000\n",
            "[繁体中文] 生成进度: 2699/4000\n",
            "[繁体中文] 生成进度: 2739/4000\n",
            "[繁体中文] 生成进度: 2779/4000\n",
            "[繁体中文] 生成进度: 2818/4000\n",
            "[繁体中文] 生成进度: 2857/4000\n",
            "[繁体中文] 生成进度: 2896/4000\n",
            "[繁体中文] 生成进度: 2936/4000\n",
            "[繁体中文] 生成进度: 2976/4000\n",
            "[繁体中文] 生成进度: 3016/4000\n",
            "[繁体中文] 生成进度: 3054/4000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CancelledError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-479377406.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;31m# 执行\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m \u001b[0;32mawait\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-479377406.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdirname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_BASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdirname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"combine_templates_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0mgenerate_for_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;32mawait\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-479377406.py\u001b[0m in \u001b[0;36mgenerate_for_dir\u001b[0;34m(dirname)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrejected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddr_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[{lang}] 批次失败: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-479377406.py\u001b[0m in \u001b[0;36mgenerate_batch\u001b[0;34m(language, addr_count, time_count)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;34m\"temperature\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTEMPERATURE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     }\n\u001b[0;32m--> 352\u001b[0;31m     \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mchat_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m     \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_jsonl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrejected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddr_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-479377406.py\u001b[0m in \u001b[0;36mchat_stream\u001b[0;34m(payload)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0mtext_buf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mENDPOINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHEADERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maiter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36maiter_lines\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLineDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m             \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maiter_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36maiter_text\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0mchunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextChunker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m             \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbyte_content\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maiter_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m                 \u001b[0mtext_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36maiter_bytes\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    995\u001b[0m             \u001b[0mchunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mByteChunker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m                 \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mraw_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maiter_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m                     \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36maiter_raw\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1055\u001b[0;31m             \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mraw_stream_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_bytes_downloaded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_stream_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_stream_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m__aiter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__aiter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAsyncIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36m__aiter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__aiter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAsyncIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_httpcore_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\u001b[0m in \u001b[0;36m__aiter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\u001b[0m in \u001b[0;36m__aiter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__aiter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAsyncIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\u001b[0m in \u001b[0;36m__aiter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mAsyncShieldCancellation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\u001b[0m in \u001b[0;36m__aiter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"receive_response_body\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\u001b[0m in \u001b[0;36m_receive_response_body\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = await self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0manyio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfail_after\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0manyio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEndOfStream\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: nocover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bytes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m65536\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_sslobject_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEndOfStream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\u001b[0m in \u001b[0;36m_call_sslobject_method\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransport_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_bio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransport_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mEndOfStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_bio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_eof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m   1261\u001b[0m             ):\n\u001b[1;32m   1262\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_reading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpause_reading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/locks.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_waiters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCancelledError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**输入输出说明：**\n",
        "*  **输入**：15种语言（中文分简体繁体）的纯地址txt文件，命名为语言名.txt，文件夹为real_address\n",
        "\n",
        "\n",
        "输入结构：\n",
        "\n",
        "\n",
        "1. 真实地址\n",
        "\n",
        "```\n",
        "/content\n",
        "└── drive\n",
        "    └── MyDrive\n",
        "        └── NER\n",
        "            └── real_address(共17个文件，要自己上传到谷歌云盘)\n",
        "                ├── 简体中文.txt\n",
        "                ├── 繁体中文.txt\n",
        "                  ......\n",
        "                └── 韩语.txt\n",
        "            └── non_address\n",
        "                ├── 简体中文.txt\n",
        "                ├── 繁体中文.txt\n",
        "                  ......\n",
        "                └── 韩语.txt           \n",
        "```\n",
        "2. 模板文件，文件结构见本文件中的模板生成代码（在上面）\n",
        "\n",
        "---\n",
        "\n",
        "*  **输出：**\n",
        "\n",
        "\n",
        "```\n",
        "/content\n",
        "└── drive\n",
        "    └── MyDrive\n",
        "        └── NER\n",
        "            ├── train_datasets(共16个文件)\n",
        "                ├── train_简体中文.conll\n",
        "                ├── train_繁体中文.conll\n",
        "                ├── train_英语.conll\n",
        "                ......\n",
        "                └── train_all.conll\n",
        "\n",
        "```\n",
        "\n",
        "**标记规则：**\n",
        "\n",
        "1. 含1~n个地址的数据\n",
        "    *   正例：应该被识别的地址->标记为ADDRESS\n",
        "        *   加噪声\n",
        "        *   不加噪声\n",
        "\n",
        "    *   负例（如有硬性不允许被识别的地址）：不应该被识别的地址->标记为O （注意覆盖全面和多样性）\n",
        "\n",
        "2. 含0个地址的数据\n",
        "    \n",
        "    整句标记为O\n",
        "\n",
        "\n",
        "\n",
        "**参数说明**\n",
        "1. N_PER_LANG:\n",
        "    \n",
        "    每种语言生成多少条训练数据，不输入（None）自动为每个模板合成，再按纯负例占比RATIO_NEG合成负例。\n",
        "\n",
        "2. PREVIEW_N:\n",
        "\n",
        "    合成结束后预览多少条数据\n",
        "  \n",
        "3. RATIO_NEG:\n",
        "    \n",
        "    整句负例概率\n",
        "\n",
        "4. NEG_ADDR_PROB:\n",
        "\n",
        "    地址负例概率\n"
      ],
      "metadata": {
        "id": "c--7_ht1Dkwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 生成训练数据（CONLL）（纯地址） {\"form-width\":\"40%\"}\n",
        "from google.colab import drive  # 在 Colab 环境下可用\n",
        "import os, re, json, random, unicodedata, argparse, sys\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "# 挂载谷歌云盘\n",
        "drive.mount('/content/drive')\n",
        "print(\"云盘挂载成功\")\n",
        "\n",
        "\n",
        "# ======================================================可按需修改的配置 ======================================================\n",
        "\n",
        "\n",
        "N_PER_LANG = 10000 # @param {\"type\":\"raw\",\"placeholder\":\"请输入每个语言的训练数据量，不输入默认遍历所有模板\"}\n",
        "\n",
        "RATIO_NEG = 0.2 # @param {\"type\":\"number\",\"placeholder\":\"整句负例占比（相对每语言总量）\"}\n",
        "NEG_ADDR_PROB = 0.3  # @param {\"type\":\"number\",\"placeholder\":\"插入地址负例的概率\"}\n",
        "\n",
        "# 预览， >0 时打印样例\n",
        "PREVIEW_N = 5  # @param {\"type\":\"integer\",\"placeholder\":\"请输入预览数据量\"}\n",
        "\n",
        "# 噪声配置\n",
        "NOISE_PROB_CASE = 0.15               # 大小写扰动概率（适用于拉丁文字）\n",
        "NOISE_PROB_FULLWIDTH = 0.10          # 全/半角互换概率（中日朝/数字/标点）\n",
        "NOISE_PROB_SEP_REPLACE = 0.10        # 分隔符替换概率\n",
        "NOISE_PROB_INSERT = 0            # 插入字符概率（会触发标签移位/内部I-延续）\n",
        "# 插入字符串的噪声\n",
        "INSERT_CHARS = [\",\", \".\", \"·\", \"—\", \"-\", \"/\", \":\", \"、\", \"・\",\"?\",\"？\", \"！\", \"!\", \"，\", \"。\", \"；\", \";\", \" \"]\n",
        "\n",
        "# 仅 address / 仅 time 的样本比例（“几乎不含O”）\n",
        "RATIO_STRONG_POS = 0.10  # 在正例中抽取的比例，构造只有 address 或只有 time 的短样本（近乎全实体）\n",
        "\n",
        "\n",
        "# =================================================================================================================================\n",
        "\n",
        "\n",
        "LANGS = [\"简体中文\",\"繁体中文\", \"英语\",\"丹麦语\",\"俄语\",\"土耳其语\",\"德语\",\"意大利语\",\"日语\",\n",
        "         \"法语\",\"瑞典语\",\"荷兰语\",\"葡萄牙语\",\"西班牙语\",\"韩语\"]\n",
        "\n",
        "\n",
        "# ================== 输入路径 ==================\n",
        "# 真实地址与非地址片段\n",
        "REAL_ADDR_ROOT = \"/content/drive/MyDrive/NER/real_address\"   # <语言>.txt 与 non_address.txt\n",
        "NON_ADDR_ROOT  = \"/content/drive/MyDrive/NER/non_address\"\n",
        "\n",
        "# 模板根目录（包含 address_templates_0..5，其中 0 = 整句纯负例模板）\n",
        "ADDR_TPL_ROOT = \"/content/drive/MyDrive/NER/address_templates\"\n",
        "\n",
        "\n",
        "# ================== 输出路径 ==================\n",
        "OUT_DIR = \"/content/drive/MyDrive/NER/train_datasets\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "PER_LANG_CONLL = lambda lang: os.path.join(OUT_DIR, f\"train_{lang}.conll\") # 拼接每语言输出路径\n",
        "MERGED_CONLL = os.path.join(OUT_DIR, \"train_all.conll\") # 拼接全部数据输出路径\n",
        "\n",
        "\n",
        "TOKENIZER_NAME = \"xlm-roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, use_fast=True)\n",
        "\n",
        "# ================== 基础IO ==================\n",
        "def read_lines(path: str) -> List[str]:\n",
        "    if not os.path.exists(path):\n",
        "        return []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return [ln.strip() for ln in f if ln.strip()]\n",
        "\n",
        "# def choose_time(lang: str, time_map: Dict[str, List[str]], time_numeric: List[str]) -> str:\n",
        "#     if random.random() < RATIO_TIME_NUMERIC and time_numeric:\n",
        "#         return random.choice(time_numeric)\n",
        "#     arr = time_map.get(lang, [])\n",
        "#     if not arr:  # 回退\n",
        "#         arr = time_numeric or [\"2025-08-12\", \"14:30\", \"2025-08-12 14:30\"]\n",
        "#     return random.choice(arr)\n",
        "\n",
        "def choose_addr(lang: str, addr_map: Dict[str, List[str]]) -> str:\n",
        "    arr = addr_map.get(lang, [])\n",
        "    if not arr:\n",
        "        # 最弱回退\n",
        "        return \"Main Street 1\"\n",
        "    return random.choice(arr)\n",
        "\n",
        "def read_jsonl_templates(path: str) -> list:\n",
        "    items = []\n",
        "    if not os.path.exists(path):\n",
        "        return items\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for ln in f:\n",
        "            ln = ln.strip()\n",
        "            if not ln:\n",
        "                continue\n",
        "            try:\n",
        "                obj = json.loads(ln)\n",
        "                t = obj.get(\"template\")\n",
        "                if isinstance(t, str) and t.strip():\n",
        "                    items.append(\" \".join(t.split()))\n",
        "            except Exception:\n",
        "                continue\n",
        "    return items\n",
        "\n",
        "def load_templates_for_lang(lang: str) -> Dict[str, list]:\n",
        "    \"\"\"\n",
        "    遍历 ADDR_TPL_ROOT 下所有 address_templates_*\n",
        "      - address_templates_0：整句纯负例模板（neg_sent）\n",
        "      - address_templates_1..N：地址模板（addr_only）\n",
        "    统一读取 <语言>_address_templates.jsonl 的 \"template\" 字段。\n",
        "    返回 {\"addr_only\": [...], \"neg_sent\": [...]}。\n",
        "    \"\"\"\n",
        "    addr_only, neg_sent = [], []\n",
        "    if not os.path.isdir(ADDR_TPL_ROOT):\n",
        "        return {\"addr_only\": addr_only, \"neg_sent\": neg_sent}\n",
        "\n",
        "    prefix = \"address_templates_\"\n",
        "    filename = f\"{lang}_address_templates.jsonl\"   # 统一命名\n",
        "\n",
        "    for name in os.listdir(ADDR_TPL_ROOT):\n",
        "        if not name.startswith(prefix):\n",
        "            continue\n",
        "        subdir = os.path.join(ADDR_TPL_ROOT, name)\n",
        "        if not os.path.isdir(subdir):\n",
        "            continue\n",
        "\n",
        "        fpath = os.path.join(subdir, filename)\n",
        "        items = read_jsonl_templates(fpath)\n",
        "        if not items:\n",
        "            continue\n",
        "\n",
        "        # 0 号目录视为整句纯负例模板\n",
        "        slot = name[len(prefix):]\n",
        "        if slot.isdigit() and int(slot) == 0:\n",
        "            neg_sent.extend(items)\n",
        "        else:\n",
        "            addr_only.extend(items)\n",
        "\n",
        "    return {\"addr_only\": addr_only, \"neg_sent\": neg_sent}\n",
        "\n",
        "\n",
        "\n",
        "# 增强稳健性：用非地址片段填充整句负例模板（即使含 {address} 也不记录 span，生成全 O）\n",
        "def fill_template_all_non_address(template: str, lang: str, non_addr_map: Dict[str, List[str]]) -> str:\n",
        "    out, cursor = [], 0\n",
        "    for m in PLACEHOLDER_RE.finditer(template):\n",
        "        out.append(template[cursor:m.start()])\n",
        "        out.append(choose_non_address(lang, non_addr_map))\n",
        "        cursor = m.end()\n",
        "    out.append(template[cursor:])\n",
        "    return \"\".join(out)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ================== 噪声：不改变“可整体识别为日期”的语义，但可能插入/替换字符 ==================\n",
        "def toggle_case(s: str) -> str:\n",
        "    def _flip(c):\n",
        "        if \"a\" <= c <= \"z\": return c.upper()\n",
        "        if \"A\" <= c <= \"Z\": return c.lower()\n",
        "        return c\n",
        "    return \"\".join(_flip(c) for c in s)\n",
        "\n",
        "def to_fullwidth(c: str) -> str:\n",
        "    # 半角 -> 全角\n",
        "    code = ord(c)\n",
        "    if 33 <= code <= 126:\n",
        "        return chr(code + 0xFEE0)\n",
        "    if c == \" \":\n",
        "        return \"\\u3000\"\n",
        "    return c\n",
        "\n",
        "def to_halfwidth(c: str) -> str:\n",
        "    # 全角 -> 半角\n",
        "    if c == \"\\u3000\":\n",
        "        return \" \"\n",
        "    code = ord(c)\n",
        "    if 65281 <= code <= 65374:\n",
        "        return chr(code - 0xFEE0)\n",
        "    return c\n",
        "\n",
        "def fullwidth_halfwidth_flip(s: str) -> str:\n",
        "    out = []\n",
        "    for ch in s:\n",
        "        if unicodedata.east_asian_width(ch) in (\"F\",\"W\") and random.random() < 0.5:\n",
        "            out.append(to_halfwidth(ch))\n",
        "        elif random.random() < 0.2:\n",
        "            out.append(to_fullwidth(ch))\n",
        "        else:\n",
        "            out.append(ch)\n",
        "    return \"\".join(out)\n",
        "\n",
        "def replace_separators(s: str) -> str:\n",
        "    # 常见分隔符替换\n",
        "    trans = {\n",
        "        \"-\": random.choice([\"—\",\"-\",\"–\"]),\n",
        "        \"/\": random.choice([\"/\",\"／\",\"∕\"]),\n",
        "        \".\": random.choice([\".\",\"·\",\"．\"]),\n",
        "        \":\": random.choice([\":\",\"：\"])\n",
        "    }\n",
        "    return \"\".join(trans.get(ch, ch) for ch in s)\n",
        "\n",
        "# 核心：插入字符并维护实体跨度\n",
        "\n",
        "def insert_char_with_spans_multi(text: str,\n",
        "                                 spans: List[Tuple[int,int]],\n",
        "                                 p: Optional[int] = None,\n",
        "                                 ch: Optional[str] = None):\n",
        "    if not text:\n",
        "        return text, spans\n",
        "    if p is None:\n",
        "        p = random.randint(0, len(text))\n",
        "    if ch is None:\n",
        "        ch = random.choice(INSERT_CHARS)\n",
        "\n",
        "    new_text = text[:p] + ch + text[p:]\n",
        "    new_spans: List[Tuple[int,int]] = []\n",
        "    for (s, e) in spans:\n",
        "        if p < s:\n",
        "            new_spans.append((s+1, e+1))         # 插到实体前 -> 整段右移\n",
        "        elif s <= p < e:\n",
        "            new_spans.append((s, e+1))           # 插到实体内 -> 扩展实体右端\n",
        "        else:\n",
        "            new_spans.append((s, e))             # 插到实体后 -> 不变\n",
        "    return new_text, new_spans\n",
        "\n",
        "\n",
        "# 对整个字符串加噪，并保持实体下标对齐\n",
        "def apply_noise(text: str,\n",
        "                spans: Dict[str, Optional[Tuple[int,int]]],\n",
        "                lang: str) -> Tuple[str, List[Tuple[int,int]]]:\n",
        "    \"\"\"\n",
        "    仅用不会破坏“整体可被识别为日期/地址”的噪声：\n",
        "    - 大小写扰动（en/da/de/it/fr/sv/nl/pt/es/tr/ru部分拉丁字母）\n",
        "    - 全/半角替换（中日朝/数字/标点）\n",
        "    - 分隔符替换\n",
        "    - 插入字符（需维护跨度）\n",
        "    \"\"\"\n",
        "    s = text\n",
        "\n",
        "    # 仅对拉丁字母系语言做大小写扰动\n",
        "    latin_langs = {\"英语\",\"丹麦语\",\"德语\",\"意大利语\",\"法语\",\"瑞典语\",\n",
        "                   \"荷兰语\",\"葡萄牙语\",\"西班牙语\",\"土耳其语\"}\n",
        "    # 大小写\n",
        "    if lang in latin_langs and random.random() < NOISE_PROB_CASE:\n",
        "        s = toggle_case(s)\n",
        "    # 全/半角\n",
        "    if random.random() < NOISE_PROB_FULLWIDTH:\n",
        "        s = fullwidth_halfwidth_flip(s)\n",
        "    # 分隔符\n",
        "    if random.random() < NOISE_PROB_SEP_REPLACE:\n",
        "        s = replace_separators(s)\n",
        "    # 插入字符（影响span）\n",
        "    s, spans = insert_char_with_spans_multi(s, spans, NOISE_PROB_INSERT)\n",
        "    return s, spans\n",
        "\n",
        "\n",
        "# 仅对 ADDRESS（可多段）进行 BIO 标注\n",
        "def spans_to_bio_addresses(text: str,\n",
        "                           addr_spans: List[Tuple[int,int]]) -> Tuple[List[str], List[str]]:\n",
        "    enc = tokenizer(text, add_special_tokens=False, return_offsets_mapping=True)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"])\n",
        "    offsets = enc[\"offset_mapping\"]\n",
        "\n",
        "    # 计算重叠长度\n",
        "    def overlap_len(a, b):\n",
        "        s = max(a[0], b[0]); e = min(a[1], b[1])\n",
        "        return max(0, e - s)\n",
        "\n",
        "    labels = []\n",
        "    for (s, e) in offsets:\n",
        "        tok_span = (s, e)\n",
        "        # 找与该 token 重叠最多的 address 段\n",
        "        best = None\n",
        "        best_ol = 0\n",
        "        for span in addr_spans:\n",
        "            ol = overlap_len(tok_span, span)\n",
        "            if ol > best_ol:\n",
        "                best_ol = ol; best = span\n",
        "        if best is None or best_ol == 0:\n",
        "            labels.append(\"O\")\n",
        "            continue\n",
        "        # 如果开头字符被包含在该token中，标记为B-\n",
        "        is_begin = (s <= best[0] < e)\n",
        "        labels.append((\"B-\" if is_begin else \"I-\") + \"ADDRESS\")\n",
        "    return tokens, labels\n",
        "\n",
        "\n",
        "\n",
        "# 选择非地址文本（优先用当前语言的 negatives 池；退化到内置若干短语）\n",
        "def choose_non_address(lang: str, non_addr_map: Dict[str, List[str]]) -> str:\n",
        "    pool = non_addr_map.get(lang, [])\n",
        "    return random.choice(pool) if pool else \"\"\n",
        "\n",
        "\n",
        "\n",
        "# ================== 组装样本 ==================\n",
        "\n",
        "PLACEHOLDER_RE = re.compile(r'\\{address\\}')\n",
        "\n",
        "\n",
        "# def make_sample_from_template(template: str,\n",
        "#                               t_text: Optional[str],\n",
        "#                               a_text: Optional[str]) -> Tuple[str, Optional[Tuple[int,int]], Optional[Tuple[int,int]]]:\n",
        "#     \"\"\"\n",
        "#     用正则分段拼接，逐段累积长度来记录 {time}/{address} 的起止位置。\n",
        "#     避免先 .replace() 再 .index() 引发的偏移/跨越问题；也避免多语标点导致的混淆。\n",
        "#     若模板里占位符出现多次，只标注“第一处”实体（其他处视为普通文本），保证一条样本只有一个 TIME/ADDRESS。\n",
        "#     \"\"\"\n",
        "#     parts = PLACEHOLDER_RE.split(template)  # 会保留分隔符\n",
        "#     out_chunks: List[str] = []\n",
        "#     time_span = None\n",
        "#     addr_span = None\n",
        "\n",
        "#     for seg in parts:\n",
        "#         if seg == \"{time}\" and (t_text is not None):\n",
        "#             start = sum(len(x) for x in out_chunks)\n",
        "#             out_chunks.append(t_text)\n",
        "#             if time_span is None:                # 只记录第一处\n",
        "#                 time_span = (start, start + len(t_text))\n",
        "#         elif seg == \"{address}\" and (a_text is not None):\n",
        "#             start = sum(len(x) for x in out_chunks)\n",
        "#             out_chunks.append(a_text)\n",
        "#             if addr_span is None:                # 只记录第一处\n",
        "#                 addr_span = (start, start + len(a_text))\n",
        "#         else:\n",
        "#             out_chunks.append(seg)\n",
        "\n",
        "#     text = \"\".join(out_chunks)\n",
        "#     return text, time_span, addr_span\n",
        "\n",
        "\n",
        "\n",
        "# 将模板中的每个 {address} 依次替换为“地址”或“非地址”文本；\n",
        "# 仅对“替换为地址”的片段记录 span，其余（非地址）不记录 -> 标为 O\n",
        "def fill_template_with_addresses(template: str,\n",
        "                                 lang: str,\n",
        "                                 addrs_map: Dict[str, List[str]],\n",
        "                                 non_addr_map: Dict[str, List[str]],   # CHANGED: 新增\n",
        "                                 neg_prob: float = NEG_ADDR_PROB\n",
        "                                 ) -> Tuple[str, List[Tuple[int,int]]]:\n",
        "    addr_spans: List[Tuple[int,int]] = []\n",
        "    out = []\n",
        "    cursor = 0\n",
        "    for m in PLACEHOLDER_RE.finditer(template):\n",
        "        # 先加占位符之前的原文\n",
        "        out.append(template[cursor:m.start()])\n",
        "        # non_address修改处\n",
        "        # 按比例选择正/负文本\n",
        "        if random.random() < neg_prob:\n",
        "            rep = choose_non_address(lang, non_addr_map)              # 负例：不应被识别为地址\n",
        "            # 不记录 span -> 该段将被标为 O\n",
        "        else:\n",
        "            rep = choose_addr(lang, addrs_map)          # 正例：应标为 ADDRESS\n",
        "            start = sum(len(s) for s in out)\n",
        "            out.append(rep)\n",
        "            addr_spans.append((start, start + len(rep)))\n",
        "            cursor = m.end()\n",
        "            continue\n",
        "        out.append(rep)\n",
        "        cursor = m.end()\n",
        "    # 结尾剩余文本\n",
        "    out.append(template[cursor:])\n",
        "    text = \"\".join(out)\n",
        "    return text, addr_spans\n",
        "\n",
        "\n",
        "\n",
        "def strong_positive_from_entity(ent_text: str, ent_type: str, lang: str) -> Tuple[str, Optional[Tuple[int,int]], Optional[Tuple[int,int]]]:\n",
        "    \"\"\"\n",
        "    构造几乎全实体的样本（只有 address 或只有 time）\n",
        "    \"\"\"\n",
        "    if ent_type == \"TIME\":\n",
        "        s = ent_text\n",
        "        return s, (0, len(s)), None\n",
        "    else:\n",
        "        s = ent_text\n",
        "        return s, None, (0, len(s))\n",
        "\n",
        "\n",
        "def _compute_targets(addr_tpls: List[str], n_per_lang: Optional[int]) -> Tuple[int,int,bool]:\n",
        "    \"\"\"\n",
        "    返回 (pos_target, neg_target, strict_once)\n",
        "    - 若 n_per_lang 为空：pos_target = 模板条数；neg_target = pos * RATIO_NEG / (1 - RATIO_NEG)\n",
        "      并置 strict_once=True（正例优先“一模板一条”）\n",
        "    - 若 n_per_lang 给定：按配额计算，strict_once=False（允许随机采样模板补足）\n",
        "    \"\"\"\n",
        "    if n_per_lang is None:\n",
        "        pos = len(addr_tpls)\n",
        "        denom = max(1e-9, (1.0 - RATIO_NEG))\n",
        "        neg = int(round(pos * RATIO_NEG / denom))\n",
        "        return pos, neg, True\n",
        "    else:\n",
        "        neg = int(n_per_lang * RATIO_NEG)\n",
        "        pos = max(0, n_per_lang - neg)\n",
        "        return pos, neg, False\n",
        "\n",
        "\n",
        "\n",
        "# 构造主逻辑\n",
        "def build_for_lang(lang: str,\n",
        "                   templates: Dict[str, List[str]],\n",
        "                   addrs_map: Dict[str, List[str]],\n",
        "                   non_addr_map: Dict[str, List[str]],      # CHANGED: 新增\n",
        "                   preview_n: int = 0,\n",
        "                   n_per_lang: Optional[int] = None) -> List[Tuple[List[str],List[str]]]:\n",
        "\n",
        "    samples_tok: List[Tuple[List[str],List[str]]] = []\n",
        "    addr_tpls = templates.get(\"addr_only\", []) or [\"{address}\"]\n",
        "    neg_tpls  = templates.get(\"neg_sent\", [])   or []  # 整句负例模板（来自 address_templates_0）\n",
        "\n",
        "    # 统一计算目标条数\n",
        "    if n_per_lang is None:\n",
        "        pos_target = len(addr_tpls)\n",
        "        denom = max(1e-9, (1.0 - RATIO_NEG))\n",
        "        neg_target = int(round(pos_target * RATIO_NEG / denom))\n",
        "        strict_once = True\n",
        "    else:\n",
        "        neg_target = int(n_per_lang * RATIO_NEG)\n",
        "        pos_target = max(0, n_per_lang - neg_target)\n",
        "        strict_once = False\n",
        "\n",
        "    # === 正例：模板内部仍按 NEG_ADDR_PROB 注占位负片段 ===\n",
        "    if strict_once and pos_target == len(addr_tpls):\n",
        "        tpl_iter = addr_tpls\n",
        "    else:\n",
        "        tpl_iter = (random.choice(addr_tpls) for _ in range(pos_target))\n",
        "\n",
        "    for tpl in tpl_iter:\n",
        "        if \"{address}\" not in tpl:\n",
        "            txt, _ = apply_noise(tpl, [], lang)\n",
        "            tokens, labels = spans_to_bio_addresses(txt, [])\n",
        "            samples_tok.append((tokens, labels))\n",
        "            continue\n",
        "        txt, addr_spans = fill_template_with_addresses(tpl, lang, addrs_map, non_addr_map, NEG_ADDR_PROB)\n",
        "        txt, addr_spans = apply_noise(txt, addr_spans, lang)\n",
        "        tokens, labels = spans_to_bio_addresses(txt, addr_spans)\n",
        "        samples_tok.append((tokens, labels))\n",
        "\n",
        "\n",
        "    # === 整句负例：优先用 address_templates_0 的模板；否则回退到 non_address.txt 直接成句 ===\n",
        "    if neg_target > 0:\n",
        "        if neg_tpls:\n",
        "            # 用整句负例模板构造全 O 的句子\n",
        "            for tpl in (random.choice(neg_tpls) for _ in range(neg_target)):\n",
        "                s = fill_template_all_non_address(tpl, lang, non_addr_map)  # 全部用非地址替换\n",
        "                s, _ = apply_noise(s, [], lang)\n",
        "                tokens, labels = spans_to_bio_addresses(s, [])\n",
        "                samples_tok.append((tokens, labels))\n",
        "        else:\n",
        "            print(\"\"\"😿注意：没有找到整句负例模板，直接取非地址片段组成句子,可能影响质量\n",
        "                    如需添加负例句子，请将上面的模板生成文件地址配置改为\n",
        "                    ADDRESS_SLOT_LIST   = [0]\n",
        "                    TARGET_PER_SLOT     = [300（请改成想要的负例句子数）] \"\"\")\n",
        "            sys.exit(1)\n",
        "    # 预览\n",
        "    if preview_n > 0:\n",
        "        print(f\"\\n[Preview {lang}] =====\")\n",
        "        for i in range(min(preview_n, len(samples_tok))):\n",
        "            toks, labs = samples_tok[i]\n",
        "            print(\"TOK:\", \" \".join(toks))\n",
        "            print(\"LBL:\", \" \".join(labs))\n",
        "    return samples_tok\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ================== 写 CONLL ==================\n",
        "def write_conll(path: str, data: List[Tuple[List[str],List[str]]]):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for toks, labs in data:\n",
        "            for t, y in zip(toks, labs):\n",
        "                f.write(f\"{t} {y}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "# ================== 主入口 ==================\n",
        "def main():\n",
        "    n_per_lang = int(N_PER_LANG) if N_PER_LANG else None\n",
        "    preview = PREVIEW_N\n",
        "\n",
        "    addrs_map = {lang: read_lines(os.path.join(REAL_ADDR_ROOT, f\"{lang}.txt\")) for lang in LANGS}\n",
        "    non_addr_map = {lang: read_lines(os.path.join(NON_ADDR_ROOT,  f\"{lang}.txt\")) for lang in LANGS}\n",
        "\n",
        "    merged = []\n",
        "    for lang in LANGS:\n",
        "        templates_per_lang = load_templates_for_lang(lang)\n",
        "        data = build_for_lang(\n",
        "            lang=lang,\n",
        "            templates=templates_per_lang,\n",
        "            addrs_map=addrs_map,\n",
        "            non_addr_map=non_addr_map,\n",
        "            preview_n=preview,\n",
        "            n_per_lang=n_per_lang\n",
        "        )\n",
        "        write_conll(PER_LANG_CONLL(lang), data)\n",
        "        merged.extend(data)\n",
        "        print(f\"[OK] {lang}: {len(data)} samples -> {PER_LANG_CONLL(lang)}\")\n",
        "\n",
        "    # 合并打乱\n",
        "    random.shuffle(merged)\n",
        "    write_conll(MERGED_CONLL, merged)\n",
        "    print(f\"\\n[OK] merged: {len(merged)} samples -> {MERGED_CONLL}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed()\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "aXRDFYrQ_XoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2915a49-a87b-459f-ad66-f6be247a9042",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "云盘挂载成功\n",
            "\n",
            "[Preview 简体中文] =====\n",
            "TOK: ▁ , 拜 访 客户的 地址 是 4 栋 , 之后 我们 去 燕 岗 吃饭 。\n",
            "LBL: O O O O O O O O O O O O O B-ADDRESS I-ADDRESS O O\n",
            "TOK: ▁/ 会议 已经 改 到 青海 湖 乡 二 郎 剑 村 26 组 115 号 进行 , 请 知 悉 。\n",
            "LBL: O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O\n",
            "TOK: ▁ 、 对 不起 , 您 提供的 耀 才 證券 暂时 无法 送 货 。\n",
            "LBL: O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O O O O O\n",
            "TOK: ▁ 。 会议 将于 D ong chen ▁retail ▁of ▁machine ry 举行 , 请 从 联 旺 信息 科技 入口 进入 , 并在 藁 城区 廉 州 镇 第二 中学 签 到 。\n",
            "LBL: O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O\n",
            "TOK: ▁ , 会议 地点 已经 确定 在 曲 阳 街道 社 保 中心 , 请 准 时 参加 。\n",
            "LBL: O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O\n",
            "[OK] 简体中文: 10000 samples -> /content/drive/MyDrive/NER/train_datasets/train_简体中文.conll\n",
            "\n",
            "[Preview 繁体中文] =====\n",
            "TOK: ▁ 。 今天的 日程 是 : 上午 前往 白 雲 區 白 雲 大道 , 下午 轉 往 美國 研究 三角 園區 ▁A 53 廠 房 。\n",
            "LBL: O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TOK: ▁ 等 一下 去 6 棟 辦 點 事 , 然後 回到 沙漠 鎮 星 空 村 74 組 355 號 再 告訴你 進 度 。\n",
            "LBL: O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O\n",
            "TOK: ▁ 、 今天 我們的 拍攝 地 點選 在了 4 棟 , 快 來 參加 吧 !\n",
            "LBL: O O O O O O O O O O O O O O O O\n",
            "TOK: ▁— 您 好 , 請 於 2 棟 簽 收 您的 包裹 。\n",
            "LBL: O O O O O O O O O O O O O\n",
            "TOK: ▁/ 最近 天氣 冷 , 我們 從 廣 西 壯 族 自治 區 南 寧 市 青 秀 區 民族 大道 123 號 出發 , 直接 到 波 蘭 克拉 科 夫 科技 園區 ▁E 83 廠 房 吧 !\n",
            "LBL: O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O\n",
            "[OK] 繁体中文: 10000 samples -> /content/drive/MyDrive/NER/train_datasets/train_繁体中文.conll\n",
            "\n",
            "[Preview 英语] =====\n",
            "TOK: ▁/ Main ten ance ▁teams ▁will ▁inspect ▁both ▁Building ▁3 ▁and ▁Building ▁6 ▁today .\n",
            "LBL: O O O O O O O O O O O O O O O\n",
            "TOK: ▁/ I ’ ll ▁text ▁you ▁the ▁details ▁once ▁I ▁reach ▁Tam worth ▁Road ▁Resource ▁Centre . ▁If ▁there ’ s ▁a ▁change , ▁I ’ ll ▁call ▁from ▁Bi d dul ph ▁Man sions ▁or ▁Plaza ▁Mayor ▁15 , ▁280 12 ▁Madrid , ▁España . ▁Meet ▁me ▁directly ▁at ▁West ▁Block .\n",
            "LBL: O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O B-ADDRESS I-ADDRESS O\n",
            "TOK: ▁Make ▁sure ▁to ▁send ▁the ▁invitation ▁to ▁Building ▁6 ▁as ▁well ▁as ▁The ▁Green .\n",
            "LBL: O O O O O O O O O O O O B-ADDRESS I-ADDRESS O\n",
            "TOK: ▁ . D id ▁you ▁forget ▁the ▁direction s ▁to ▁Building ▁6 ▁after ▁leaving ▁Building ▁8 ▁yesterday ?\n",
            "LBL: O O O O O O O O O O O O O O O O O O\n",
            "TOK: ▁; j O IN ▁US ▁AT ▁e MER SON ▁p ARK ▁s O CIA L ▁h ALL ▁FOR ▁AN ▁EXCLUSIV E ▁WORK SHOP ▁TH IS ▁WE EK END .\n",
            "LBL: O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O O\n",
            "[OK] 英语: 10000 samples -> /content/drive/MyDrive/NER/train_datasets/train_英语.conll\n",
            "\n",
            "[Preview 丹麦语] =====\n",
            "TOK: ▁ 、 V ær ▁ venlig ▁at ▁bekræfte , ▁om ▁Sk jul høj gård ▁2. ▁sal ▁stadig ▁er ▁det ▁samme ▁som ▁Nø gl ens ▁Kvar ter ▁og ▁A AB ▁Vejle .\n",
            "LBL: O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O B-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TOK: ▁ , Jeg ▁arbejder ▁i ▁nærheden ▁af ▁Ry parken ▁St . .\n",
            "LBL: O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TOK: ▁ 、 Din ▁aftale ▁er ▁bekræfte t ▁på ▁M års let .\n",
            "LBL: O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TOK: ▁: Kan ▁du ▁sende ▁en ▁kopi ▁af ▁dokument et ▁til ▁Spi rea ▁Hus ▁og ▁også ▁til ▁DK -3 700 ▁Rø nne , ▁Store gade ▁6 , ▁1. ▁sal , ▁Bornholm ?\n",
            "LBL: O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TOK: ▁・ Jeg ▁er ▁netop ▁an komme t ▁til ▁Det ▁grønne ▁område . ▁Hvor ▁langt ▁er ▁du ▁fra ▁By g ning ▁4 ?\n",
            "LBL: O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O\n",
            "[OK] 丹麦语: 10000 samples -> /content/drive/MyDrive/NER/train_datasets/train_丹麦语.conll\n",
            "\n",
            "[Preview 俄语] =====\n",
            "TOK: ▁ 。 Вы ▁можете ▁найти ▁нас ▁по ▁адресу ▁З да ние ▁3 , ▁а ▁затем ▁зай ти ▁в ▁З да ние ▁9.\n",
            "LBL: O O O O O O O O O O O O O O O O O O O O O O\n",
            "TOK: ▁! Ва ш ▁сто лик ▁за резерв ирован ▁в ▁З да ние ▁1.\n",
            "LBL: O O O O O O O O O O O O O\n",
            "TOK: ▁; В стре ча емся ▁на ▁Г им на зия ▁No ▁66 ▁( началь ные ▁класс ы ), ▁после ▁чего ▁ ид ём ▁к ▁ООО ▁\" АС - Марк ет \". ▁Аль тер на тив ные ▁маршрут ы : ▁через ▁Ре пка ▁или ▁У чи лище ▁олимпий ского ▁резерв а ▁No ▁1.\n",
            "LBL: O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O B-ADDRESS I-ADDRESS O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS\n",
            "TOK: ▁; При вет ! ▁Ты ▁ помни шь ▁о ▁сегодня ш ней ▁встреч е ▁у ▁Вол ж ский ▁бул ь вар ? ▁Если ▁что , ▁я ▁уже ▁иду ▁через ▁Там ань Не ф те Газ . ▁В стре тим ся ▁у ▁Социал ьно - ре а били та ционно е ▁отделение ▁No ▁3 , ▁а ▁потом ▁по й дем ▁в ▁Парк ▁Петра ▁Алекс е ева .\n",
            "LBL: O O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TOK: ▁Для ▁получения ▁заказа ▁обрати тесь ▁в ▁Ше реме ть ев ская ▁улица ▁или ▁ЖК ▁« И ван - да - Мар ья », ▁либо ▁заб ерите ▁его ▁из ▁Гри б ные ▁ворота .\n",
            "LBL: O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "[OK] 俄语: 10000 samples -> /content/drive/MyDrive/NER/train_datasets/train_俄语.conll\n",
            "\n",
            "[Preview 土耳其语] =====\n",
            "TOK: ▁: Sam sun ▁Kap ısı ▁adresi ne ▁yakın ▁bir ▁otel ▁bul abilir ▁mi siniz ?\n",
            "LBL: B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O\n",
            "TOK: ▁ 。 T üm ▁belgeler i ▁Forum ▁Erzurum ▁adresi ne ▁gönder dim , ▁bir ▁kop yası ▁da ▁19 A ▁Blok ▁adres inde ▁olacak .\n",
            "LBL: O O O O O O B-ADDRESS I-ADDRESS O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O O O O\n",
            "TOK: ▁ . 3 ▁b L OK ▁A DRE SIN DE ▁Gö R üş ü R ü Z , ▁BI TIN CE ▁m ETE ORO L OJI ▁mü D ü RL ü ğü ▁AD RES INE ▁GE ç ERI Z .\n",
            "LBL: O O O O O O O O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O\n",
            "TOK: ▁; D üz ce ▁Üniversitesi ▁Stad y umu ▁adres inde ▁bir ▁sürpriz ▁sizi ▁bekliyor !\n",
            "LBL: B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O\n",
            "TOK: ▁? 10 ▁b L OK ▁A DRE SIN DE KI ▁OF ISE ▁ ULA şı R ▁ ULA ş MAZ , ▁10 ▁b L OK ▁AD RES INE ▁GE ç ECE ğ IZ .\n",
            "LBL: O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "[OK] 土耳其语: 10000 samples -> /content/drive/MyDrive/NER/train_datasets/train_土耳其语.conll\n",
            "\n",
            "[Preview 德语] =====\n",
            "TOK: ▁・ B itte ▁ holen ▁Sie ▁die ▁Schlüssel ▁bei ▁Landes am t ▁für ▁Bürger - ▁und ▁Ordnung san gelegenheit en : ▁Referat ▁Fahrer la ub nisse , ▁Personen - ▁und ▁Gü ter be förderung ▁ab . ▁Alternativ ▁stehen ▁Apollo ▁Kino center ▁und ▁Car illon ▁zur ▁Verfügung .\n",
            "LBL: O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O B-ADDRESS I-ADDRESS I-ADDRESS O B-ADDRESS I-ADDRESS O O O\n",
            "TOK: ▁! Die ▁Zeit ▁zwischen ▁Bundes ministeri um ▁für ▁Verkehr ▁und ▁Betreuung s zentrum ▁Let te witz ▁beträgt ▁etwa ▁20 ▁Minuten ▁mit ▁dem ▁Auto .\n",
            "LBL: O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O\n",
            "TOK: ▁; Das ▁Konzert ▁war ▁bei ▁Roll berg ▁Kino , ▁aber ▁die ▁After party ▁fand ▁bei ▁Pflege zentrum ▁Kir ch dorf er ▁Heide ▁statt .\n",
            "LBL: O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O\n",
            "TOK: ▁・ War en ▁Sie ▁schon ▁bei ▁Re it stad ion ▁Gera — Mil bit z ? ▁Gebäude ▁5 ▁und ▁Sch lo ß hof ▁sind ▁auch ▁sehen s wert .\n",
            "LBL: O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O\n",
            "TOK: ▁! B itte ▁ holen ▁Sie ▁die ▁Lieferung ▁bei ▁der ▁Film Ra um ▁ab . ▁Falls ▁das ▁nicht ▁möglich ▁ist , ▁können ▁wir ▁sie ▁auch ▁zu ▁Gebäude ▁3 , ▁Pension ▁am ▁Vogel n est ▁oder ▁Gebäude ▁9 ▁liefern .\n",
            "LBL: O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O\n",
            "[OK] 德语: 10000 samples -> /content/drive/MyDrive/NER/train_datasets/train_德语.conll\n",
            "\n",
            "[Preview 意大利语] =====\n",
            "TOK: ▁ , C ia o , ▁hai ▁dimentica to ▁qualcosa ▁a ▁Anti ca ▁Scuola ▁Rural e ▁Di ▁Or be illa z ? ▁Puoi ▁rit ira rlo ▁a ▁Edi fici o ▁8. ▁Se ▁trovi ▁sco modo , ▁possiamo ▁spe dir lo ▁a ▁Gi ardi no ▁dei ▁Giu sti ▁del ▁Mondo ▁di ▁No venta ▁Pad ovana . ▁Fac ci ▁sapere ▁se ▁preferi sci ▁passare ▁da ▁Co tral .\n",
            "LBL: O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O B-ADDRESS I-ADDRESS O\n",
            "TOK: ▁; Per ▁favore , ▁in via ▁il ▁pa cco ▁a ▁Edi fici o ▁1 , ▁oppure ▁a ▁Parco ▁ Associazione ▁Min er aria ▁Sar da ▁o ▁O de on .\n",
            "LBL: O O O O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O B-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TOK: ▁Un ▁nuovo ▁negozio ▁è ▁stato ▁inaugura to ▁a ▁Villa ▁Annu nzi ata . ▁Da ▁vedere !\n",
            "LBL: O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O\n",
            "TOK: ▁ , Pu oi ▁contatta rmi ▁a ▁Stad io ▁Comunale ▁per ▁discuter e ▁i ▁dettagli . ▁In ▁alternativa , ▁in vi ami ▁una ▁proposta ▁a ▁Stad io ▁Alberto ▁Pin to ▁oppure ▁incontri amo ci ▁a ▁ Istituto ▁Compre ns ivo ▁IV .\n",
            "LBL: O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O B-ADDRESS B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TOK: ▁ 、 Il ▁nostro ▁prossimo ▁appuntamento ▁è ▁a ▁Edi fici o ▁2. ▁Tuttavia , ▁prima ▁dobbiamo ▁visitare ▁Edi fici o ▁3 ▁e ▁casa ▁al ▁lago . ▁L ' ultimo ▁incontro ▁sarà ▁a ▁Edi fici o ▁5.\n",
            "LBL: O O O O O O O O O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O\n",
            "[OK] 意大利语: 10000 samples -> /content/drive/MyDrive/NER/train_datasets/train_意大利语.conll\n",
            "\n",
            "[Preview 日语] =====\n",
            "TOK: ▁; 来 月の 研修 は 〒 990 — 08 21 ▁ 山 形 県 山 形 市 町 田 町 5 丁目 1 番 2 号 で 行 います 。 参加者 は TO LER ANT Z ▁MOTOR ING に 集合 してください 。 研修 資料 は 安 住 商店 で 配布 され 、 終了 後は 森林 センター 前 で アンケート をお願いします 。\n",
            "LBL: O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O O O O\n",
            "TOK: ▁? 新しい オフィス は 尾 張 町 児童 公園 で 、 旧 オフィス は 6 棟 にあります 。\n",
            "LBL: O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O\n",
            "TOK: ▁? 新しい カフェ が 有 料 老人 ホーム フ ルール ハ ピ ネ ス は こ だ て に オープン しました 。 さらに 4 棟 の 近くに も 支 店 があります 。 そして 6 棟 で 特別な イベント があり 、 最後に 中 島 なか よ し 公園 で 締め く く り です 。\n",
            "LBL: O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O\n",
            "TOK: ▁ 次の 旅行 先 は C - Plat z ▁CHA ▁CHA に しよう と思って る んだ !\n",
            "LBL: O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O\n",
            "TOK: ▁: 集合 時間 に 遅れ る 場合は 、 直接 9 棟 に 向かう か 、 ミニ オン ・ ハ チャ メ チャ ・ アイ ス または 大 豆 島 公園 に 連絡 してください 。\n",
            "LBL: O O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O\n",
            "[OK] 日语: 10000 samples -> /content/drive/MyDrive/NER/train_datasets/train_日语.conll\n",
            "\n",
            "[Preview 法语] =====\n",
            "TOK: ▁! Les ▁locaux ▁de ▁l ' entreprise ▁se ▁trouve nt ▁à ▁Coll ège ▁Saint - La ud ▁et ▁à ▁K ▁Proje t .\n",
            "LBL: O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O B-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TOK: ▁- Le ▁point ▁de ▁rendez - vous ▁est ▁fixé ▁à ▁Coco on .\n",
            "LBL: O O O O O O O O O O B-ADDRESS I-ADDRESS O\n",
            "TOK: ▁: ré SERV EZ ▁ UNE ▁T ABLE ▁P OUR ▁DE UX ▁à ▁p AR C ▁DU ▁v ILLA GE , ▁ET ▁UN ▁DES S ERT ▁à ▁p L ACE ▁s TAN IS LAS , ▁n ANC Y , ▁g RAN D ▁e ST , ▁5 4000 .\n",
            "LBL: O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TOK: ▁ , c E ▁R ESTA URAN T ▁à ▁bâ TI MENT ▁6 ▁EST ▁INC RO Y ABLE , ▁JE ▁RE COM M ANDE ▁V IV EMENT .\n",
            "LBL: O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "TOK: ▁; On ▁pourrait ▁se ▁voir ▁à ▁D OOR TAL ▁GL ASS , ▁ou ▁alors ▁à ▁Ré siden ce ▁La ▁Dom no née , ▁et ▁pourquoi ▁pas ▁à ▁La ▁plain e .\n",
            "LBL: O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "[OK] 法语: 10000 samples -> /content/drive/MyDrive/NER/train_datasets/train_法语.conll\n",
            "\n",
            "[Preview 瑞典语] =====\n",
            "TOK: ▁ , Vi ▁börjar ▁vid ▁Bygg nad ▁9 ▁och ▁avsluta r ▁dagen ▁vid ▁Bygg nad ▁1.\n",
            "LBL: O O O O O O O O O O O O O O O O\n",
            "TOK: ▁ , V än ligen ▁ följ ▁väg beskrivning en ▁till ▁O lof ▁Palm es ▁Plat s ▁för ▁att ▁delta .\n",
            "LBL: O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O\n",
            "TOK: ▁ . S ka ▁vi ▁ta ▁en ▁promenad ▁till ▁Me mbro ?\n",
            "LBL: O O O O O O O O O B-ADDRESS I-ADDRESS O\n",
            "TOK: ▁; vä N LI GEN ▁BE KR ä FTA ▁DIN ▁Nä RV ARO ▁ VID ▁b Y GG NAD ▁9 ▁OCH ▁s AN KT ▁r AG N HIL DS ▁Kä LL A .\n",
            "LBL: O O O O O O O O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TOK: ▁/ Kan ▁du ▁ge ▁mig ▁väg beskrivning ▁till ▁Amb assa der na ▁från ▁Kung sgatan ▁33 ▁Ap t ▁6 A , ▁111 ▁30 ▁Stockholm , ▁Sverige ?\n",
            "LBL: O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "[OK] 瑞典语: 10000 samples -> /content/drive/MyDrive/NER/train_datasets/train_瑞典语.conll\n",
            "\n",
            "[Preview 荷兰语] =====\n",
            "TOK: ▁; De ▁beste ▁hotels ▁in ▁de ▁buurt ▁zijn ▁bij ▁Hof stad ▁Ly ce um ▁en ▁Eindhoven ▁Centra al .\n",
            "LBL: O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O B-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TOK: ▁; De ▁repara ties ▁aan ▁Bos jes ▁van ▁Pe x ▁zijn ▁voltooi d .\n",
            "LBL: O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O\n",
            "TOK: ▁De ▁ vergadering ▁wordt ▁gehouden ▁op ▁Ge bouw ▁9.\n",
            "LBL: O O O O O O O O O\n",
            "TOK: ▁? Wat ▁is ▁jouw ▁favoriete ▁plek ? ▁Is ▁het ▁Nicola as school , ▁Ge bouw ▁4 ▁of ▁misschien ▁G TO ▁Plat ing ?\n",
            "LBL: O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TOK: ▁? Ik ▁stuur ▁een ▁ kaartje ▁vanaf ▁Ge bouw ▁3 , ▁maar ▁ik ▁kan ▁ook ▁langs ▁In ▁het ▁Hart ▁van ▁het ▁Hou t ▁en ▁Edi son straat ▁gaan .\n",
            "LBL: O O O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O B-ADDRESS I-ADDRESS I-ADDRESS O O\n",
            "[OK] 荷兰语: 10000 samples -> /content/drive/MyDrive/NER/train_datasets/train_荷兰语.conll\n",
            "\n",
            "[Preview 葡萄牙语] =====\n",
            "TOK: ▁; V amos ▁começar ▁em ▁Comissão ▁de ▁Pro te cção ▁de ▁Criança s ▁e ▁Jove ns ▁das ▁La jes ▁do ▁Pic o , ▁então ▁seguir ▁para ▁Pra ça ▁do ▁Comércio , ▁no ▁25 , ▁1 a ▁Torre , ▁Lisboa , ▁1100 - 000 , ▁Portugal ▁e ▁terminar ▁em ▁Re bord osa ▁( Val es ).\n",
            "LBL: O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS\n",
            "TOK: ▁A ▁entrega ▁será ▁enviada ▁para ▁Alam eda ▁Ary ▁dos ▁Santos .\n",
            "LBL: O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TOK: ▁O ▁evento ▁começar á ▁no ▁Antes ▁CZ ▁Fi gue ir inhas ▁e ▁termina rá ▁no ▁ muse . ai .\n",
            "LBL: O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O B-ADDRESS B-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TOK: ▁! e STA MOS ▁ AGU ARD ANDO ▁VOC ê ▁EM ▁e D IF í CIO ▁3 ▁E ▁EM ▁S EGU IDA ▁EM ▁b EMP OST A ▁( r ▁c OMBA TEN TES ) ▁c HA FAR IZ .\n",
            "LBL: O O O O O O O O O O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TOK: ▁; a L GU é M ▁S ABE ▁COMO ▁CHE GAR ▁A ▁v I TER BO ▁c AMP OS ▁ RAP ID AMENTE ?\n",
            "LBL: O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O\n",
            "[OK] 葡萄牙语: 10000 samples -> /content/drive/MyDrive/NER/train_datasets/train_葡萄牙语.conll\n",
            "\n",
            "[Preview 西班牙语] =====\n",
            "TOK: ▁¿ No s ▁vemos ▁en ▁A ▁La xe ▁para ▁cena r ?\n",
            "LBL: O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O O O O\n",
            "TOK: ▁— El ▁programa ▁será ▁en ▁El ▁Cast ilo . ▁Ha brá ▁actividades ▁adicional es ▁en ▁Edi fici o ▁9. ▁Los ▁invitados ▁especiales ▁estarán ▁en ▁Edi fici o ▁6 ▁y ▁después ▁en ▁Plaza ▁de ▁Toro s .\n",
            "LBL: O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TOK: ▁: ¿ Sab ías ▁que ▁hay ▁una ▁tienda ▁nueva ▁en ▁Edi fici o ▁6 ?\n",
            "LBL: O O O O O O O O O O O O O O O\n",
            "TOK: ▁— El ▁servicio ▁técnico ▁está ▁ubicado ▁en ▁Consorci ▁per ▁la ▁Normal ització ▁Lingüística .\n",
            "LBL: O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TOK: ▁・ Si ▁vas ▁a ▁Edi fici ▁Ei vissa , ▁no ▁olvide s ▁visitar ▁Centro ▁Comercial ▁De za ▁y ▁también ▁Re siden cia ▁de ▁Mayor es ▁Mad re ▁de ▁Dios .\n",
            "LBL: O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "[OK] 西班牙语: 10000 samples -> /content/drive/MyDrive/NER/train_datasets/train_西班牙语.conll\n",
            "\n",
            "[Preview 韩语] =====\n",
            "TOK: ▁! 약 속 ▁장소 는 ▁벌 교 생 태 공원 고 , ▁이후 에는 ▁롯데 시 네 마 ▁상 주 로 ▁이동 할 ▁계획 입니다 .\n",
            "LBL: O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O\n",
            "TOK: ▁? 다 음 ▁주 에 ▁상 모 사 곡 동 ▁행정 복지 센터 에서 ▁ 워크 숍 이 ▁열 립니다 . ▁3 동 와 ▁ 옥 동 초등학교 도 ▁일정 에 ▁포함 됩니다 .\n",
            "LBL: O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O B-ADDRESS B-ADDRESS I-ADDRESS I-ADDRESS O O O O O O\n",
            "TOK: ▁; 다 음 ▁모 임 ▁장소 는 ▁8 동 이고 , ▁이후 ▁저녁 은 ▁116 동 에서 ▁진행 될 ▁예정 입니다 .\n",
            "LBL: O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS O O O O O O\n",
            "TOK: ▁ 、 긴 급 ▁상황 ▁발생 : ▁3 동 로 ▁지금 ▁바로 ▁이동 해주세요 . ▁이후 ▁2 동 에서 ▁대 기 하시기 ▁바랍니다 . ▁이후 ▁배 드 민 턴 클럽 에서 ▁지 시 를 ▁받고 ▁마지막 으로 ▁광주 광역시 교육 청 로 ▁이동 해 ▁주세요 .\n",
            "LBL: O O O O O O O O O O O O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O\n",
            "TOK: ▁· 혹 시 ▁ 해양 경 찰 서 ▁근처 에 ▁있는 ▁한반도 의 ▁풍경 ▁아 세요 ?\n",
            "LBL: O O O B-ADDRESS B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O B-ADDRESS I-ADDRESS I-ADDRESS O O O\n",
            "[OK] 韩语: 10000 samples -> /content/drive/MyDrive/NER/train_datasets/train_韩语.conll\n",
            "\n",
            "[OK] merged: 150000 samples -> /content/drive/MyDrive/NER/train_datasets/train_all.conll\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 生成训练数据（CONLL）（混合模板） {\"form-width\":\"40%\"}\n",
        "from google.colab import drive\n",
        "import os, re, json, random, unicodedata, sys\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "print(\"云盘挂载成功\")\n",
        "\n",
        "# ======== 可修改配置 ========\n",
        "N_PER_LANG = 500 # @param {\"type\":\"raw\",\"placeholder\":\"请输入每个语言的训练数据量，不输入默认遍历所有模板\"}\n",
        "RATIO_NEG = 0.15 # @param {\"type\":\"number\",\"placeholder\":\"整句负例占比（相对每语言总量）\"}\n",
        "NEG_ADDR_PROB = 0.2  # @param {\"type\":\"number\",\"placeholder\":\"插入地址负例的概率\"}\n",
        "NEG_TIME_PROB = 0.2  # @param {\"type\":\"number\",\"placeholder\":\"插入时间负例的概率\"}\n",
        "# 预览， >0 时打印样例\n",
        "PREVIEW_N = 5  # @param {\"type\":\"integer\",\"placeholder\":\"请输入预览数据量\"}\n",
        "\n",
        "#数据比例控制\n",
        "per_00=0.10# @param\n",
        "per_11=0.30# @param\n",
        "\n",
        "# 噪声概率\n",
        "NOISE_PROB_CASE = 0.15\n",
        "NOISE_PROB_FULLWIDTH = 0.10\n",
        "NOISE_PROB_SEP_REPLACE = 0.10\n",
        "NOISE_PROB_INSERT = 0.10\n",
        "INSERT_CHARS = [\",\", \".\", \"·\", \"—\", \"-\", \"/\", \":\", \"、\", \"・\",\"?\",\"？\", \"！\", \"!\", \"，\", \"。\", \"；\", \";\", \" \"]\n",
        "RATIO_STRONG_POS = 0.10\n",
        "\n",
        "LANGS = [\"简体中文\",\"繁体中文\", \"英语\",\"丹麦语\",\"俄语\",\"土耳其语\",\"德语\",\"意大利语\",\"日语\",\n",
        "         \"法语\",\"瑞典语\",\"荷兰语\",\"葡萄牙语\",\"西班牙语\",\"韩语\"]\n",
        "\n",
        "# ====== 新路径结构 ======\n",
        "COMBINE_TPL_ROOT = \"/content/drive/MyDrive/NER/combine_templates\"\n",
        "REAL_ADDR_ROOT = \"/content/drive/MyDrive/NER/real_address\"\n",
        "REAL_TIME_ROOT = \"/content/drive/MyDrive/NER/real_time\"\n",
        "NON_ADDR_ROOT  = \"/content/drive/MyDrive/NER/non_address\"\n",
        "NON_TIME_ROOT  = \"/content/drive/MyDrive/NER/non_time\"\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/NER/train_datasets_AC\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "PER_LANG_CONLL = lambda lang: os.path.join(OUT_DIR, f\"train_{lang}.conll\")\n",
        "MERGED_CONLL = os.path.join(OUT_DIR, \"train_all.conll\")\n",
        "\n",
        "TOKENIZER_NAME = \"xlm-roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, use_fast=True)\n",
        "\n",
        "# ===== 基础 IO =====\n",
        "def read_lines(path: str) -> List[str]:\n",
        "    if not os.path.exists(path):\n",
        "        return []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return [ln.strip() for ln in f if ln.strip()]\n",
        "\n",
        "def read_jsonl_templates(path: str) -> list:\n",
        "    items = []\n",
        "    if not os.path.exists(path):\n",
        "        return items\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for ln in f:\n",
        "            ln = ln.strip()\n",
        "            if not ln:\n",
        "                continue\n",
        "            try:\n",
        "                obj = json.loads(ln)\n",
        "                t = obj.get(\"template\")\n",
        "                if isinstance(t, str) and t.strip():\n",
        "                    items.append(\" \".join(t.split()))\n",
        "            except Exception:\n",
        "                continue\n",
        "    return items\n",
        "\n",
        "def load_templates_for_lang(lang: str) -> Dict[str, list]:\n",
        "    # 分三类：正例11、正例其他、纯负例00\n",
        "    pos_11_tpls, pos_other_tpls, neg_sent_tpls = [], [], []\n",
        "    filename = f\"{lang}_combine_templates.jsonl\"\n",
        "    for name in os.listdir(COMBINE_TPL_ROOT):\n",
        "        if not name.startswith(\"combine_templates_\"):\n",
        "            continue\n",
        "        subdir = os.path.join(COMBINE_TPL_ROOT, name)\n",
        "        if not os.path.isdir(subdir):\n",
        "            continue\n",
        "        fpath = os.path.join(subdir, filename)\n",
        "        items = read_jsonl_templates(fpath)\n",
        "        if not items:\n",
        "            continue\n",
        "        slot = name[len(\"combine_templates_\"):]\n",
        "        if slot.isdigit():\n",
        "            if int(slot) == 0:\n",
        "                neg_sent_tpls.extend(items)     # 纯负例\n",
        "            elif int(slot) == 11:\n",
        "                pos_11_tpls.extend(items)       # 正例 11\n",
        "            else:\n",
        "                pos_other_tpls.extend(items)    # 正例 其他\n",
        "    return {\n",
        "        \"pos_11\": pos_11_tpls,\n",
        "        \"pos_other\": pos_other_tpls,\n",
        "        \"neg_sent\": neg_sent_tpls\n",
        "    }\n",
        "\n",
        "# ===== 噪声处理（保留原逻辑） =====\n",
        "def toggle_case(s: str) -> str:\n",
        "    def _flip(c):\n",
        "        if \"a\" <= c <= \"z\": return c.upper()\n",
        "        if \"A\" <= c <= \"Z\": return c.lower()\n",
        "        return c\n",
        "    return \"\".join(_flip(c) for c in s)\n",
        "\n",
        "def to_fullwidth(c: str) -> str:\n",
        "    code = ord(c)\n",
        "    if 33 <= code <= 126:\n",
        "        return chr(code + 0xFEE0)\n",
        "    if c == \" \":\n",
        "        return \"\\u3000\"\n",
        "    return c\n",
        "\n",
        "def to_halfwidth(c: str) -> str:\n",
        "    if c == \"\\u3000\": return \" \"\n",
        "    code = ord(c)\n",
        "    if 65281 <= code <= 65374:\n",
        "        return chr(code - 0xFEE0)\n",
        "    return c\n",
        "\n",
        "def fullwidth_halfwidth_flip(s: str) -> str:\n",
        "    out=[]\n",
        "    for ch in s:\n",
        "        if unicodedata.east_asian_width(ch) in (\"F\",\"W\") and random.random()<0.5:\n",
        "            out.append(to_halfwidth(ch))\n",
        "        elif random.random() < 0.2:\n",
        "            out.append(to_fullwidth(ch))\n",
        "        else:\n",
        "            out.append(ch)\n",
        "    return \"\".join(out)\n",
        "\n",
        "def replace_separators(s: str) -> str:\n",
        "    trans = {\n",
        "        \"-\": random.choice([\"—\",\"-\",\"–\"]),\n",
        "        \"/\": random.choice([\"/\",\"／\",\"∕\"]),\n",
        "        \".\": random.choice([\".\",\"·\",\"．\"]),\n",
        "        \":\": random.choice([\":\",\"：\"])\n",
        "    }\n",
        "    return \"\".join(trans.get(ch, ch) for ch in s)\n",
        "\n",
        "def insert_char_with_spans_multi(text, spans, p=None, ch=None):\n",
        "    if not text: return text, spans\n",
        "    if p is None:\n",
        "      if random.random() < 0.5:   # 开头噪声概率50%\n",
        "        p = 0\n",
        "      else:\n",
        "        p = random.randint(1, len(text))\n",
        "    if ch is None:\n",
        "        ch = random.choice(INSERT_CHARS)\n",
        "    new_text = text[:p] + ch + text[p:]\n",
        "    new_spans=[]\n",
        "    for (s,e) in spans:\n",
        "        if p < s:\n",
        "            new_spans.append((s+1,e+1))\n",
        "        elif p == s:  # 插在实体起点\n",
        "            new_spans.append((s+1,e+1))\n",
        "        elif s < p < e:\n",
        "            new_spans.append((s,e+1))\n",
        "        else:\n",
        "            new_spans.append((s,e))\n",
        "    return new_text, new_spans\n",
        "\n",
        "def apply_noise(text: str, spans: List[Tuple[int,int]], lang: str) -> Tuple[str, List[Tuple[int,int]]]:\n",
        "    latin_langs = {...}\n",
        "    if lang in latin_langs and random.random() < NOISE_PROB_CASE:\n",
        "        text = toggle_case(text)\n",
        "    if random.random() < NOISE_PROB_FULLWIDTH:\n",
        "        text = fullwidth_halfwidth_flip(text)\n",
        "    if random.random() < NOISE_PROB_SEP_REPLACE:\n",
        "        text = replace_separators(text)\n",
        "    if random.random() < NOISE_PROB_INSERT:   # 只有命中概率才会插\n",
        "        text, spans = insert_char_with_spans_multi(text, spans)\n",
        "    return text, spans\n",
        "\n",
        "# ===== BIO 标注（支持 ADDRESS 和 TIME） =====\n",
        "def spans_to_bio_tokens(\n",
        "    text: str,\n",
        "    addr_spans: List[Tuple[int,int]],\n",
        "    time_spans: List[Tuple[int,int]]\n",
        "):\n",
        "    # 可替换成你原来的 tokenizer\n",
        "    enc = tokenizer(text, add_special_tokens=False, return_offsets_mapping=True)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"])\n",
        "    offsets = enc[\"offset_mapping\"]\n",
        "\n",
        "    # 定义噪声字符集合（和INSERT_CHARS类似，但用于token层）\n",
        "    NOISE_CHARS = set([\n",
        "        \"!\", \".\", \",\", \"·\", \"—\", \"-\", \"/\", \":\", \"、\", \"・\",\n",
        "        \"?\", \"？\", \"！\", \"，\", \"。\", \"；\", \";\",\n",
        "        \"(\", \")\", \"[\", \"]\", \"\\\"\", \"\\'\", \" \"\n",
        "    ])\n",
        "\n",
        "    def is_noise_token(tok_text: str) -> bool:\n",
        "        \"\"\"\n",
        "        判断一个分词后的token是否为纯噪声（去掉▁后只剩噪声字符）\n",
        "        \"\"\"\n",
        "        stripped = tok_text.lstrip(\"▁\")\n",
        "        # 如果去掉起始的▁后，剩下的每个字符都在NOISE_CHARS里，说明是纯噪声\n",
        "        return stripped != \"\" and all(ch in NOISE_CHARS for ch in stripped)\n",
        "\n",
        "    def assign_bio(spans, label_type):\n",
        "        res = [\"O\"] * len(tokens)\n",
        "        for sp in spans:\n",
        "            first_found = False\n",
        "            for i, (tok_s, tok_e) in enumerate(offsets):\n",
        "                # 判断token和span是否有重叠\n",
        "                if tok_e <= sp[0] or tok_s >= sp[1]:\n",
        "                    continue\n",
        "                if not first_found:\n",
        "                    tok_text = tokens[i]\n",
        "                    if is_noise_token(tok_text):\n",
        "                        # 纯噪声token，标为O，继续找下一个token做B\n",
        "                        res[i] = \"O\"\n",
        "                        continue\n",
        "                    else:\n",
        "                        res[i] = f\"B-{label_type}\"\n",
        "                        first_found = True\n",
        "                else:\n",
        "                    res[i] = f\"I-{label_type}\"\n",
        "        return res\n",
        "\n",
        "    addr_labels = assign_bio(addr_spans, \"ADDRESS\")\n",
        "    time_labels = assign_bio(time_spans, \"TIME\")\n",
        "\n",
        "    final_labels = []\n",
        "    for a_lbl, t_lbl in zip(addr_labels, time_labels):\n",
        "        if a_lbl != \"O\" and t_lbl == \"O\":\n",
        "            final_labels.append(a_lbl)\n",
        "        elif t_lbl != \"O\" and a_lbl == \"O\":\n",
        "            final_labels.append(t_lbl)\n",
        "        else:\n",
        "            final_labels.append(\"O\")\n",
        "\n",
        "    return tokens, final_labels\n",
        "\n",
        "# ===== 占位符替换 =====\n",
        "PLACEHOLDER_ADDR = re.compile(r'\\{address\\}')\n",
        "PLACEHOLDER_TIME = re.compile(r'\\{time\\}')\n",
        "\n",
        "def fill_template_with_entities(template: str, lang: str, addrs_map: Dict[str,List[str]], times_map: Dict[str,List[str]],\n",
        "                                 non_addr_map: Dict[str,List[str]], non_time_map: Dict[str,List[str]]) -> Tuple[str, List[Tuple[int,int]], List[Tuple[int,int]]]:\n",
        "    addr_spans=[]; time_spans=[]\n",
        "    out=[]; cursor=0\n",
        "    for m in re.finditer(r'\\{address\\}|\\{time\\}', template):\n",
        "        out.append(template[cursor:m.start()])\n",
        "        if m.group() == \"{address}\":\n",
        "            if random.random() < NEG_ADDR_PROB:\n",
        "                rep = random.choice(non_addr_map.get(lang, [\"\"]))  # negative\n",
        "            else:\n",
        "                rep = random.choice(addrs_map.get(lang, [\"Main Street\"]))\n",
        "                start=sum(len(s) for s in out)\n",
        "                addr_spans.append((start, start+len(rep)))\n",
        "            out.append(rep)\n",
        "        elif m.group() == \"{time}\":\n",
        "            if random.random() < NEG_TIME_PROB:\n",
        "                rep = random.choice(non_time_map.get(lang, [\"\"]))\n",
        "            else:\n",
        "                rep = random.choice(times_map.get(lang, [\"2024-07-01\"]))\n",
        "                start=sum(len(s) for s in out)\n",
        "                time_spans.append((start, start+len(rep)))\n",
        "            out.append(rep)\n",
        "        cursor=m.end()\n",
        "    out.append(template[cursor:])\n",
        "    return \"\".join(out), addr_spans, time_spans\n",
        "#新建方法，用于修正标注对齐问题\n",
        "import regex as re\n",
        "\n",
        "def is_symbol(tok: str) -> bool:\n",
        "    \"\"\"判断 token 是否是纯符号或标点\"\"\"\n",
        "    stripped = tok.replace('▁', '')\n",
        "    return bool(re.fullmatch(r'[\\p{P}\\p{S}]+', stripped))\n",
        "\n",
        "def merge_tokens_by_underscore(tokens, labels):\n",
        "    \"\"\"\n",
        "    只合并字母或数字之间的拆分，不合并符号。\n",
        "    保留符号及符号前空格原样。\n",
        "    \"\"\"\n",
        "    merged_tokens = []\n",
        "    merged_labels = []\n",
        "    cur_tok = \"\"\n",
        "    cur_lab = None\n",
        "\n",
        "    for tok, lab in zip(tokens, labels):\n",
        "        if tok is None:\n",
        "            tok = \"\"\n",
        "        elif not isinstance(tok, str):\n",
        "            tok = str(tok)\n",
        "\n",
        "        # 符号 或 新词起始（▁开头） → 另起一个 token\n",
        "        if is_symbol(tok) or tok.startswith(\"▁\"):\n",
        "            # 如果之前在拼接单词，先保存\n",
        "            if cur_tok:\n",
        "                merged_tokens.append(cur_tok)\n",
        "                merged_labels.append(cur_lab)\n",
        "            # 新开 token\n",
        "            cur_tok = tok\n",
        "            cur_lab = lab\n",
        "        else:\n",
        "            # 普通常规 subword（单词内部拆分），合并\n",
        "            cur_tok += tok\n",
        "            cur_lab = cur_lab or lab  # 保留第一次的标签\n",
        "\n",
        "    # 保存最后一个 token\n",
        "    if cur_tok:\n",
        "        merged_tokens.append(cur_tok)\n",
        "        merged_labels.append(cur_lab)\n",
        "\n",
        "    return merged_tokens, merged_labels\n",
        "\n",
        "\n",
        "def fill_and_align_to_bio(tpl, lang, addrs_map, times_map, non_addr_map, non_time_map, add_noise=True):\n",
        "    SPACE_LANGS = [\n",
        "\n",
        "    ]\n",
        "    NO_SPACE_LANGS = [\"简体中文\",\"繁体中文\",\"日语\",\"韩语\",\"英语\",\"法语\",\"德语\",\"意大利语\",\"西班牙语\",\"葡萄牙语\",\n",
        "        \"荷兰语\",\"瑞典语\",\"丹麦语\",\"俄语\",\"土耳其语\"]\n",
        "\n",
        "    # 占位符替换\n",
        "    parts = []\n",
        "    entities = []  # (start_char, end_char, label_type)\n",
        "    last_pos = 0\n",
        "    for m in re.finditer(r'\\{address\\}|\\{time\\}', tpl):\n",
        "        parts.append(tpl[last_pos:m.start()])\n",
        "        if m.group() == \"{address}\":\n",
        "            if random.random() < NEG_ADDR_PROB:\n",
        "                rep = random.choice(non_addr_map.get(lang, [\"\"]))\n",
        "            else:\n",
        "                rep = random.choice(addrs_map.get(lang, [\"Main Street\"]))\n",
        "                start_char = sum(len(p) for p in parts)\n",
        "                entities.append((start_char, start_char + len(rep), \"ADDRESS\"))\n",
        "            parts.append(rep)\n",
        "        elif m.group() == \"{time}\":\n",
        "            if random.random() < NEG_TIME_PROB:\n",
        "                rep = random.choice(non_time_map.get(lang, [\"\"]))\n",
        "            else:\n",
        "                rep = random.choice(times_map.get(lang, [\"2024-07-01\"]))\n",
        "                start_char = sum(len(p) for p in parts)\n",
        "                entities.append((start_char, start_char + len(rep), \"TIME\"))\n",
        "            parts.append(rep)\n",
        "        last_pos = m.end()\n",
        "    parts.append(tpl[last_pos:])\n",
        "    text = \"\".join(parts)\n",
        "\n",
        "    # 噪声处理\n",
        "    if add_noise:\n",
        "        text, new_spans = apply_noise(text, [(s, e) for s, e, _ in entities], lang)\n",
        "        for i, (_, _, typ) in enumerate(entities):\n",
        "            entities[i] = (new_spans[i][0], new_spans[i][1], typ)\n",
        "\n",
        "    # 分词 & BIO对齐\n",
        "    enc = tokenizer(text, add_special_tokens=False, return_offsets_mapping=True)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"])\n",
        "    offsets = enc[\"offset_mapping\"]\n",
        "\n",
        "    labels = [\"O\"] * len(tokens)\n",
        "    for start_char, end_char, ent_type in entities:\n",
        "        first_flag = True\n",
        "        for i, (tok_s, tok_e) in enumerate(offsets):\n",
        "            if tok_e <= start_char or tok_s >= end_char:\n",
        "                continue\n",
        "            if first_flag:\n",
        "                labels[i] = f\"B-{ent_type}\"\n",
        "                first_flag = False\n",
        "            else:\n",
        "                labels[i] = f\"I-{ent_type}\"\n",
        "\n",
        "    # === 按▁合并为单词（仅空格语言）\n",
        "    if lang in SPACE_LANGS:\n",
        "        tokens, labels = merge_tokens_by_underscore(tokens, labels)\n",
        "\n",
        "    return tokens, labels\n",
        "# ===== 主构建 =====\n",
        "def build_for_lang(lang: str, templates: Dict[str, list],\n",
        "                   addrs_map, times_map, non_addr_map, non_time_map,\n",
        "                   preview_n=0, n_per_lang=None):\n",
        "    samples = []\n",
        "\n",
        "    # 数据量控制\n",
        "    total_target = n_per_lang if n_per_lang else len(templates.get(\"pos_11\", [])) + len(templates.get(\"pos_other\", [])) + len(templates.get(\"neg_sent\", []))\n",
        "\n",
        "    # 纯负例\n",
        "    neg_target = int(total_target * (per_00))\n",
        "    # 正例总量\n",
        "    pos_target_total = total_target - neg_target\n",
        "    # 正例中 11\n",
        "    pos_11_target = int(pos_target_total * (per_11))\n",
        "    pos_other_target = pos_target_total - pos_11_target\n",
        "\n",
        "    pos_11_tpls = templates.get(\"pos_11\", []) or [\"{address} {time}\"]\n",
        "    pos_other_tpls = templates.get(\"pos_other\", []) or [\"{address} {time}\"]\n",
        "    neg_tpls = templates.get(\"neg_sent\", []) or []\n",
        "\n",
        "    # 正例11\n",
        "    for _ in range(pos_11_target):\n",
        "        tpl = random.choice(pos_11_tpls)\n",
        "        toks, lbls = fill_and_align_to_bio(tpl, lang, addrs_map, times_map,\n",
        "                                           non_addr_map, non_time_map, add_noise=True)\n",
        "        samples.append((toks, lbls))\n",
        "\n",
        "    # 正例其他\n",
        "    for _ in range(pos_other_target):\n",
        "        tpl = random.choice(pos_other_tpls)\n",
        "        toks, lbls = fill_and_align_to_bio(tpl, lang, addrs_map, times_map,\n",
        "                                           non_addr_map, non_time_map, add_noise=True)\n",
        "        samples.append((toks, lbls))\n",
        "\n",
        "    # 负例\n",
        "    for _ in range(neg_target):\n",
        "        if neg_tpls:\n",
        "            tpl = random.choice(neg_tpls)\n",
        "            toks, lbls = spans_to_bio_tokens(tpl, [], [])  # 全 O 标签\n",
        "            samples.append((toks, lbls))\n",
        "\n",
        "    # 预览\n",
        "    if preview_n > 0:\n",
        "        print(f\"[Preview {lang}]\")\n",
        "        for i in range(min(preview_n, len(samples))):\n",
        "            print(\"TOK:\", \" \".join(samples[i][0]))\n",
        "            print(\"LBL:\", \" \".join(samples[i][1]))\n",
        "\n",
        "    return samples\n",
        "# ===== 写 CONLL =====\n",
        "def write_conll(path, data):\n",
        "    \"\"\"将 (tokens, labels) 样本列表写入到 CONLL 格式文件\"\"\"\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for toks, lbls in data:\n",
        "            for t, y in zip(toks, lbls):\n",
        "                f.write(f\"{t} {y}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "# ===== 主入口 =====\n",
        "def main():\n",
        "    n_per_lang = N_PER_LANG if N_PER_LANG else None\n",
        "    addrs_map={lang: read_lines(os.path.join(REAL_ADDR_ROOT,f\"{lang}.txt\")) for lang in LANGS}\n",
        "    times_map={lang: read_lines(os.path.join(REAL_TIME_ROOT,f\"{lang}.txt\")) for lang in LANGS}\n",
        "    non_addr_map={lang: read_lines(os.path.join(NON_ADDR_ROOT,f\"{lang}.txt\")) for lang in LANGS}\n",
        "    non_time_map={lang: read_lines(os.path.join(NON_TIME_ROOT,f\"{lang}.txt\")) for lang in LANGS}\n",
        "    merged=[]\n",
        "    for lang in LANGS:\n",
        "        templates_per_lang = load_templates_for_lang(lang)\n",
        "        data = build_for_lang(lang, templates_per_lang, addrs_map, times_map, non_addr_map, non_time_map, preview_n=PREVIEW_N, n_per_lang=n_per_lang)\n",
        "        write_conll(PER_LANG_CONLL(lang), data)\n",
        "        merged.extend(data)\n",
        "        print(f\"[OK] {lang}: {len(data)} samples -> {PER_LANG_CONLL(lang)}\")\n",
        "    random.shuffle(merged)\n",
        "    write_conll(MERGED_CONLL, merged)\n",
        "    print(f\"[OK] merged: {len(merged)} samples -> {MERGED_CONLL}\")\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYVbrrdsWgno",
        "outputId": "6c737795-e2d6-49bb-f764-d58ed3f8eec6",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "云盘挂载成功\n",
            "[Preview 简体中文]\n",
            "TOK: ▁ 七 月中旬 的一个 周五 下午 13 点 整 我们会 在 A 15 举办 活动 , 期待 你 的到来 !\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O B-ADDRESS I-ADDRESS O O O O O O O\n",
            "TOK: ▁1974 年 10 月 4 日 我和 同事 会在 大 万 世 居 进行 一次 重要的 会议 。\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O\n",
            "TOK: ▁我们 相 约 今年 国庆 节 下午 四 点 四 十五 分 在 12 # 见面 , 一起 探讨 未来的 合作 计划 。\n",
            "LBL: O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O O O\n",
            "TOK: ▁今年 冬 至 凌晨 三 点 我会 去 成都 高新技术 产业 开发区 ▁I 9 厂 房 与 老 朋友 碰 面 , 聊 聊 合作 事宜 。\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O\n",
            "TOK: ▁我 喜欢 十 月 5 日下午 6 点 整 去 陶 利 庙 南 散步 , 感受 自然 的 气息 。\n",
            "LBL: O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O\n",
            "[OK] 简体中文: 500 samples -> /content/drive/MyDrive/NER/train_datasets_AC/train_简体中文.conll\n",
            "[Preview 繁体中文]\n",
            "TOK: ▁如果你 日 後來 到 恩 施 土 家族 苗 族 自治 州 , 一定要 去 看看 當地 的 傳統 手 工藝 品 。\n",
            "LBL: O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O\n",
            "TOK: ▁ 我和 家人 最近 去 羅 平 縣 羅 雄 鎮 野 餐 , 度 過了 愉快 的 一天 。\n",
            "LBL: O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O\n",
            "TOK: ▁在 明天 中午 十二 點 四 十五 分 , 河南省 的 花 卉 展覽 吸引了 大量的 遊客 。\n",
            "LBL: O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O B-ADDRESS O O O O O O O O\n",
            "TOK: ▁ 明天 凌晨 五 點 半 我 計劃 到 29 # 與 朋友 聚 餐 , 期待 美食 和 美好 時光 。\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O O O O O O O O\n",
            "TOK: ▁ 河南省 是個 適合 日 後 放鬆 心情 的好 地方 。\n",
            "LBL: B-ADDRESS I-ADDRESS O O O O O O O O O\n",
            "[OK] 繁体中文: 500 samples -> /content/drive/MyDrive/NER/train_datasets_AC/train_繁体中文.conll\n",
            "[Preview 英语]\n",
            "TOK: ▁You ’ ll ▁find ▁the ▁market ▁bu st ling ▁with ▁activity ▁at ▁36 # ▁November ▁15 , ▁2023 .\n",
            "LBL: O O O O O O O O O O O O O O B-TIME I-TIME I-TIME I-TIME O\n",
            "TOK: ▁You ▁should ▁visit ▁Ha il ▁& ▁Ri de ▁Ster ry ▁Road ▁eventually ▁to ▁capture ▁the ▁best ▁ lighting ▁for ▁photograph s .\n",
            "LBL: O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O\n",
            "TOK: ▁We ▁decided ▁to ▁stop ▁by ▁St ▁Peter ▁and ▁St ▁Paul ▁School ▁eventually ▁for ▁a ▁quick ▁break .\n",
            "LBL: O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O\n",
            "TOK: ▁I ▁remember ▁writing ▁my ▁journal ▁at ▁Oak worth ▁Road ▁23 ▁Oct ▁1970 ▁on ▁a ▁rain y ▁evening .\n",
            "LBL: O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME O O O O O O\n",
            "TOK: ▁I ▁ran ▁into ▁an ▁old ▁friend ▁at ▁Rut land ▁Road ▁We d , ▁27 ▁May ▁1987 ▁08 :04 :32 ▁+ 1000 ▁completely ▁by ▁chance .\n",
            "LBL: O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O\n",
            "[OK] 英语: 500 samples -> /content/drive/MyDrive/NER/train_datasets_AC/train_英语.conll\n",
            "[Preview 丹麦语]\n",
            "TOK: ▁Jeg ▁planlægge r ▁at ▁tage ▁til ▁By g ning ▁17 ▁efter ▁et ▁stykke ▁tid ▁for ▁at ▁se ▁den ▁be rø m te ▁kunst . ud stilling .\n",
            "LBL: O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "TOK: ▁Der ▁er ▁en ▁ny ▁udstilling ▁ved ▁F lix bus / Pen dul fart . dk / tog bus ▁11. ▁Sankt han s aften ▁kl . ▁23:00 , ▁der ▁er ▁værd ▁at ▁se .\n",
            "LBL: O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O\n",
            "TOK: ▁Jeg ▁tager ▁til ▁Lund en ▁14. ▁august ▁kl . ▁20:30 ▁for ▁at ▁deltage ▁i ▁ mødet .\n",
            "LBL: O O O B-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O\n",
            "TOK: ▁Vi ▁skal ▁rejse ▁til ▁29 # ▁45 . ▁S kær tor sdag ▁kl . ▁11:00 ▁og ▁ud forsk e ▁området . ,\n",
            "LBL: O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O\n",
            "TOK: ▁Jeg ▁skal ▁af le ve — re ▁dokumenter ne ▁i ▁By g ning ▁13 ▁6. ▁oktober ▁kl . ▁22 :15 ▁og ▁derefter ▁videre ▁til ▁næste ▁sted .\n",
            "LBL: O O O O O O O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O\n",
            "[OK] 丹麦语: 500 samples -> /content/drive/MyDrive/NER/train_datasets_AC/train_丹麦语.conll\n",
            "[Preview 俄语]\n",
            "TOK: ▁В ▁Район ный ▁центр ▁« Э ль б рус » ▁21 ▁июня ▁в ▁13:00 ▁часто ▁можно ▁встретить ▁артист ов ▁и ▁музыкант ов .\n",
            "LBL: O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME O O O O O O O O O\n",
            "TOK: ▁Мы ▁провели ▁в ▁Сав ёл ов ская ▁Через ▁неделю ▁утром ▁в ▁10:00 ▁несколько ▁часов , ▁наслажда ясь ▁местно й ▁атмосфер ой .\n",
            "LBL: O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O O\n",
            "TOK: ▁Трен ировка ▁начнет ся ▁в ▁З да ние ▁44 ▁Завтра ▁до ▁обед а ▁в ▁11 :15 , ▁не ▁за буд ьте ▁форму .\n",
            "LBL: O O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O\n",
            "TOK: ▁· В ▁Торгов ый ▁центр ▁\" Т ук - Т ук \" ▁6 ▁января ▁2010 ▁г . ▁будет ▁открыт ▁новый ▁музей .\n",
            "LBL: O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME O O O O O\n",
            "TOK: ▁Если ▁оказаться ▁у ▁Про м зон а ▁Кот ля ково ▁Через ▁три ▁дня ▁в ▁14 :15 , ▁можно ▁почув ствовать ▁настоя щую ▁атмосфер у ▁города .\n",
            "LBL: O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O O\n",
            "[OK] 俄语: 500 samples -> /content/drive/MyDrive/NER/train_datasets_AC/train_俄语.conll\n",
            "[Preview 土耳其语]\n",
            "TOK: ▁Yeni ▁film ▁gala sı ▁yakın da ▁Mer al ▁A VM ' te ▁düzenlenecek .\n",
            "LBL: O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O\n",
            "TOK: ▁Temmuz ’ un ▁son ▁per ş embe ▁günü ▁saat ▁20 :15 ’ te ▁iti bar ıyla ▁26 # ' te ▁hava ▁koşulları ▁oldukça ▁iyi ydi .\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O O O O O\n",
            "TOK: ▁36. ▁Kasım ' ın ▁son ▁hafta sının ▁pazar tesi ▁sabah ı ▁saat ▁09:00 ' da ▁günü ▁1 # ' te ▁bir ▁sem iner ▁düzenlenecek .\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O O\n",
            "TOK: ▁Kur ban ▁Bayramı ▁öğle den ▁sonra ▁saatlerinde ▁L OCA ▁İstanbul ' te ▁alışveriş ▁yapmak ▁daha ▁rahat ▁oluyor .\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O\n",
            "TOK: ▁Yazar , ▁roman ında ▁14. ▁Bugün ▁saat ▁15 :45 ▁24 # ' te ▁yaşadığı ▁un utul . maz ▁an ları ▁anlattı .\n",
            "LBL: O O O O B-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O O O O O\n",
            "[OK] 土耳其语: 500 samples -> /content/drive/MyDrive/NER/train_datasets_AC/train_土耳其语.conll\n",
            "[Preview 德语]\n",
            "TOK: ▁Der ▁Film abend ▁27. ▁Feb . ▁20 28 ▁im ▁Hell er sdorf er ▁Straße / G ärt en ▁der ▁Welt ▁war ▁ein ▁voller ▁Erfolg .\n",
            "LBL: O O O B-TIME I-TIME I-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O\n",
            "TOK: ▁Wir ▁haben ▁uns ▁entschieden , ▁bald ▁zum ▁Zucker fabrik ▁Hal ber stadt ▁zu ▁gehen .\n",
            "LBL: O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O\n",
            "TOK: ▁Ein ▁Treffen ▁27. ▁März ▁um ▁11:00 ▁Uhr ▁im ▁44 # ▁könnte ▁für ▁unsere ▁Planung ▁hilfreich ▁sein .\n",
            "LBL: O O B-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O O\n",
            "TOK: ▁Der ▁Spa zier gang ▁Montag , ▁21. ▁Oktober ▁1996 , ▁18 :16 ▁durch ▁den ▁Hollywood ▁am ▁Inn ▁war ▁sehr ▁er hol sam .\n",
            "LBL: O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O B-ADDRESS I-ADDRESS I-ADDRESS O O O O O O\n",
            "TOK: ▁; Die ▁Wander ung ▁Heute ▁Morgen ▁um ▁10:00 ▁Uhr ▁zum ▁Augustin er str . ▁2 , ▁55 116 ▁Main z , ▁Rheinland - Pfalz ▁war ▁eine ▁echte ▁Herausforderung .\n",
            "LBL: O O O O B-TIME I-TIME I-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O\n",
            "[OK] 德语: 500 samples -> /content/drive/MyDrive/NER/train_datasets_AC/train_德语.conll\n",
            "[Preview 意大利语]\n",
            "TOK: ▁Anda re ▁a ▁Parco ▁Com merci ale ▁appena ▁possibile ▁è ▁stata ▁una ▁delle ▁migliori ▁decision i ▁della ▁giornata .\n",
            "LBL: O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O O\n",
            "TOK: ▁Il ▁concerto ▁ inizi erà ▁a ▁Nu ova ▁Aurora ▁8 ▁novembre ▁20 29 , ▁quindi ▁non ▁arrivare ▁in ▁rit ardo .\n",
            "LBL: O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME O O O O O O O O\n",
            "TOK: ▁Ho ▁seguito ▁una ▁le zione ▁di ▁yoga ▁a ▁Dipartimento ▁Am ministra tivo ▁Finanz i ario ▁- ▁Auto par co ▁Il ▁15 ▁marzo ▁del ▁2025 ▁e ▁mi ▁sono ▁sentit o ▁ri genera to .\n",
            "LBL: O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O\n",
            "TOK: ▁Un ' import ante ▁scoperta ▁archeologi ca ▁è ▁stata ▁fatta ▁a ▁Via ▁Coraz za ▁- ▁Via ▁Bologna ▁La ▁sera ▁del ▁giorno ▁dopo ▁domani ▁alle ▁22:00 .\n",
            "LBL: O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TOK: ▁Edi fici o ▁19 ▁è ▁una ▁destina zione ▁popolare ▁per ▁le ▁famiglie ▁Il ▁1 o ▁maggio ▁( F esta ▁dei ▁La vor atori ).\n",
            "LBL: O O O O O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME\n",
            "[OK] 意大利语: 500 samples -> /content/drive/MyDrive/NER/train_datasets_AC/train_意大利语.conll\n",
            "[Preview 日语]\n",
            "TOK: ▁ 市 立 下 橋 中 学校 の 広 場 では 20 27 年 7 月 22 日に 大道 芸 が行われ / る 予定です 。\n",
            "LBL: B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O\n",
            "TOK: ▁31. ▁ 来 週 の 水 曜 日 午後 6 時に 〒 06 4 -08 01 ▁ 北海道 札幌 市 中央 区 北 1 条 西 28 丁目 5-6 で 行われる 講演 会 に参加 しました 。\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O\n",
            "TOK: ▁ 後 で に 甲 陽 学院 高等学校 の 公園 で ラン ニング を しました 。\n",
            "LBL: O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O\n",
            "TOK: ▁ 私たちは 来 年の 海 の 日 朝 9 時に 40 棟 で 待ち 合わせ を しました 。\n",
            "LBL: O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O\n",
            "TOK: ▁ しばらく して に 川 崎 北 労働 基準 監督 署 にある 公園 で ピ ク ニック を しました 。\n",
            "LBL: O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O\n",
            "[OK] 日语: 500 samples -> /content/drive/MyDrive/NER/train_datasets_AC/train_日语.conll\n",
            "[Preview 法语]\n",
            "TOK: ▁Les ▁ru es ▁de ▁Square ▁Louis ▁Le page ▁sont ▁calme s ▁et ▁p · ais ibles ▁après ▁un ▁certain ▁temps .\n",
            "LBL: O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O O O\n",
            "TOK: ▁Je ▁par s ▁pour ▁Sou s - P ré fec ture ▁de ▁Ge x ▁Le ▁15 ▁août ▁à ▁22 h 10 ▁afin ▁de ▁rencontrer ▁mon ▁ami .\n",
            "LBL: O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O\n",
            "TOK: ▁Un ▁concert ▁exceptionnel ▁a ▁eu ▁lieu ▁à ▁23 ▁Ru e ▁Victor ▁Hugo , ▁3 1000 ▁Toulouse , ▁FR ▁Le ▁12 ▁juin ▁à ▁18 h 00 .\n",
            "LBL: O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TOK: ▁Nous ▁avons ▁prévu ▁de ▁dî ner ▁à ▁Centre ▁Comme r cial ▁Ple in ▁Air ▁Le ▁1 er ▁janvier ▁2025 .\n",
            "LBL: O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TOK: ▁Le ▁festival ▁annuel ▁se ▁ti endra ▁à ▁13 # ▁bientôt , ▁comme ▁chaque ▁année .\n",
            "LBL: O O O O O O O O O O O O O O O\n",
            "[OK] 法语: 500 samples -> /content/drive/MyDrive/NER/train_datasets_AC/train_法语.conll\n",
            "[Preview 瑞典语]\n",
            "TOK: ▁De ▁samla des ▁vid ▁Sul zer ▁Två ▁dagar ▁innan ▁jul ▁kl . ▁16:00 ▁för ▁att ▁fira .\n",
            "LBL: O O O O B-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O\n",
            "TOK: ▁Vi ▁hade ▁en ▁pic nic ▁vid ▁Hy lli e vå ng s parken ▁I ▁morgon ▁eftermiddag ▁vid ▁klockan ▁14:30 ▁och ▁n jö t ▁av ▁utsikt en .\n",
            "LBL: O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O\n",
            "TOK: ▁Mö tet ▁ä gde ▁rum ▁vid ▁La holm s ▁Be läggning s teknik ▁någon ▁gång ▁och ▁var ▁mycket ▁produktiv t .\n",
            "LBL: O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O\n",
            "TOK: ▁Vi ▁tog ▁en ▁paus ▁vid ▁Bygg nad ▁14 ▁S kär tors dagen ▁kl . ▁10:00 ▁efter ▁en ▁lång ▁promenad .\n",
            "LBL: O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O\n",
            "TOK: ▁/ Jag ▁tänker ▁besök a ▁11 # ▁7 ▁aug . ▁1974 ▁för ▁att ▁koppla ▁av .\n",
            "LBL: O O O O O O O B-TIME I-TIME I-TIME I-TIME O O O O O\n",
            "[OK] 瑞典语: 500 samples -> /content/drive/MyDrive/NER/train_datasets_AC/train_瑞典语.conll\n",
            "[Preview 荷兰语]\n",
            "TOK: ▁Een ▁wandel ing ▁door ▁Christ elijke ▁Op leiding s school ▁op ▁term ijn ▁gaf ▁me ▁friss e ▁energie .\n",
            "LBL: O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O\n",
            "TOK: ▁We ▁kunnen ▁elkaar ▁Voor laat ste ▁donderdag ▁van ▁augustus ▁ontmoet en ▁bij ▁Schol en gemeenschap .\n",
            "LBL: O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O B-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TOK: ▁De ▁trein ▁naar ▁Service bios coop ▁Industry ▁vertrek t ▁Laat ste ▁vrijdag ▁in ▁oktober ▁vanaf ▁het ▁station .\n",
            "LBL: O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O B-TIME I-TIME I-TIME I-TIME I-TIME O O O O\n",
            "TOK: ▁Wagen borg er bos ▁was ▁De ▁derde ▁donderdag ▁van ▁december ▁een ▁mag ische ▁plek ▁voor ▁fotografie .\n",
            "LBL: B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O B-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O\n",
            "TOK: ▁Ze ▁ver trekken ▁7 ▁maart ▁om ▁drie ▁uur ▁' s ▁middag s ▁vanaf ▁Micro - Mail ▁Zee wol de ▁B . V . ▁voor ▁een ▁lange ▁reis .\n",
            "LBL: O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O\n",
            "[OK] 荷兰语: 500 samples -> /content/drive/MyDrive/NER/train_datasets_AC/train_荷兰语.conll\n",
            "[Preview 葡萄牙语]\n",
            "TOK: ▁Deci di mos ▁ir ▁ao ▁Parque ▁do ▁Qui os que ▁Dia ▁10 ▁de ▁outubro ▁às ▁21 h 50 ▁e ▁experimentar ▁prato s ▁locais ▁delicioso s .\n",
            "LBL: O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O\n",
            "TOK: ▁Deci di ▁ir ▁ao ▁Rua ▁Central ▁5 , ▁Anda r ▁2 , ▁1 o ▁Es q . , ▁3 100 - 123 ▁Po mbal , ▁PT ▁Dia ▁25 ▁do ▁non o ▁mês ▁lunar , ▁para ▁buscar ▁inspira ção ▁para ▁meu ▁próximo ▁projeto .\n",
            "LBL: O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O O\n",
            "TOK: ▁31 ▁de ▁julho ▁de ▁20 28 ▁decidi ▁fazer ▁um ▁passeio ▁no ▁É vo J ard ins ▁para ▁relaxar .\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O\n",
            "TOK: ▁No ▁Estrada ▁da ▁Circ un val ação , ▁4500 - 001 ▁Mato s inhos , ▁Porto , ▁algum ▁dia ▁participe i ▁de ▁uma ▁aula ▁de ▁dan ça ▁ao ▁ar ▁livre .\n",
            "LBL: O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O O O O O\n",
            "TOK: ▁em ▁breve , ▁explore i ▁as ▁galeria s ▁de ▁arte ▁localizada s ▁perto ▁do ▁Se te ▁Rio s .\n",
            "LBL: O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "[OK] 葡萄牙语: 500 samples -> /content/drive/MyDrive/NER/train_datasets_AC/train_葡萄牙语.conll\n",
            "[Preview 西班牙语]\n",
            "TOK: ▁Durante ▁pronto , ▁pas e amos ▁por ▁el ▁centro ▁histórico ▁de ▁Sant ▁Joan ▁y ▁tom amos ▁muchas ▁fotos .\n",
            "LBL: O O O O O O O O O O O B-ADDRESS I-ADDRESS O O O O O O\n",
            "TOK: ▁Tom é ▁fotos ▁increíble s ▁de ▁Re siden cia ▁Nuestra ▁Señor a ▁de ▁los ▁Dolor es ▁25 ▁de ▁diciembre ▁de ▁2025 .\n",
            "LBL: O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TOK: ▁El ▁primer ▁viernes ▁de ▁diciembre ▁a ▁las ▁19:00 ▁explo ré ▁los ▁alrededor es ▁de ▁Beg onia ▁para ▁buscar ▁inspira ción ▁para ▁mis ▁pintura s .\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O B-ADDRESS I-ADDRESS O O O O O O O O O\n",
            "TOK: ▁El ▁tercer ▁viernes ▁de ▁marzo ▁por ▁la ▁tarde ▁fui ▁de ▁compras ▁a ▁Oficina ▁no ▁5 ▁Mu face ▁buscando ▁artículos ▁especiales .\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O\n",
            "TOK: ▁5 ▁de ▁marzo ▁de ▁2013 ▁explo ré ▁los ▁museo s ▁de ▁Edi fici o ▁19, ▁que ▁tienen ▁una ▁colección ▁impresionante .\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O O O O O O O O\n",
            "[OK] 西班牙语: 500 samples -> /content/drive/MyDrive/NER/train_datasets_AC/train_西班牙语.conll\n",
            "[Preview 韩语]\n",
            "TOK: ▁49 # 에서 ▁출발 하는 ▁비 행 기는 ▁8 월 ▁15 일 ▁광 복 절 에 ▁공항 에 ▁도착 합니다 .\n",
            "LBL: O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O\n",
            "TOK: ▁Lotte ▁Cinema 에서는 ▁ 훗 날 마다 ▁특별한 ▁이벤트 가 ▁열 립니다 .\n",
            "LBL: B-ADDRESS I-ADDRESS O O O O O O O O O O O\n",
            "TOK: ▁ 、 곧 에는 ▁법 1 동 ▁행정 복지 센터 로 ▁가는 ▁사람들 로 ▁ 붐 빕 니다 .\n",
            "LBL: O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O\n",
            "TOK: ▁성 광 온 누리 학교 에서 ▁열린 ▁ 축제 는 ▁서 서 히 에 ▁끝 났 어요 .\n",
            "LBL: B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O O O\n",
            "TOK: ▁33. ▁내 일 ▁점심 ▁12 시 ▁정 각 에는 ▁정 지 용 ▁ 문학 관 에서 ▁열린 ▁파 티 에 ▁초대 받 았습니다 .\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O\n",
            "[OK] 韩语: 500 samples -> /content/drive/MyDrive/NER/train_datasets_AC/train_韩语.conll\n",
            "[OK] merged: 7500 samples -> /content/drive/MyDrive/NER/train_datasets_AC/train_all.conll\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 合成测试数据（json）\n",
        "输入输出说明：\n",
        "\n",
        "\n",
        "*   输入\n",
        "\n",
        "*   输出\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fyjB9Ba1E3xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "from typing import List, Dict, Iterable\n",
        "from itertools import cycle\n",
        "\n",
        "# ========= 配置 =========\n",
        "ADDRESSES_DIR = \"/content/drive/MyDrive/NER/real_address\"\n",
        "TIMES_DIR     = \"/content/drive/MyDrive/NER/real_time\"   # 新增：真实时间文件目录\n",
        "TEMPLATES_DIR = \"/content/drive/MyDrive/NER/combine_templates/combine_templates_22\"\n",
        "\n",
        "N_address = int(TEMPLATES_DIR[-1])  # 每条句子包含多少个地址\n",
        "N_time    = 2                       # 每条句子包含多少个时间（可改）\n",
        "\n",
        "OUTPUT_DIR    = os.path.join(\"./templates_out\", str(N_address))\n",
        "OUTPUT_NAME   = \"{lang}.json\"\n",
        "\n",
        "LANGUAGES = [\n",
        "    \"简体中文\", \"英语\", \"丹麦语\", \"俄语\", \"土耳其语\", \"德语\",\n",
        "    \"意大利语\", \"日语\", \"法语\", \"瑞典语\", \"荷兰语\", \"葡萄牙语\",\n",
        "    \"西班牙语\", \"韩语\",\n",
        "]\n",
        "\n",
        "# ========= 工具函数 =========\n",
        "def ensure_dir(p: str):\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "def read_list_from_txt(path: str) -> List[str]:\n",
        "    items = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            s = line.strip()\n",
        "            if s:\n",
        "                items.append(s)\n",
        "    random.shuffle(items)\n",
        "    return items\n",
        "\n",
        "def find_template_file(templates_dir: str, lang: str) -> str:\n",
        "    prefix = f\"{lang}_combine_templates\"\n",
        "    for fn in os.listdir(templates_dir):\n",
        "        if fn.startswith(prefix):\n",
        "            return os.path.join(templates_dir, fn)\n",
        "    raise FileNotFoundError(f\"未找到模板文件：{prefix}* 于 {templates_dir}\")\n",
        "\n",
        "def iter_templates_from_jsonl(path: str):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f, 1):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                obj = json.loads(line)\n",
        "                t = obj.get(\"template\", \"\")\n",
        "                if isinstance(t, str) and t:\n",
        "                    yield t\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] JSONL 解析失败 {path}:{i} | {e}\")\n",
        "\n",
        "def read_templates(path: str) -> List[str]:\n",
        "    templates: List[str] = []\n",
        "    try:\n",
        "        for t in iter_templates_from_jsonl(path):\n",
        "            templates.append(t)\n",
        "        if templates:\n",
        "            return list(dict.fromkeys(templates))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        content = f.read().strip()\n",
        "    data = json.loads(content)\n",
        "    out = []\n",
        "    if isinstance(data, list):\n",
        "        for item in data:\n",
        "            if isinstance(item, dict):\n",
        "                t = item.get(\"template\", \"\")\n",
        "                if isinstance(t, str) and t:\n",
        "                    out.append(t)\n",
        "    elif isinstance(data, dict):\n",
        "        t = data.get(\"template\", \"\")\n",
        "        if isinstance(t, str) and t:\n",
        "            out.append(t)\n",
        "    else:\n",
        "        raise ValueError(f\"不支持的模板 JSON 结构：{path}\")\n",
        "    return list(dict.fromkeys(out))\n",
        "\n",
        "def embed_addresses_and_times(template: str, address_list: List[str], time_list: List[str]) -> str:\n",
        "    result = template\n",
        "    for t in time_list:\n",
        "        if \"{time}\" in result:\n",
        "            result = result.replace(\"{time}\", t, 1)\n",
        "        else:\n",
        "            break\n",
        "    for addr in address_list:\n",
        "        if \"{address}\" in result:\n",
        "            result = result.replace(\"{address}\", addr, 1)\n",
        "        else:\n",
        "            break\n",
        "    return result\n",
        "\n",
        "def write_json_array(path: str, records: List[Dict]):\n",
        "    ensure_dir(os.path.dirname(path))\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(records, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"[OK] 写出：{path} ({len(records)} 条)\")\n",
        "\n",
        "# ========= 主流程 =========\n",
        "def main():\n",
        "    ensure_dir(OUTPUT_DIR)\n",
        "\n",
        "    for lang in LANGUAGES:\n",
        "        addr_file = os.path.join(ADDRESSES_DIR, f\"{lang}.txt\")\n",
        "        time_file = os.path.join(TIMES_DIR,     f\"{lang}.txt\")\n",
        "\n",
        "        if not os.path.isfile(addr_file):\n",
        "            print(f\"[SKIP] 未找到地址文件：{addr_file}\")\n",
        "            continue\n",
        "        if not os.path.isfile(time_file):\n",
        "            print(f\"[SKIP] 未找到时间文件：{time_file}\")\n",
        "            continue\n",
        "\n",
        "        addresses = read_list_from_txt(addr_file)\n",
        "        times     = read_list_from_txt(time_file)\n",
        "        if not addresses:\n",
        "            print(f\"[SKIP] 地址为空：{addr_file}\")\n",
        "            continue\n",
        "        if not times:\n",
        "            print(f\"[SKIP] 时间为空：{time_file}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            tmpl_path = find_template_file(TEMPLATES_DIR, lang)\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"[SKIP] {e}\")\n",
        "            continue\n",
        "        templates = read_templates(tmpl_path)\n",
        "        if not templates:\n",
        "            print(f\"[SKIP] 模板为空：{tmpl_path}\")\n",
        "            continue\n",
        "\n",
        "        tmpl_iter = cycle(templates)\n",
        "        records: List[Dict] = []\n",
        "\n",
        "        # 按地址数推进，时间也循环\n",
        "        max_len = min(len(addresses) // N_address, len(times) // N_time) * N_address\n",
        "        for idx in range(0, max_len, N_address):\n",
        "            addr_list = addresses[idx: idx + N_address]\n",
        "            time_idx  = (idx // N_address) * N_time\n",
        "            time_list = times[time_idx: time_idx + N_time]\n",
        "\n",
        "            templ = next(tmpl_iter)\n",
        "            sentence = embed_addresses_and_times(templ, addr_list, time_list)\n",
        "\n",
        "            records.append({\n",
        "                \"id\": idx // N_address + 1,\n",
        "                \"sentence\": sentence,\n",
        "                \"time_tokens\": time_list,\n",
        "                \"location_tokens\": addr_list\n",
        "            })\n",
        "\n",
        "        out_file = os.path.join(OUTPUT_DIR, OUTPUT_NAME.format(lang=lang))\n",
        "        write_json_array(out_file, records)\n",
        "\n",
        "    print(\"全部完成。输出目录：\", OUTPUT_DIR)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "o8b3iPpZE3EW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfb8eb88-b7c2-49ac-d3a8-499cf7085f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] 写出：./templates_out/2/简体中文.json (1900 条)\n",
            "[OK] 写出：./templates_out/2/英语.json (3200 条)\n",
            "[OK] 写出：./templates_out/2/丹麦语.json (2000 条)\n",
            "[OK] 写出：./templates_out/2/俄语.json (2300 条)\n",
            "[OK] 写出：./templates_out/2/土耳其语.json (1999 条)\n",
            "[OK] 写出：./templates_out/2/德语.json (2200 条)\n",
            "[OK] 写出：./templates_out/2/意大利语.json (2000 条)\n",
            "[OK] 写出：./templates_out/2/日语.json (1900 条)\n",
            "[OK] 写出：./templates_out/2/法语.json (2100 条)\n",
            "[OK] 写出：./templates_out/2/瑞典语.json (2000 条)\n",
            "[OK] 写出：./templates_out/2/荷兰语.json (1547 条)\n",
            "[OK] 写出：./templates_out/2/葡萄牙语.json (2100 条)\n",
            "[OK] 写出：./templates_out/2/西班牙语.json (2100 条)\n",
            "[OK] 写出：./templates_out/2/韩语.json (1900 条)\n",
            "全部完成。输出目录： ./templates_out/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **训练模型**"
      ],
      "metadata": {
        "id": "g79rHllDgQ8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**参数设置说明**（可直接在代码右侧的空格处设置，或直接使用默认值）：\n",
        "*   DATA_PATH: 数据集路径（conll格式，可运行本笔记本中的数据构造部分得到）\n",
        "*   OUTPUT_DIR: 模型输出路径，如果没有保存到云盘则需要在长时间断开运行时前将模型下载或移动到云盘（移动代码见本笔记本的“文件操作”部分）\n",
        "*   EPOCHS: 训练轮数\n",
        "*   BATCH: 每次加载的批次大小\n",
        "*   LR: 学习率\n",
        "*   WARMUP: 学习率预热率\n",
        "\n",
        "**参数设置策略：**\n",
        "\n",
        "\n",
        "*   **EPOCHS:**\n",
        "\n",
        "    * 理想情况：训练集与验证集的 loss 先下降，末期趋于平稳。\n",
        "\n",
        "    * 若两者到最后仍持续下降 → 可能未收敛，可适当增大 EPOCHS。\n",
        "\n",
        "    * 若训练集 loss 继续降、验证集先降后升 → 过拟合；通常应减小 EPOCHS或启用早停，不过由于代码里设置了metric_for_best_model='f1',会自动保存f1分数最高的模型，所以不太会影响最后的模型质量。\n",
        "\n",
        "*   **BATCH**:\n",
        "    *  最好是2的n次方，值越大训练得越快，但是不能超出gpu的显存限制，可以在训练时点击右上角的“RAM 磁盘”查看gpu ram的占用情况，如果离最大容量距离较大可以尝试增大BATCH SIZE来加速。\n",
        "\n",
        "    *  建议遵循线性缩放法则：当 batch size 增大时，等比例增大学习率 LR（例如 batch×2 → LR×≈2）\n",
        "*   **LR:**\n",
        "    * 值越大模型收敛得越快。\n",
        "    * 过大：训练/验证loss波动；\n",
        "    * 过小：loss 下降缓慢；\n",
        "*   **WARMUP:**\n",
        "    * 含义： 前 warmup 比例的 step（一次参数更新为 1 step）内，将 LR 从 0 线性升到目标 LR，以避免冷启动震荡。\n",
        "    * 训练初期 loss 大幅波动 → warmup 太短；\n",
        "    * 初期 loss 下降过慢 → warmup 太长。\n"
      ],
      "metadata": {
        "id": "Y2QshbHFme14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 训练代码 {\"form-width\":\"35%\"}\n",
        "\n",
        "# Train XLM-R NER ConLL\n",
        "!pip -q install -U transformers datasets evaluate seqeval accelerate\n",
        "\n",
        "import os, numpy as np, torch, random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "from packaging import version\n",
        "import transformers\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoConfig, AutoModelForTokenClassification,\n",
        "    DataCollatorForTokenClassification, Trainer, TrainingArguments, set_seed\n",
        ")\n",
        "\n",
        "#  Config\n",
        "# 训练集路径\n",
        "DATA_PATH = \"/content/drive/MyDrive/NER/train_datasets/train_all.conll\" # @param {\"type\":\"string\",\"placeholder\":\"请输入数据集路径\"}\n",
        "OUTPUT_DIR = \"/content/ner_xlmr_out\" # @param {\"type\":\"string\",\"placeholder\":\"请输入模型保存路径\"}\n",
        "\n",
        "\n",
        "# 训练轮数（过大会导致过拟合）\n",
        "EPOCHS = 3 # @param {\"type\":\"integer\"}\n",
        "\n",
        "# batch size 256对应的是A100GPU，若选用显存较小的gpu请适当调小，防止cuda out of memory\n",
        "# 如果调小batch size请同时把LR（学习率）也适当调小。\n",
        "BATCH = 64 # @param {\"type\":\"integer\", \"placeholder\":\"请训练轮数，默认为3\"}\n",
        "LR = 1.25e-5   # @param {\"type\":\"number\", \"placeholder\":\"请输入学习率，默认为5e-5\"}\n",
        "\n",
        "# 学习率预热，在初期使用较小学习率，防止震荡或梯度爆炸\n",
        "WARMUP = 0.1 # @param {\"type\":\"number\", \"placeholder\":\"请输入学习率预热率，默认为0.1\"}\n",
        "\n",
        "\n",
        "PRETOKENIZED = True     # 已是XLM-R子词选True；还是是原词级未分词选False\n",
        "MODEL_NAME = \"xlm-roberta-base\"\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "#  Labels\n",
        "LABELS = [\"O\",\"B-ADDRESS\", \"I-ADDRESS\"]\n",
        "label2id = {l:i for i,l in enumerate(LABELS)}\n",
        "id2label = {i:l for l,i in label2id.items()}\n",
        "\n",
        "#  Read CoNLL\n",
        "def read_conll(path: str):\n",
        "    sents_tok, sents_lab = [], []\n",
        "    toks, labs = [], []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line=line.strip()\n",
        "            if not line:\n",
        "                if toks:\n",
        "                    assert len(toks)==len(labs), (toks, labs)\n",
        "                    sents_tok.append(toks); sents_lab.append(labs)\n",
        "                    toks, labs = [], []\n",
        "                continue\n",
        "            if \" \" in line:\n",
        "                t, y = line.split(\" \", 1)\n",
        "            else:\n",
        "                t, y = line, \"O\"\n",
        "            if y not in label2id: y = \"O\"\n",
        "            toks.append(t); labs.append(y)\n",
        "    if toks:\n",
        "        sents_tok.append(toks); sents_lab.append(labs)\n",
        "    return sents_tok, sents_lab\n",
        "\n",
        "tokens_all, tags_all = read_conll(DATA_PATH)\n",
        "print(f\"Loaded {len(tokens_all)} sentences. Example tokens[:20]:\", tokens_all[0][:20])\n",
        "print(\"Example labels[:20]:\", tags_all[0][:20])\n",
        "\n",
        "#  Train/Val split\n",
        "from sklearn.model_selection import train_test_split\n",
        "idx = np.arange(len(tokens_all))\n",
        "tr_idx, va_idx = train_test_split(idx, test_size=0.1, random_state=SEED, shuffle=True)\n",
        "def pick(idxs):\n",
        "    return [tokens_all[i] for i in idxs], [tags_all[i] for i in idxs]\n",
        "train_tokens, train_tags = pick(tr_idx)\n",
        "val_tokens, val_tags = pick(va_idx)\n",
        "\n",
        "#  Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "#  Path A: PRETOKENIZED = True (直接映射到 vocab id) -\n",
        "def build_dataset_pretokenized(tokens_list, tags_list) -> Dataset:\n",
        "    CLS_ID = tokenizer.convert_tokens_to_ids(tokenizer.cls_token)  # <s>\n",
        "    SEP_ID = tokenizer.convert_tokens_to_ids(tokenizer.sep_token)  # </s>\n",
        "    UNK_ID = tokenizer.unk_token_id\n",
        "    PAD_ID = tokenizer.pad_token_id\n",
        "\n",
        "    recs = []\n",
        "    for toks, labs in zip(tokens_list, tags_list):\n",
        "        ids = tokenizer.convert_tokens_to_ids(toks)\n",
        "        ids = [i if i is not None else UNK_ID for i in ids]\n",
        "        input_ids = [CLS_ID] + ids + [SEP_ID]\n",
        "        labels_ids = [-100] + [label2id[y] for y in labs] + [-100]\n",
        "        attn = [1]*len(input_ids)\n",
        "        recs.append({\"input_ids\": input_ids, \"labels\": labels_ids, \"attention_mask\": attn})\n",
        "    return Dataset.from_list(recs)\n",
        "\n",
        "#  Path B: PRETOKENIZED = False（原词级 -> 重新分词并对齐） -\n",
        "def build_dataset_wordlevel(tokens_list, tags_list) -> Dataset:\n",
        "    def _to_records(toks_list, labs_list):\n",
        "        for t, y in zip(toks_list, labs_list):\n",
        "            yield {\"tokens\": t, \"ner_tags\": y}\n",
        "    ds = Dataset.from_list(list(_to_records(tokens_list, tags_list)))\n",
        "    def tokenize_and_align(examples):\n",
        "        tokenized = tokenizer(\n",
        "            examples[\"tokens\"], is_split_into_words=True,\n",
        "            truncation=True, padding=False, max_length=256\n",
        "        )\n",
        "        aligned = []\n",
        "        for i, labs in enumerate(examples[\"ner_tags\"]):\n",
        "            word_ids = tokenized.word_ids(i)\n",
        "            prev = None; out = []\n",
        "            for wid in word_ids:\n",
        "                if wid is None:\n",
        "                    out.append(-100)\n",
        "                elif wid != prev:\n",
        "                    out.append(label2id[labs[wid]])\n",
        "                else:\n",
        "                    lab = labs[wid]\n",
        "                    if lab.startswith(\"B-\"): lab = \"I-\"+lab[2:]\n",
        "                    out.append(label2id[lab])\n",
        "                prev = wid\n",
        "            aligned.append(out)\n",
        "        tokenized[\"labels\"] = aligned\n",
        "        return tokenized\n",
        "    return ds.map(tokenize_and_align, batched=True)\n",
        "\n",
        "if PRETOKENIZED:\n",
        "    ds_train = build_dataset_pretokenized(train_tokens, train_tags)\n",
        "    ds_val   = build_dataset_pretokenized(val_tokens,   val_tags)\n",
        "\n",
        "    # 手写collator做padding\n",
        "    from dataclasses import dataclass\n",
        "    @dataclass\n",
        "    class CollatorPT:\n",
        "        pad_id: int = tokenizer.pad_token_id\n",
        "        def __call__(self, batch):\n",
        "            maxlen = max(len(x[\"input_ids\"]) for x in batch)\n",
        "            input_ids, attn, labels = [], [], []\n",
        "            for x in batch:\n",
        "                pad = maxlen - len(x[\"input_ids\"])\n",
        "                input_ids.append(x[\"input_ids\"] + [self.pad_id]*pad)\n",
        "                attn.append(x[\"attention_mask\"] + [0]*pad)\n",
        "                labels.append(x[\"labels\"] + [-100]*pad)\n",
        "            return {\n",
        "                \"input_ids\": torch.tensor(input_ids),\n",
        "                \"attention_mask\": torch.tensor(attn),\n",
        "                \"labels\": torch.tensor(labels)\n",
        "            }\n",
        "    data_collator = CollatorPT()\n",
        "else:\n",
        "    ds_train = build_dataset_wordlevel(train_tokens, train_tags)\n",
        "    ds_val   = build_dataset_wordlevel(val_tokens,   val_tags)\n",
        "    from transformers import DataCollatorForTokenClassification\n",
        "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "#  Model\n",
        "config = AutoConfig.from_pretrained(\n",
        "    MODEL_NAME, num_labels=len(LABELS), id2label=id2label, label2id=label2id\n",
        ")\n",
        "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config)\n",
        "\n",
        "\n",
        "#  Metrics\n",
        "seqeval = evaluate.load(\"seqeval\")\n",
        "def compute_metrics(p):\n",
        "    logits, labels = p\n",
        "    preds = logits.argmax(-1)\n",
        "    true_preds, true_labels = [], []\n",
        "    for pred, lab in zip(preds, labels):\n",
        "        cur_p, cur_l = [], []\n",
        "        for pi, li in zip(pred, lab):\n",
        "            if li == -100: continue\n",
        "            cur_p.append(id2label[int(pi)]); cur_l.append(id2label[int(li)])\n",
        "        true_preds.append(cur_p); true_labels.append(cur_l)\n",
        "    res = seqeval.compute(predictions=true_preds, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": res[\"overall_precision\"],\n",
        "        \"recall\":    res[\"overall_recall\"],\n",
        "        \"f1\":        res[\"overall_f1\"],\n",
        "        \"accuracy\":  res[\"overall_accuracy\"],\n",
        "    }\n",
        "\n",
        "#  Training\n",
        "common_kwargs = dict(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH,\n",
        "    per_device_eval_batch_size=BATCH,\n",
        "    learning_rate=LR,\n",
        "    warmup_ratio=WARMUP,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    report_to=\"none\",\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "ver = version.parse(transformers.__version__)\n",
        "try:\n",
        "    if ver >= version.parse(\"4.55.0\"):\n",
        "        # 新版（>=4.55）优先使用 eval_strategy\n",
        "        args = TrainingArguments(**common_kwargs, eval_strategy=\"epoch\")\n",
        "    else:\n",
        "        # 旧版使用 evaluation_strategy\n",
        "        args = TrainingArguments(**common_kwargs, evaluation_strategy=\"epoch\")\n",
        "except TypeError:\n",
        "    # 极旧版本：先不传参创建，再在 Trainer 里设置\n",
        "    args = TrainingArguments(**common_kwargs)\n",
        "\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=ds_train,\n",
        "    eval_dataset=ds_val,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# 统一设置评估策略\n",
        "try:\n",
        "    # 4.55+ 新API\n",
        "    trainer.set_evaluate(strategy=\"epoch\")\n",
        "except Exception:\n",
        "    try:\n",
        "        # 通用老API\n",
        "        from transformers.trainer_utils import IntervalStrategy\n",
        "        trainer.args.evaluation_strategy = IntervalStrategy.EPOCH\n",
        "    except Exception:\n",
        "        # 字符串兜底\n",
        "        trainer.args.evaluation_strategy = \"epoch\"\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "print(\"== Dev metrics ==\", trainer.evaluate())\n",
        "\n",
        "trainer.save_model(OUTPUT_DIR); tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Saved to:\", OUTPUT_DIR)\n"
      ],
      "metadata": {
        "id": "VmqL0OClp36R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "569954bc-034d-4e32-8b44-6bfecb66a8ed",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3972532.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msents_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents_lab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mtokens_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_conll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded {len(tokens_all)} sentences. Example tokens[:20]:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Example labels[:20]:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3972532.py\u001b[0m in \u001b[0;36mread_conll\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mtoks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 训练代码（兼容时间地点） {\"form-width\":\"35%\"}\n",
        "!pip install \"pyarrow>=14.0.0,<20.0.0\" --force-reinstall\n",
        "# Train XLM-R NER ConLL\n",
        "!pip -q install -U transformers datasets evaluate seqeval accelerate\n",
        "\n",
        "import os, numpy as np, torch, random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "from packaging import version\n",
        "import transformers\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoConfig, AutoModelForTokenClassification,\n",
        "    DataCollatorForTokenClassification, Trainer, TrainingArguments, set_seed\n",
        ")\n",
        "\n",
        "#  Config\n",
        "# 训练集路径\n",
        "DATA_PATH = \"/content/drive/MyDrive/NER/train_datasets_AC/train_all.conll\" # @param {\"type\":\"string\",\"placeholder\":\"请输入数据集路径\"}\n",
        "OUTPUT_DIR = \"/content/ner_xlmr_out\" # @param {\"type\":\"string\",\"placeholder\":\"请输入模型保存路径\"}\n",
        "\n",
        "\n",
        "# 训练轮数（过大会导致过拟合）\n",
        "EPOCHS = 3 # @param {\"type\":\"integer\"}\n",
        "\n",
        "# batch size 256对应的是A100GPU，若选用显存较小的gpu请适当调小，防止cuda out of memory\n",
        "# 如果调小batch size请同时把LR（学习率）也适当调小。\n",
        "BATCH = 128 # @param {\"type\":\"integer\", \"placeholder\":\"请训练轮数，默认为3\"}\n",
        "LR = 2.5e-5   # @param {\"type\":\"number\", \"placeholder\":\"请输入学习率，默认为5e-5\"}\n",
        "\n",
        "# 学习率预热，在初期使用较小学习率，防止震荡或梯度爆炸\n",
        "WARMUP = 0.1 # @param {\"type\":\"number\", \"placeholder\":\"请输入学习率预热率，默认为0.1\"}\n",
        "\n",
        "\n",
        "PRETOKENIZED = True     # 已是XLM-R子词选True；还是是原词级未分词选False\n",
        "MODEL_NAME = \"xlm-roberta-base\"\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "#  Labels\n",
        "LABELS = [\"O\", \"B-ADDRESS\", \"I-ADDRESS\", \"B-TIME\", \"I-TIME\"]\n",
        "label2id = {l: i for i, l in enumerate(LABELS)}\n",
        "id2label = {i: l for l, i in label2id.items()}\n",
        "\n",
        "#  Read CoNLL\n",
        "def read_conll(path: str):\n",
        "    sents_tok, sents_lab = [], []\n",
        "    toks, labs = [], []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line=line.strip()\n",
        "            if not line:\n",
        "                if toks:\n",
        "                    assert len(toks)==len(labs), (toks, labs)\n",
        "                    sents_tok.append(toks); sents_lab.append(labs)\n",
        "                    toks, labs = [], []\n",
        "                continue\n",
        "            if \" \" in line:\n",
        "                t, y = line.split(\" \", 1)\n",
        "            else:\n",
        "                t, y = line, \"O\"\n",
        "            if y not in label2id: y = \"O\"\n",
        "            toks.append(t); labs.append(y)\n",
        "    if toks:\n",
        "        sents_tok.append(toks); sents_lab.append(labs)\n",
        "    return sents_tok, sents_lab\n",
        "\n",
        "tokens_all, tags_all = read_conll(DATA_PATH)\n",
        "print(f\"Loaded {len(tokens_all)} sentences. Example tokens[:20]:\", tokens_all[0][:20])\n",
        "print(\"Example labels[:20]:\", tags_all[0][:20])\n",
        "\n",
        "#  Train/Val split\n",
        "from sklearn.model_selection import train_test_split\n",
        "idx = np.arange(len(tokens_all))\n",
        "tr_idx, va_idx = train_test_split(idx, test_size=0.1, random_state=SEED, shuffle=True)\n",
        "def pick(idxs):\n",
        "    return [tokens_all[i] for i in idxs], [tags_all[i] for i in idxs]\n",
        "train_tokens, train_tags = pick(tr_idx)\n",
        "val_tokens, val_tags = pick(va_idx)\n",
        "\n",
        "#  Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "#  Path A: PRETOKENIZED = True (直接映射到 vocab id) -\n",
        "def build_dataset_pretokenized(tokens_list, tags_list) -> Dataset:\n",
        "    CLS_ID = tokenizer.convert_tokens_to_ids(tokenizer.cls_token)  # <s>\n",
        "    SEP_ID = tokenizer.convert_tokens_to_ids(tokenizer.sep_token)  # </s>\n",
        "    UNK_ID = tokenizer.unk_token_id\n",
        "    PAD_ID = tokenizer.pad_token_id\n",
        "\n",
        "    recs = []\n",
        "    for toks, labs in zip(tokens_list, tags_list):\n",
        "        ids = tokenizer.convert_tokens_to_ids(toks)\n",
        "        ids = [i if i is not None else UNK_ID for i in ids]\n",
        "        input_ids = [CLS_ID] + ids + [SEP_ID]\n",
        "        labels_ids = [-100] + [label2id[y] for y in labs] + [-100]\n",
        "        attn = [1]*len(input_ids)\n",
        "        recs.append({\"input_ids\": input_ids, \"labels\": labels_ids, \"attention_mask\": attn})\n",
        "    return Dataset.from_list(recs)\n",
        "\n",
        "#  Path B: PRETOKENIZED = False（原词级 -> 重新分词并对齐） -\n",
        "def build_dataset_wordlevel(tokens_list, tags_list) -> Dataset:\n",
        "    def _to_records(toks_list, labs_list):\n",
        "        for t, y in zip(toks_list, labs_list):\n",
        "            yield {\"tokens\": t, \"ner_tags\": y}\n",
        "    ds = Dataset.from_list(list(_to_records(tokens_list, tags_list)))\n",
        "    def tokenize_and_align(examples):\n",
        "        tokenized = tokenizer(\n",
        "            examples[\"tokens\"], is_split_into_words=True,\n",
        "            truncation=True, padding=False, max_length=256\n",
        "        )\n",
        "        aligned = []\n",
        "        for i, labs in enumerate(examples[\"ner_tags\"]):\n",
        "            word_ids = tokenized.word_ids(i)\n",
        "            prev = None; out = []\n",
        "            for wid in word_ids:\n",
        "                if wid is None:\n",
        "                    out.append(-100)\n",
        "                elif wid != prev:\n",
        "                    out.append(label2id[labs[wid]])\n",
        "                else:\n",
        "                    lab = labs[wid]\n",
        "                    if lab.startswith(\"B-\"): lab = \"I-\"+lab[2:]\n",
        "                    out.append(label2id[lab])\n",
        "                prev = wid\n",
        "            aligned.append(out)\n",
        "        tokenized[\"labels\"] = aligned\n",
        "        return tokenized\n",
        "    return ds.map(tokenize_and_align, batched=True)\n",
        "\n",
        "if PRETOKENIZED:\n",
        "    ds_train = build_dataset_pretokenized(train_tokens, train_tags)\n",
        "    ds_val   = build_dataset_pretokenized(val_tokens,   val_tags)\n",
        "\n",
        "    # 手写collator做padding\n",
        "    from dataclasses import dataclass\n",
        "    @dataclass\n",
        "    class CollatorPT:\n",
        "        pad_id: int = tokenizer.pad_token_id\n",
        "        def __call__(self, batch):\n",
        "            maxlen = max(len(x[\"input_ids\"]) for x in batch)\n",
        "            input_ids, attn, labels = [], [], []\n",
        "            for x in batch:\n",
        "                pad = maxlen - len(x[\"input_ids\"])\n",
        "                input_ids.append(x[\"input_ids\"] + [self.pad_id]*pad)\n",
        "                attn.append(x[\"attention_mask\"] + [0]*pad)\n",
        "                labels.append(x[\"labels\"] + [-100]*pad)\n",
        "            return {\n",
        "                \"input_ids\": torch.tensor(input_ids),\n",
        "                \"attention_mask\": torch.tensor(attn),\n",
        "                \"labels\": torch.tensor(labels)\n",
        "            }\n",
        "    data_collator = CollatorPT()\n",
        "else:\n",
        "    ds_train = build_dataset_wordlevel(train_tokens, train_tags)\n",
        "    ds_val   = build_dataset_wordlevel(val_tokens,   val_tags)\n",
        "    from transformers import DataCollatorForTokenClassification\n",
        "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "#  Model\n",
        "config = AutoConfig.from_pretrained(\n",
        "    MODEL_NAME, num_labels=len(LABELS), id2label=id2label, label2id=label2id\n",
        ")\n",
        "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config)\n",
        "\n",
        "\n",
        "#  Metrics\n",
        "seqeval = evaluate.load(\"seqeval\")\n",
        "def compute_metrics(p):\n",
        "    logits, labels = p\n",
        "    preds = logits.argmax(-1)\n",
        "    true_preds, true_labels = [], []\n",
        "    for pred, lab in zip(preds, labels):\n",
        "        cur_p, cur_l = [], []\n",
        "        for pi, li in zip(pred, lab):\n",
        "            if li == -100: continue\n",
        "            cur_p.append(id2label[int(pi)]); cur_l.append(id2label[int(li)])\n",
        "        true_preds.append(cur_p); true_labels.append(cur_l)\n",
        "    res = seqeval.compute(predictions=true_preds, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": res[\"overall_precision\"],\n",
        "        \"recall\":    res[\"overall_recall\"],\n",
        "        \"f1\":        res[\"overall_f1\"],\n",
        "        \"accuracy\":  res[\"overall_accuracy\"],\n",
        "    }\n",
        "\n",
        "#  Training\n",
        "common_kwargs = dict(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH,\n",
        "    per_device_eval_batch_size=BATCH,\n",
        "    learning_rate=LR,\n",
        "    warmup_ratio=WARMUP,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    report_to=\"none\",\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "ver = version.parse(transformers.__version__)\n",
        "try:\n",
        "    if ver >= version.parse(\"4.55.0\"):\n",
        "        # 新版（>=4.55）优先使用 eval_strategy\n",
        "        args = TrainingArguments(**common_kwargs, eval_strategy=\"epoch\")\n",
        "    else:\n",
        "        # 旧版使用 evaluation_strategy\n",
        "        args = TrainingArguments(**common_kwargs, evaluation_strategy=\"epoch\")\n",
        "except TypeError:\n",
        "    # 极旧版本：先不传参创建，再在 Trainer 里设置\n",
        "    args = TrainingArguments(**common_kwargs)\n",
        "\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=ds_train,\n",
        "    eval_dataset=ds_val,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# 统一设置评估策略\n",
        "try:\n",
        "    # 4.55+ 新API\n",
        "    trainer.set_evaluate(strategy=\"epoch\")\n",
        "except Exception:\n",
        "    try:\n",
        "        # 通用老API\n",
        "        from transformers.trainer_utils import IntervalStrategy\n",
        "        trainer.args.evaluation_strategy = IntervalStrategy.EPOCH\n",
        "    except Exception:\n",
        "        # 字符串兜底\n",
        "        trainer.args.evaluation_strategy = \"epoch\"\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "print(\"== Dev metrics ==\", trainer.evaluate())\n",
        "\n",
        "trainer.save_model(OUTPUT_DIR); tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Saved to:\", OUTPUT_DIR)\n"
      ],
      "metadata": {
        "id": "QNSF6J9bMlPS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671,
          "referenced_widgets": [
            "997d801c1a254c26a5e4414fbcf96083",
            "945cf93a2b6a4c16aa13f066dcf75b6f",
            "7d8dab323aec4247a9c19037afd6b7c5",
            "8e67329302ac47969200c3fd286f8dc2",
            "47b42ea311cc44f092e2ecb9332bd665",
            "ae6c9c56ca59451e857cb5b9bfbd18f0",
            "93e71fea81af46da8a1fea802fec4fd1",
            "cd31f0aac1bd409a88457c5767f85533",
            "5cc31b585f49415096f0d3c8f6cc2a54",
            "2ddbfc26e018477c8b07fce1bf6f7d7b",
            "5cc74437d49740e298f791ab8165626d",
            "96c94bc1cd7645e083a2ecf23467b161",
            "9408941d92394a57abf558b70defa1e4",
            "b949ba66b41e469bbe502c47b45917cb",
            "cf9cb44b3e824fddafce14b3bf88fb65",
            "e3bda7677e214dafa113d908b5e31af8",
            "b9c8b75421d44f53a834bbe6dde6eaff",
            "f40cab9b54674b52ba5510ac60c5c2b2",
            "b1c78b11455d4629a2f8b4feb158fc92",
            "7f7515cc2fd2405eba41652008538b38",
            "1bb86119faa84c4bba8d585fba5d6361",
            "afdbf51791c442248088aee77314cd4f"
          ]
        },
        "outputId": "08326610-d054-4391-ce00-24b1b844bc76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyarrow<20.0.0,>=14.0.0\n",
            "  Using cached pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Using cached pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (42.1 MB)\n",
            "Installing collected packages: pyarrow\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 22.0.0\n",
            "    Uninstalling pyarrow-22.0.0:\n",
            "      Successfully uninstalled pyarrow-22.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.3.0 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyarrow-19.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mLoaded 7500 sentences. Example tokens[:20]: ['▁Nous', '▁avons', '▁découvert', '▁un', '▁restaurant', '▁merveille', 'ux', '▁à', '▁750', '11', '▁Paris', ',', '▁B', 'd', '▁Vol', 'taire', ',', '▁France', '▁pendant', '▁nos']\n",
            "Example labels[:20]: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'O', 'O']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "997d801c1a254c26a5e4414fbcf96083"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96c94bc1cd7645e083a2ecf23467b161"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-511366454.py:224: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='159' max='159' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [159/159 03:21, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.663400</td>\n",
              "      <td>0.064529</td>\n",
              "      <td>0.890069</td>\n",
              "      <td>0.916988</td>\n",
              "      <td>0.903328</td>\n",
              "      <td>0.985633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.045400</td>\n",
              "      <td>0.028117</td>\n",
              "      <td>0.951469</td>\n",
              "      <td>0.958816</td>\n",
              "      <td>0.955128</td>\n",
              "      <td>0.992606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.022800</td>\n",
              "      <td>0.026332</td>\n",
              "      <td>0.955272</td>\n",
              "      <td>0.962033</td>\n",
              "      <td>0.958641</td>\n",
              "      <td>0.993589</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Dev metrics == {'eval_loss': 0.026332031935453415, 'eval_precision': 0.9552715654952076, 'eval_recall': 0.9620334620334621, 'eval_f1': 0.9586405899326707, 'eval_accuracy': 0.993588543616623, 'eval_runtime': 0.9889, 'eval_samples_per_second': 758.448, 'eval_steps_per_second': 6.068, 'epoch': 3.0}\n",
            "Saved to: /content/ner_xlmr_out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **在设置好的conll格式验证集下验证CheckPoints下模型效果**"
      ],
      "metadata": {
        "id": "Iu8kk_em8Xjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct inference for PRETOKENIZED XLM-R tokens\n",
        "import torch, random, numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "MODEL_DIR = \"/content/drive/MyDrive/NER/CheckPoints/checkpoint-6330_c2\"      # 最新checkpoint根目录\n",
        "DATA_PATH = \"/content/drive/MyDrive/NER/train_datasets/train_all.conll\"   # 或验证集conll路径\n",
        "\n",
        "LABELS = [\"O\", \"B-ADDRESS\", \"I-ADDRESS\", \"B-TIME\", \"I-TIME\"]\n",
        "#LABELS = [\"O\", \"B-ADDRESS\", \"I-ADDRESS\"]\n",
        "label2id = {l:i for i,l in enumerate(LABELS)}\n",
        "id2label = {i:l for l,i in label2id.items()}\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\n",
        "model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR)\n",
        "model.eval()\n",
        "\n",
        "def read_conll(path):\n",
        "    sents_tok, sents_lab, toks, labs = [], [], [], []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line=line.strip()\n",
        "            if not line:\n",
        "                if toks:\n",
        "                    sents_tok.append(toks); sents_lab.append(labs)\n",
        "                    toks, labs = [], []\n",
        "                continue\n",
        "            if \" \" in line:\n",
        "                t, y = line.split(\" \", 1)\n",
        "            else:\n",
        "                t, y = line, \"O\"\n",
        "            if y not in label2id: y = \"O\"\n",
        "            toks.append(t); labs.append(y)\n",
        "    if toks:\n",
        "        sents_tok.append(toks); sents_lab.append(labs)\n",
        "    return sents_tok, sents_lab\n",
        "\n",
        "tokens_all, tags_all = read_conll(DATA_PATH)\n",
        "\n",
        "def predict_labels_for_pretokenized(tokens):\n",
        "    # tokens -> ids（保持与训练一致，不再重新分词）\n",
        "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    # 未知子词用 unk\n",
        "    ids = [i if i is not None else tokenizer.unk_token_id for i in ids]\n",
        "    # 加 special tokens\n",
        "    CLS_ID = tokenizer.convert_tokens_to_ids(tokenizer.cls_token)  # <s>\n",
        "    SEP_ID = tokenizer.convert_tokens_to_ids(tokenizer.sep_token)  # </s>\n",
        "    input_ids = [CLS_ID] + ids + [SEP_ID]\n",
        "    attn = [1] * len(input_ids)\n",
        "\n",
        "    enc = {\n",
        "        \"input_ids\": torch.tensor([input_ids]),\n",
        "        \"attention_mask\": torch.tensor([attn]),\n",
        "    }\n",
        "    with torch.no_grad():\n",
        "        logits = model(**enc).logits[0]  # (seq_len, num_labels)\n",
        "    pred_ids = logits.argmax(-1).tolist()\n",
        "\n",
        "    # 去掉 special tokens 的标签\n",
        "    pred_ids = pred_ids[1:-1]\n",
        "    assert len(pred_ids) == len(tokens), (len(pred_ids), len(tokens))\n",
        "    return [id2label[i] for i in pred_ids]\n",
        "\n",
        "# 随机抽 10 条看看\n",
        "random.seed(42)\n",
        "idxs = random.sample(range(len(tokens_all)), 10)\n",
        "for i in idxs:\n",
        "    toks = tokens_all[i]\n",
        "    gold = tags_all[i]\n",
        "    pred = predict_labels_for_pretokenized(toks)\n",
        "\n",
        "    print(\"=\"*40)\n",
        "    print(\"原始 Tokens:\", \" \".join(toks))\n",
        "    print(\"真实 Labels:\", \" \".join(gold))\n",
        "    print(\"预测 Labels:\", \" \".join(pred))\n",
        "    # 一致性断言\n",
        "    if len(pred) != len(toks):\n",
        "        print(\"[WARN] 长度不一致！\", len(pred), len(toks))\n"
      ],
      "metadata": {
        "id": "DBshibPx6qzM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ad7ec33-64b5-4815-80e5-bbde0032c0cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "原始 Tokens: ▁— M öt et ▁börjar ▁klockan ▁10.00 ▁imorgon .\n",
            "真实 Labels: O O O O O O O O O\n",
            "预测 Labels: O O O O O I-TIME I-TIME O O\n",
            "========================================\n",
            "原始 Tokens: ▁ , b IT TE ▁DEN KEN ▁s IE ▁D ARAN , ▁DIE ▁d OKU MENTE ▁ BIS ▁m ON TAG ▁ ZU ▁SE NDEN .\n",
            "真实 Labels: O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "预测 Labels: O O I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O O O I-TIME I-TIME I-TIME O O O O O\n",
            "========================================\n",
            "原始 Tokens: ▁/ 请 前往 1 栋 登记 , 并 同时 关注 百 联 东 郊 购物 中心 进展 。\n",
            "真实 Labels: O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O\n",
            "预测 Labels: O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O\n",
            "========================================\n",
            "原始 Tokens: ▁- В чер а ▁я ▁был ▁в ▁Ме тро ▁« О кт яб рь ская » ▁( ради альная ), ▁и ▁мне ▁понравилось . ▁Надо ▁еще ▁с ходить ▁в ▁З да ние ▁10 ▁и ▁сравни ть . ▁Кстати , ▁как ▁тебе ▁Ми чу рин ский ▁проспект ▁и ▁ул . ▁Жу кова , ▁д . ▁5 , ▁ оф . ▁301 , ▁Волгоград , ▁Волгоград ская ▁область , ▁400 001 ?\n",
            "真实 Labels: O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "预测 Labels: O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "========================================\n",
            "原始 Tokens: ▁? 说明 书 在 幸福 蓝 海 国际 影 城 , 试 用 装 在 5 栋 。 如果有 需要 , 请 到 室外 排 球场 登记 , 领 取 地点 是 7 栋 。\n",
            "真实 Labels: O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O\n",
            "预测 Labels: O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O\n",
            "========================================\n",
            "原始 Tokens: ▁ . Се год ня ▁был ▁отличный ▁день , ▁пойду ▁отдыха ть !\n",
            "真实 Labels: O O O O O O O O O O O O O\n",
            "预测 Labels: O O O O I-TIME O O O O O O O O\n",
            "========================================\n",
            "原始 Tokens: ▁· Vo cê ▁pode ▁me ▁encontrar ▁em ▁Junta ▁de ▁Fre gues ia ▁de ▁São ▁João ▁da ▁Cor ve ira ▁às ▁15 h .\n",
            "真实 Labels: O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O\n",
            "预测 Labels: O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-TIME I-TIME I-TIME I-TIME O\n",
            "========================================\n",
            "原始 Tokens: ▁? h AR ▁DU ▁SET ▁ DAG ENS ▁NY HE DER ?\n",
            "真实 Labels: O O O O O O O O O O O O\n",
            "预测 Labels: O O O O O O O O O O O O\n",
            "========================================\n",
            "原始 Tokens: ▁ , Т ы ▁уже ▁посмотрел ▁новый ▁фильм ?\n",
            "真实 Labels: O O O O O O O O O\n",
            "预测 Labels: O O O O O O O O O\n",
            "========================================\n",
            "原始 Tokens: ▁! Din ▁bokning ▁har ▁bekräfta ts ▁och ▁ platsen ▁är ▁Hum le gården .\n",
            "真实 Labels: O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "预测 Labels: O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 检查check point里的模型（pytorch）"
      ],
      "metadata": {
        "id": "Vpk5ajhNK6sM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, torch, numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "# 模型目录：可以是总目录（包含多个 checkpoint-* 子目录），也可以直接是某个 checkpoint-xxx\n",
        "MODEL_DIR = \"/content/drive/MyDrive/NER/CheckPoints/bestCheck\"  # 实际目录\n",
        "\n",
        "LABELS = [\"O\", \"B-ADDRESS\", \"I-ADDRESS\", \"B-TIME\", \"I-TIME\"]\n",
        "#LABELS = [\"O\", \"B-ADDRESS\", \"I-ADDRESS\"]\n",
        "id2label = {i:l for i,l in enumerate(LABELS)}\n",
        "\n",
        "def latest_checkpoint(base_dir: str) -> str:\n",
        "    if os.path.basename(base_dir).startswith(\"checkpoint-\") and os.path.isdir(base_dir):\n",
        "        return base_dir\n",
        "    cks = [d for d in os.listdir(base_dir) if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(base_dir, d))]\n",
        "    if not cks:\n",
        "        return base_dir\n",
        "    cks.sort(key=lambda x: int(x.split(\"-\")[-1]))\n",
        "    return os.path.join(base_dir, cks[-1])\n",
        "\n",
        "CKPT = latest_checkpoint(MODEL_DIR)\n",
        "print(\"Load from:\", CKPT)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(CKPT, use_fast=True)\n",
        "model = AutoModelForTokenClassification.from_pretrained(CKPT)\n",
        "model.eval()\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "def predict_entities(text: str, max_len: int = 256):\n",
        "    enc = tokenizer(text, return_offsets_mapping=True, truncation=True,\n",
        "                    max_length=max_len, return_tensors=\"pt\")\n",
        "    offsets = enc.pop(\"offset_mapping\")\n",
        "    if torch.cuda.is_available():\n",
        "        enc = {k: v.cuda() for k, v in enc.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**enc).logits[0]  # (seq_len, num_labels)\n",
        "\n",
        "    pred_ids = logits.argmax(-1).detach().cpu().numpy().tolist()\n",
        "    input_ids = enc[\"input_ids\"][0].detach().cpu().tolist()\n",
        "    offsets = offsets[0].detach().cpu().tolist()\n",
        "    toks = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    # 过滤 special tokens & 0 长度偏移\n",
        "    keep = []\n",
        "    for tok, (st, ed) in zip(toks, offsets):\n",
        "        if tok in (tokenizer.cls_token, tokenizer.sep_token, \"<s>\", \"</s>\", tokenizer.pad_token):\n",
        "            keep.append(False)\n",
        "        elif (st == 0 and ed == 0):\n",
        "            keep.append(False)\n",
        "        else:\n",
        "            keep.append(True)\n",
        "\n",
        "    toks = [t for t,k in zip(toks, keep) if k]\n",
        "    offs = [tuple(map(int, o)) for o,k in zip(offsets, keep) if k]\n",
        "    labs = [id2label[i] for i,k in zip(pred_ids, keep) if k]\n",
        "\n",
        "    # 合并 B-/I- 为实体片段（按字符偏移还原原文片段）\n",
        "    spans = []\n",
        "    cur_type, cur_s, cur_e = None, None, None\n",
        "    for lab, (st, ed) in zip(labs, offs):\n",
        "        if lab.startswith(\"B-\"):\n",
        "            if cur_type:\n",
        "                spans.append((cur_type, cur_s, cur_e))\n",
        "            cur_type, cur_s, cur_e = lab[2:], st, ed\n",
        "        elif lab.startswith(\"I-\") and cur_type == lab[2:]:\n",
        "            cur_e = ed\n",
        "        else:\n",
        "            if cur_type:\n",
        "                spans.append((cur_type, cur_s, cur_e))\n",
        "            cur_type, cur_s, cur_e = None, None, None\n",
        "    if cur_type:\n",
        "        spans.append((cur_type, cur_s, cur_e))\n",
        "\n",
        "    entities = [(typ, text[s:e]) for typ, s, e in spans]\n",
        "    return toks, labs, entities\n",
        "\n",
        "# 样例\n",
        "samples = [\n",
        "    \"TCL手机25.8.12震撼发售\",\n",
        "    \"25.8.12\",\n",
        "    \"甲辰年七月初十\",\n",
        "    \"下周五下午2点 客户拜访/地点：北京分公司\",\n",
        "    \"明天早上来6楼会议室开会\",\n",
        "    \"我站在南京市长江大桥眺望\",\n",
        "    \"我想去东洞镇小亮村\",\n",
        "    \"我打算農曆十月初五中午十二點零五分到西藏自治區山南市浪卡子縣羊卓雍錯參加讀書會,聽說活動很有趣。\",\n",
        "    \"请在2026年4月8日早上9:30到恩施土家族苗族自治州集合。\",\n",
        "    \"Meet me at 221B Baker Street, London at 7 pm tomorrow.\",\n",
        "    \"6月5日（月曜日）に新潟県新潟市中央区万代3丁目7-8で集合しましょう。\",\n",
        "    \"Perşembe 22 Ağustos 2024 saat 10:15'te İstiklal Cd. No:56, Beyoğlu, İstanbul'da buluşalım。\",\n",
        "    \"今晚8点我们去尝尝新安街道的那家麦当劳怎么样，我没什么别的想吃的啦，还是说你打算在公司食堂吃？那样的话我就回西乡吃晚餐了。\",\n",
        "    \"只有你好好听我们的话，后天晚上我们才有机会去西部欢乐园玩，否则一直到国庆节结束，你都得待在兰州西站的兰州中心2楼里的西西弗书店里乖乖看书。好好想想吧，明天晚上6点前我要你给出答复。\"\n",
        "]\n",
        "\n",
        "for i, s in enumerate(samples, 1):\n",
        "    toks, labs, ents = predict_entities(s)\n",
        "    print(\"=\"*68)\n",
        "    print(f\"[{i}] Text:\", s)\n",
        "    print(\"TOK:\", \" \".join(toks))\n",
        "    print(\"LBL:\", \" \".join(labs))\n",
        "    print(\"Spans:\", ents)\n"
      ],
      "metadata": {
        "id": "hsXFc5DR86zO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a292960-473b-40cd-a7ad-ad317c3bfdbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load from: /content/drive/MyDrive/NER/CheckPoints/bestCheck\n",
            "====================================================================\n",
            "[1] Text: TCL手机25.8.12震撼发售\n",
            "TOK: ▁T CL 手机 25 .8. 12 震撼 发 售\n",
            "LBL: B-ADDRESS I-ADDRESS O B-TIME I-TIME I-TIME O O O\n",
            "Spans: [('ADDRESS', 'TCL'), ('TIME', '25.8.12')]\n",
            "====================================================================\n",
            "[2] Text: 25.8.12\n",
            "TOK: ▁25 .8. 12\n",
            "LBL: B-TIME I-TIME I-TIME\n",
            "Spans: [('TIME', '25.8.12')]\n",
            "====================================================================\n",
            "[3] Text: 甲辰年七月初十\n",
            "TOK: ▁ 甲 辰 年 七 月初 十\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME\n",
            "Spans: [('TIME', '甲辰年七月初十')]\n",
            "====================================================================\n",
            "[4] Text: 下周五下午2点 客户拜访/地点：北京分公司\n",
            "TOK: ▁ 下 周五 下午 2 点 ▁ 客户 拜 访 / 地点 : 北京 分公司\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O B-ADDRESS I-ADDRESS\n",
            "Spans: [('TIME', '下周五下午2点'), ('ADDRESS', '北京分公司')]\n",
            "====================================================================\n",
            "[5] Text: 明天早上来6楼会议室开会\n",
            "TOK: ▁ 明天 早上 来 6 楼 会议 室 开 会\n",
            "LBL: B-TIME I-TIME I-TIME O O O O O O O\n",
            "Spans: [('TIME', '明天早上')]\n",
            "====================================================================\n",
            "[6] Text: 我站在南京市长江大桥眺望\n",
            "TOK: ▁我 站在 南京 市 长江 大 桥 眺 望\n",
            "LBL: O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O\n",
            "Spans: [('ADDRESS', '南京市长江大桥')]\n",
            "====================================================================\n",
            "[7] Text: 我想去东洞镇小亮村\n",
            "TOK: ▁我 想去 东 洞 镇 小 亮 村\n",
            "LBL: O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS\n",
            "Spans: [('ADDRESS', '东洞镇小亮村')]\n",
            "====================================================================\n",
            "[8] Text: 我打算農曆十月初五中午十二點零五分到西藏自治區山南市浪卡子縣羊卓雍錯參加讀書會,聽說活動很有趣。\n",
            "TOK: ▁我 打算 農曆 十 月初 五 中午 十二 點 零 五 分 到 西藏 自治 區 山 南 市 浪 卡 子 縣 羊 卓 雍 錯 參加 讀書 會 , 聽說 活動 很 有趣 。\n",
            "LBL: O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O\n",
            "Spans: [('TIME', '農曆十月初五中午十二點零五分'), ('ADDRESS', '西藏自治區山南市浪卡子縣羊卓雍錯')]\n",
            "====================================================================\n",
            "[9] Text: 请在2026年4月8日早上9:30到恩施土家族苗族自治州集合。\n",
            "TOK: ▁ 请 在 20 26 年 4 月 8 日 早上 9 : 30 到 恩 施 土 家族 苗 族 自治 州 集合 。\n",
            "LBL: O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O\n",
            "Spans: [('TIME', '2026年4月8日早上9:30'), ('ADDRESS', '恩施土家族苗族自治州')]\n",
            "====================================================================\n",
            "[10] Text: Meet me at 221B Baker Street, London at 7 pm tomorrow.\n",
            "TOK: ▁Meet ▁me ▁at ▁221 B ▁Baker ▁Street , ▁London ▁at ▁7 ▁pm ▁tomorrow .\n",
            "LBL: O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME O\n",
            "Spans: [('ADDRESS', '221B Baker Street, London'), ('TIME', 'at 7 pm tomorrow')]\n",
            "====================================================================\n",
            "[11] Text: 6月5日（月曜日）に新潟県新潟市中央区万代3丁目7-8で集合しましょう。\n",
            "TOK: ▁6 月 5 日 ( 月 曜 日 ) に 新潟 県 新潟 市 中央 区 万 代 3 丁目 7-8 で 集合 しましょう 。\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O\n",
            "Spans: [('TIME', '6月5日（月曜日）'), ('ADDRESS', '新潟県新潟市中央区万代3丁目7-8')]\n",
            "====================================================================\n",
            "[12] Text: Perşembe 22 Ağustos 2024 saat 10:15'te İstiklal Cd. No:56, Beyoğlu, İstanbul'da buluşalım。\n",
            "TOK: ▁Perşembe ▁22 ▁Ağustos ▁20 24 ▁saat ▁10 :15 ' te ▁İstiklal ▁C d . ▁No :56 , ▁Bey oğlu , ▁İstanbul ' da ▁buluş alım 。\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O\n",
            "Spans: [('TIME', \"Perşembe 22 Ağustos 2024 saat 10:15'\"), ('ADDRESS', 'İstiklal Cd. No:56, Beyoğlu, İstanbul')]\n",
            "====================================================================\n",
            "[13] Text: 今晚8点我们去尝尝新安街道的那家麦当劳怎么样，我没什么别的想吃的啦，还是说你打算在公司食堂吃？那样的话我就回西乡吃晚餐了。\n",
            "TOK: ▁ 今晚 8 点 我们 去 尝 尝 新 安 街道 的那 家 麦 当 劳 怎么样 , 我 没什么 别 的 想 吃的 啦 , 还是 说 你 打算 在 公司 食堂 吃 ? 那样 的话 我就 回 西 乡 吃 晚餐 了 。\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME O O O O B-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS O O O O O O B-ADDRESS I-ADDRESS O O O O\n",
            "Spans: [('TIME', '今晚8点'), ('ADDRESS', '新安街道'), ('ADDRESS', '公司食堂'), ('ADDRESS', '西乡')]\n",
            "====================================================================\n",
            "[14] Text: 只有你好好听我们的话，后天晚上我们才有机会去西部欢乐园玩，否则一直到国庆节结束，你都得待在兰州西站的兰州中心2楼里的西西弗书店里乖乖看书。好好想想吧，明天晚上6点前我要你给出答复。\n",
            "TOK: ▁ 只有 你 好好 听 我们 的话 , 后 天 晚上 我们 才 有机会 去 西部 欢乐 园 玩 , 否则 一直 到 国庆 节 结束 , 你 都 得 待 在 兰州 西 站 的 兰州 中心 2 楼 里的 西 西 弗 书 店 里 乖乖 看 书 。 好好 想想 吧 , 明天 晚上 6 点 前 我要 你 给出 答 复 。\n",
            "LBL: O O O O O O O O B-TIME I-TIME I-TIME O O O O B-ADDRESS I-ADDRESS I-ADDRESS O O O O O B-TIME I-TIME I-TIME O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O\n",
            "Spans: [('TIME', '后天晚上'), ('ADDRESS', '西部欢乐园'), ('TIME', '国庆节结束'), ('ADDRESS', '兰州西站的'), ('ADDRESS', '兰州中心2楼里的'), ('ADDRESS', '西西弗书店'), ('TIME', '明天晚上6点前')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **在json格式的datasets测试集下进行测试**"
      ],
      "metadata": {
        "id": "7KoyTca2ab4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   展示部分：前 SHOW_N 条样本输出预测实体对照 Gold 标签\n",
        "*   词级指标：集合交集计算 precision/recall/F1\n",
        "*   句级指标：如果 Gold 这个类型的 token 全部出现在预测 token 集合中，则该句该类实体句级准确率计 1，否则计 0\n",
        "每个文件单独统计两个类别（TIME/ADDRESS）的指标"
      ],
      "metadata": {
        "id": "X77Sdqprfio5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, torch\n",
        "import regex as re\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# ========= 参数 =========\n",
        "MODEL_DIR = \"/content/ner_xlmr_out/checkpoint-159\"\n",
        "LABELS = [\"O\", \"B-ADDRESS\", \"I-ADDRESS\", \"B-TIME\", \"I-TIME\"]\n",
        "id2label = {i: l for i, l in enumerate(LABELS)}\n",
        "DATASET_DIR = \"/content/drive/MyDrive/NER/datasets\"\n",
        "SHOW_N = 3  # 每个文件展示多少示例\n",
        "STOPWORDS_DIR = \"/content/drive/MyDrive/NER/stopwords\"  # 停用词目录（新增）\n",
        "\n",
        "SPACE_LANGS = [\n",
        "\n",
        "]\n",
        "NO_SPACE_LANGS = [\"简体中文\", \"繁体中文\", \"日语\", \"英语\", \"法语\", \"德语\", \"意大利语\", \"西班牙语\", \"葡萄牙语\",\n",
        "    \"荷兰语\", \"瑞典语\", \"丹麦语\", \"俄语\", \"土耳其语\", \"韩语\"]\n",
        "\n",
        "LANGUAGES = [\n",
        "    \"简体中文\", \"英语\", \"丹麦语\", \"俄语\", \"土耳其语\", \"德语\",\n",
        "    \"意大利语\", \"日语\", \"法语\", \"瑞典语\", \"荷兰语\", \"葡萄牙语\",\n",
        "    \"西班牙语\", \"韩语\", \"繁体中文\"\n",
        "]\n",
        "\n",
        "# ========= 加载停用词 =========\n",
        "def load_stopwords(stop_dir, languages):\n",
        "    sw_dict = {}\n",
        "    for lang in languages:\n",
        "        path = os.path.join(stop_dir, f\"{lang}.txt\")\n",
        "        sw_set = set()\n",
        "        if os.path.isfile(path):\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                for line in f:\n",
        "                    w = line.strip()\n",
        "                    if w:\n",
        "                        sw_set.add(w.lower())\n",
        "        sw_dict[lang] = sw_set\n",
        "    return sw_dict\n",
        "\n",
        "STOPWORDS = load_stopwords(STOPWORDS_DIR, LANGUAGES)\n",
        "\n",
        "def process_entity(entity, sw_set, is_addr):\n",
        "    \"\"\"\n",
        "    按规则过滤单个实体：\n",
        "    1. 停用词在首尾 => 删除，保持实体个数不变\n",
        "    2. 停用词在中间：地址 => 拆分成两个实体；时间 => 删除停用词直接合并\n",
        "    \"\"\"\n",
        "    ent = entity.strip()\n",
        "    ent_low = ent.lower()\n",
        "    for sw in sw_set:\n",
        "        if sw in ent_low:\n",
        "            # 首部\n",
        "            if ent_low.startswith(sw):\n",
        "                cleaned = ent[len(sw):].strip()\n",
        "                return [cleaned] if cleaned else []\n",
        "            # 末尾\n",
        "            elif ent_low.endswith(sw):\n",
        "                cleaned = ent[:-len(sw)].strip()\n",
        "                return [cleaned] if cleaned else []\n",
        "            else:\n",
        "                # 中间部分\n",
        "                if is_addr:\n",
        "                    parts = ent.split(sw)\n",
        "                    parts = [p.strip() for p in parts if p.strip()]\n",
        "                    return parts\n",
        "                else:\n",
        "                    cleaned = ent.replace(sw, \"\").strip()\n",
        "                    return [cleaned] if cleaned else []\n",
        "    return [ent]\n",
        "\n",
        "def filter_entities_with_split(entities, lang, is_addr):\n",
        "    sw_set = STOPWORDS.get(lang, set())\n",
        "    result = []\n",
        "    for ent in entities:\n",
        "        result.extend(process_entity(ent, sw_set, is_addr))\n",
        "    return result\n",
        "\n",
        "# ========= 加载最新checkpoint =========\n",
        "def latest_checkpoint(base_dir: str) -> str:\n",
        "    if os.path.basename(base_dir).startswith(\"checkpoint-\") and os.path.isdir(base_dir):\n",
        "        return base_dir\n",
        "    cks = [d for d in os.listdir(base_dir) if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(base_dir, d))]\n",
        "    if not cks:\n",
        "        return base_dir\n",
        "    cks.sort(key=lambda x: int(x.split(\"-\")[-1]))\n",
        "    return os.path.join(base_dir, cks[-1])\n",
        "\n",
        "CKPT = latest_checkpoint(MODEL_DIR)\n",
        "print(\"Load from:\", CKPT)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(CKPT, use_fast=True)\n",
        "model = AutoModelForTokenClassification.from_pretrained(CKPT)\n",
        "model.eval()\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "# ========= 工具函数 =========\n",
        "def is_symbol(tok: str) -> bool:\n",
        "    stripped = tok.replace('▁', '')\n",
        "    return bool(re.fullmatch(r'[\\p{P}\\p{S}]+', stripped))\n",
        "\n",
        "def merge_tokens_by_underscore_infer(tokens, labels, offsets):\n",
        "    merged_tokens, merged_labels, merged_offsets = [], [], []\n",
        "    cur_tok, cur_lab, cur_start, cur_end = \"\", None, None, None\n",
        "    for tok, lab, (st, ed) in zip(tokens, labels, offsets):\n",
        "        if tok is None:\n",
        "            tok = \"\"\n",
        "        elif not isinstance(tok, str):\n",
        "            tok = str(tok)\n",
        "        if tok.startswith(\"▁\") or is_symbol(tok):\n",
        "            if cur_tok:\n",
        "                merged_tokens.append(cur_tok)\n",
        "                merged_labels.append(cur_lab)\n",
        "                merged_offsets.append((cur_start, cur_end))\n",
        "            cur_tok = tok\n",
        "            cur_lab = lab\n",
        "            cur_start = st\n",
        "            cur_end = ed\n",
        "        else:\n",
        "            cur_tok += tok\n",
        "            cur_end = ed\n",
        "    if cur_tok:\n",
        "        merged_tokens.append(cur_tok)\n",
        "        merged_labels.append(cur_lab)\n",
        "        merged_offsets.append((cur_start, cur_end))\n",
        "    return merged_tokens, merged_labels, merged_offsets\n",
        "\n",
        "def predict_entities(text: str, lang: str, max_len: int = 256):\n",
        "    enc = tokenizer(text, return_offsets_mapping=True, truncation=True,\n",
        "                    max_length=max_len, return_tensors=\"pt\")\n",
        "    offsets = enc.pop(\"offset_mapping\")\n",
        "    if torch.cuda.is_available():\n",
        "        enc = {k: v.cuda() for k, v in enc.items()}\n",
        "    with torch.no_grad():\n",
        "        logits = model(**enc).logits[0]\n",
        "    pred_ids = logits.argmax(-1).detach().cpu().numpy().tolist()\n",
        "    input_ids = enc[\"input_ids\"][0].detach().cpu().tolist()\n",
        "    offsets = offsets[0].detach().cpu().tolist()\n",
        "    toks = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    keep = []\n",
        "    for tok, (st, ed) in zip(toks, offsets):\n",
        "        if tok in (tokenizer.cls_token, tokenizer.sep_token, \"<s>\", \"</s>\", tokenizer.pad_token):\n",
        "            keep.append(False)\n",
        "        elif (st == 0 and ed == 0):\n",
        "            keep.append(False)\n",
        "        else:\n",
        "            keep.append(True)\n",
        "    toks = [t for t, k in zip(toks, keep) if k]\n",
        "    offs = [tuple(map(int, o)) for o, k in zip(offsets, keep) if k]\n",
        "    labs = [id2label[i] for i, k in zip(pred_ids, keep) if k]\n",
        "    if lang in SPACE_LANGS:\n",
        "        toks, labs, offs = merge_tokens_by_underscore_infer(toks, labs, offs)\n",
        "    spans = []\n",
        "    cur_type, cur_s, cur_e = None, None, None\n",
        "    for lab, (st, ed) in zip(labs, offs):\n",
        "        if lab.startswith(\"B-\"):\n",
        "            if cur_type:\n",
        "                spans.append((cur_type, cur_s, cur_e))\n",
        "            cur_type, cur_s, cur_e = lab[2:], st, ed\n",
        "        elif lab.startswith(\"I-\") and cur_type == lab[2:]:\n",
        "            cur_e = ed\n",
        "        else:\n",
        "            if cur_type:\n",
        "                spans.append((cur_type, cur_s, cur_e))\n",
        "            cur_type, cur_s, cur_e = None, None, None\n",
        "    if cur_type:\n",
        "        spans.append((cur_type, cur_s, cur_e))\n",
        "    entities = [(typ, text[s:e]) for typ, s, e in spans]\n",
        "    return toks, labs, entities\n",
        "\n",
        "def token_level_metrics(pred_tokens, gold_tokens):\n",
        "    pred_set, gold_set = set(pred_tokens), set(gold_tokens)\n",
        "    if not gold_set:\n",
        "        return None\n",
        "    tp = len(pred_set & gold_set)\n",
        "    precision = tp / len(pred_set) if pred_set else 0.0\n",
        "    recall = tp / len(gold_set) if gold_set else 0.0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision+recall) > 0 else 0.0\n",
        "    return precision, recall, f1\n",
        "\n",
        "def char_overlap_rate(pred_strs, gold_strs):\n",
        "    if not gold_strs:\n",
        "        return 1.0 if not pred_strs else 0.0\n",
        "    pred_text = \"\".join(pred_strs)\n",
        "    gold_text = \"\".join(gold_strs)\n",
        "    common = sum(min(pred_text.count(c), gold_text.count(c)) for c in set(gold_text))\n",
        "    return common / len(gold_text) if gold_text else 0.0\n",
        "# 在循环开始前加：\n",
        "time_F1_list = []\n",
        "addr_F1_list = []\n",
        "# ========= 主测试流程 =========\n",
        "for fname in os.listdir(DATASET_DIR):\n",
        "    if not fname.endswith(\".json\"):\n",
        "        continue\n",
        "    lang = fname.replace(\".json\", \"\")\n",
        "    path = os.path.join(DATASET_DIR, fname)\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    print(f\"\\n==== Testing on file: {fname} ====\")\n",
        "    metrics_time_tok, metrics_addr_tok = [], []\n",
        "    overlap_time_list, overlap_addr_list = [], []\n",
        "    for idx, sample in enumerate(data):\n",
        "        sid = sample.get(\"id\")\n",
        "        sent = sample.get(\"sentence\")\n",
        "        gold_time = sample.get(\"time_tokens\", [])\n",
        "        gold_addr = sample.get(\"location_tokens\", [])\n",
        "        toks, labs, ents = predict_entities(sent, lang)\n",
        "\n",
        "        # Gold tokens\n",
        "        gold_time_toks, gold_addr_toks = [], []\n",
        "        for gt in gold_time:\n",
        "            g_tokens = tokenizer.tokenize(gt)\n",
        "            g_labels = [\"B-TIME\"] + [\"I-TIME\"] * (len(g_tokens)-1)\n",
        "            g_offs = [(0,0)] * len(g_tokens)\n",
        "            if lang in SPACE_LANGS:\n",
        "                g_tokens, g_labels, g_offs = merge_tokens_by_underscore_infer(g_tokens, g_labels, g_offs)\n",
        "            gold_time_toks.extend(g_tokens)\n",
        "        for ga in gold_addr:\n",
        "            g_tokens = tokenizer.tokenize(ga)\n",
        "            g_labels = [\"B-ADDRESS\"] + [\"I-ADDRESS\"] * (len(g_tokens)-1)\n",
        "            g_offs = [(0,0)] * len(g_tokens)\n",
        "            if lang in SPACE_LANGS:\n",
        "                g_tokens, g_labels, g_offs = merge_tokens_by_underscore_infer(g_tokens, g_labels, g_offs)\n",
        "            gold_addr_toks.extend(g_tokens)\n",
        "\n",
        "        pred_time_toks = [t for t, l in zip(toks, labs) if l.endswith(\"TIME\")]\n",
        "        pred_addr_toks = [t for t, l in zip(toks, labs) if l.endswith(\"ADDRESS\")]\n",
        "\n",
        "        m_time = token_level_metrics(pred_time_toks, gold_time_toks)\n",
        "        m_addr = token_level_metrics(pred_addr_toks, gold_addr_toks)\n",
        "        if m_time: metrics_time_tok.append(m_time)\n",
        "        if m_addr: metrics_addr_tok.append(m_addr)\n",
        "\n",
        "        # 原字符串列表\n",
        "        pred_time_strs = [e[1] for e in ents if e[0] == \"TIME\"]\n",
        "        pred_addr_strs = [e[1] for e in ents if e[0] == \"ADDRESS\"]\n",
        "\n",
        "        # ==== 停用词后处理 ====\n",
        "        pred_time_strs = filter_entities_with_split(pred_time_strs, lang, is_addr=False)\n",
        "        pred_addr_strs = filter_entities_with_split(pred_addr_strs, lang, is_addr=True)\n",
        "\n",
        "        ov_time = char_overlap_rate(pred_time_strs, gold_time)\n",
        "        ov_addr = char_overlap_rate(pred_addr_strs, gold_addr)\n",
        "        if ov_time is not None: overlap_time_list.append(ov_time)\n",
        "        if ov_addr is not None: overlap_addr_list.append(ov_addr)\n",
        "\n",
        "        if idx < SHOW_N:\n",
        "            print(\"-\"*68)\n",
        "            print(f\"[{sid}] Text:\", sent)\n",
        "            print(\"TOK:\", \" \".join(toks))\n",
        "            print(\"LBL:\", \" \".join(labs))\n",
        "            print(f\"PRED TIME: {pred_time_strs}\")\n",
        "            print(f\"GOLD TIME: {gold_time}\")\n",
        "            print(f\"PRED ADDR: {pred_addr_strs}\")\n",
        "            print(f\"GOLD ADDR: {gold_addr}\")\n",
        "            print(f\"字符重合率: TIME={ov_time:.3f}, ADDR={ov_addr:.3f}\")\n",
        "\n",
        "\n",
        "    def avg_metrics(m_list):\n",
        "        if not m_list: return (0,0,0)\n",
        "        p = sum(x[0] for x in m_list) / len(m_list)\n",
        "        r = sum(x[1] for x in m_list) / len(m_list)\n",
        "        f1 = sum(x[2] for x in m_list) / len(m_list)\n",
        "        return (p,r,f1)\n",
        "\n",
        "    avg_time = avg_metrics(metrics_time_tok)\n",
        "    avg_addr = avg_metrics(metrics_addr_tok)\n",
        "    avg_overlap_time = sum(overlap_time_list) / len(overlap_time_list) if overlap_time_list else 0\n",
        "    avg_overlap_addr = sum(overlap_addr_list) / len(overlap_addr_list) if overlap_addr_list else 0\n",
        "\n",
        "    print(f\"[Metrics TIME] token-level: P={avg_time[0]:.3f}, R={avg_time[1]:.3f}, F1={avg_time[2]:.3f}\")\n",
        "    print(f\"[Metrics ADDR] token-level: P={avg_addr[0]:.3f}, R={avg_addr[1]:.3f}, F1={avg_addr[2]:.3f}\")\n",
        "    print(f\"[Overlap] 平均字符重合率: TIME={avg_overlap_time:.3f}, ADDR={avg_overlap_addr:.3f}\")\n",
        "    #添加记录\n",
        "    time_F1_list.append(avg_time[2])  # F1值\n",
        "    addr_F1_list.append(avg_addr[2])\n",
        "# ====================== 循环结束后绘图 ======================\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Bar(\n",
        "    x=LANGUAGES[:len(time_F1_list)],  # 只取测试到的语言\n",
        "    y=time_F1_list,\n",
        "    name='TIME F1',\n",
        "    marker_color='#1f77b4',\n",
        "    text=[f\"{v:.3f}\" for v in time_F1_list],\n",
        "    textposition='outside'\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Bar(\n",
        "    x=LANGUAGES[:len(addr_F1_list)],\n",
        "    y=addr_F1_list,\n",
        "    name='ADDR F1',\n",
        "    marker_color='#ff7f0e',\n",
        "    text=[f\"{v:.3f}\" for v in addr_F1_list],\n",
        "    textposition='outside'\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    barmode='group',\n",
        "    bargap=0.25,\n",
        "    plot_bgcolor='rgba(250,250,250,1)',\n",
        "    paper_bgcolor='white',\n",
        "    xaxis=dict(title='语言', tickangle=-30),\n",
        "    yaxis=dict(title='F1 Score', range=[0,1], dtick=0.1),\n",
        "    legend=dict(title=dict(text='指标类型'), orientation='h',\n",
        "                yanchor='bottom', y=1.05,\n",
        "                xanchor='right', x=1),\n",
        "    font=dict(family='Microsoft YaHei, sans-serif', size=13)\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gr6eyX3oamNY",
        "outputId": "f1b6bb1d-0c98-46c8-a0e1-71319ef94dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load from: /content/ner_xlmr_out/checkpoint-159\n",
            "\n",
            "==== Testing on file: 简体中文.json ====\n",
            "--------------------------------------------------------------------\n",
            "[1] Text: 2024年6月15日早上8:10，在北京天坛公园晨跑，空气里全是青草味。\n",
            "TOK: ▁20 24 年 6 月 15 日 早上 8 :10 , 在北京 天 坛 公园 晨 跑 , 空气 里 全 是 青 草 味 。\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O\n",
            "PRED TIME: ['2024年6月15日早上8:10']\n",
            "GOLD TIME: ['2024年6月15日早上8:10']\n",
            "PRED ADDR: []\n",
            "GOLD ADDR: ['北京天坛公园']\n",
            "字符重合率: TIME=1.000, ADDR=0.000\n",
            "--------------------------------------------------------------------\n",
            "[2] Text: 重庆解放碑的洪鼎火锅尝了一次就上瘾，麻辣香气绕嘴不散，就是等位太久了。\n",
            "TOK: ▁ 重庆 解放 碑 的 洪 鼎 火 锅 尝 了 一次 就 上 瘾 , 麻 辣 香 气 绕 嘴 不 散 , 就是 等 位 太 久 了 。\n",
            "LBL: B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O O O O O O O O O O O O O\n",
            "PRED TIME: []\n",
            "GOLD TIME: []\n",
            "PRED ADDR: ['重庆解放碑', '洪鼎火锅']\n",
            "GOLD ADDR: ['重庆解放碑洪鼎火锅']\n",
            "字符重合率: TIME=1.000, ADDR=1.000\n",
            "--------------------------------------------------------------------\n",
            "[3] Text: 【活动提醒】6月18日10:00在杭州西湖音乐喷泉集合拍照，下午换到柳浪闻莺野餐，不要迟到。\n",
            "TOK: ▁【 活动 提醒 】 6 月 18 日 10 :00 在 杭州 西 湖 音乐 喷 泉 集合 拍照 , 下午 换 到 柳 浪 闻 莺 野 餐 , 不要 迟 到 。\n",
            "LBL: O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O\n",
            "PRED TIME: ['6月18日10:00']\n",
            "GOLD TIME: ['6月18日10:00']\n",
            "PRED ADDR: ['杭州西湖音乐喷泉', '柳浪闻莺']\n",
            "GOLD ADDR: ['杭州西湖音乐喷泉', '柳浪闻莺']\n",
            "字符重合率: TIME=1.000, ADDR=1.000\n",
            "[Metrics TIME] token-level: P=0.907, R=0.900, F1=0.902\n",
            "[Metrics ADDR] token-level: P=0.782, R=0.686, F1=0.721\n",
            "[Overlap] 平均字符重合率: TIME=0.975, ADDR=0.788\n",
            "\n",
            "==== Testing on file: 英语.json ====\n",
            "--------------------------------------------------------------------\n",
            "[1] Text: At 7:20 AM on May 5, 2024, I was jogging along the city wall of Xi'an, watching the sunrise over the ancient gates.\n",
            "TOK: ▁At ▁7 :20 ▁AM ▁on ▁May ▁5 , ▁20 24 , ▁I ▁was ▁jo gging ▁along ▁the ▁city ▁wall ▁of ▁Xi ' an , ▁watching ▁the ▁sun rise ▁over ▁the ▁an cient ▁gate s .\n",
            "LBL: O B-TIME I-TIME I-TIME B-TIME B-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O I-ADDRESS O B-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O O\n",
            "PRED TIME: ['7:20 AM', 'on', 'May 5, 2024']\n",
            "GOLD TIME: ['7:20 AM on May 5, 2024']\n",
            "PRED ADDR: [\"Xi'an\"]\n",
            "GOLD ADDR: [\"city wall of Xi'an\", 'ancient gates']\n",
            "字符重合率: TIME=0.909, ADDR=0.161\n",
            "--------------------------------------------------------------------\n",
            "[2] Text: The steamed buns from a small stall in Beijing's Donghuamen Night Market were warm and fluffy.\n",
            "TOK: ▁The ▁stea med ▁bun s ▁from ▁a ▁small ▁ stall ▁in ▁Beijing ' s ▁Dong hua men ▁Night ▁Market ▁were ▁warm ▁and ▁flu ffy .\n",
            "LBL: O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O\n",
            "PRED TIME: []\n",
            "GOLD TIME: []\n",
            "PRED ADDR: [\"Beijing's Donghuamen Night Market\"]\n",
            "GOLD ADDR: [\"Beijing's Donghuamen Night Market\"]\n",
            "字符重合率: TIME=1.000, ADDR=1.000\n",
            "--------------------------------------------------------------------\n",
            "[3] Text: [Reminder] Please gather at the main gate of the Humble Administrator’s Garden in Suzhou at 9:10 AM on April 18.\n",
            "TOK: ▁[ Re min der ] ▁Please ▁ga ther ▁at ▁the ▁main ▁gate ▁of ▁the ▁Hum ble ▁Administrator ’ s ▁Garden ▁in ▁Su zhou ▁at ▁9 :10 ▁AM ▁on ▁April ▁18.\n",
            "LBL: O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O I-TIME I-TIME I-TIME B-TIME I-TIME I-TIME\n",
            "PRED TIME: ['on April 18.']\n",
            "GOLD TIME: ['9:10 AM on April 18']\n",
            "PRED ADDR: ['Humble Administrator’s Garden in Suzhou']\n",
            "GOLD ADDR: ['main gate of the Humble Administrator’s Garden in Suzhou']\n",
            "字符重合率: TIME=0.579, ADDR=0.696\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1456434929.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mgold_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time_tokens\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mgold_addr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"location_tokens\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mtoks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;31m# Gold tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1456434929.py\u001b[0m in \u001b[0;36mpredict_entities\u001b[0;34m(text, lang, max_len)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0mpred_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1387\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1388\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    854\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0mlayer_head_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhead_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    608\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0mcache_position\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     ) -> tuple[torch.Tensor]:\n\u001b[0;32m--> 514\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    515\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0mcache_position\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     ) -> tuple[torch.Tensor]:\n\u001b[0;32m--> 441\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    341\u001b[0m             )\n\u001b[1;32m    342\u001b[0m             value_layer = (\n\u001b[0;32m--> 343\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **模型量化与转tflite格式**"
      ],
      "metadata": {
        "id": "3SVz6RKLDD4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **转tflite同时进行动态int8量化**"
      ],
      "metadata": {
        "id": "wUdp4giVEJ58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# pytorch模型文件路径（训练时保存的）\n",
        "saved_model_dir = \"/content/export_xlmr_tf/saved_model\"  # @param {\"type\":\"string\",\"placeholder\":\"请输入模型路径\"}\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant_model = converter.convert()\n",
        "tflite_model_name = 'model_dy_quant.tflite'# @param {\"type\":\"string\",\"placeholder\":\"请输入tflite模型名称\"}\n",
        "# Save the model.\n",
        "# tflite模型保存到左侧文件栏中，需要及时下载\n",
        "with open(tflite_model_name, 'wb') as f:\n",
        "  f.write(tflite_quant_model)"
      ],
      "metadata": {
        "id": "2xpBEESZfDZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **直接转tflite**\n",
        "正式流程中不需要使用，但是如果转tflite同时量化的结果不理想，可以运行改代码对比量化和未量化的模型来排查问题出在转tflite还是量化"
      ],
      "metadata": {
        "id": "6MnyFRRdfUZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "saved_model_dir = r\"/content/export_xlmr_tf/saved_model\"\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "metadata": {
        "id": "9RhOmrS2coA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2pRFhWf-RrQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 系统优化模型代码：如何得到最佳模型？"
      ],
      "metadata": {
        "id": "C_lQzLcwRiIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 在确定了每种语言五百条数据时的模型性能已经普遍达标的情况下：\n",
        "产生了一个重要的问题——\n",
        "怎么样才能从海量模板和词汇中挑选出那些真正有价值的生成数据？\n",
        "\n",
        "在一次同样参数数据设置下的不同模型测量性能出现较大区分后，发现了关键问题：\n",
        "可以通过反复训练小模型并测量不同语言上的性能差异来得到所谓的最优训练集：\n",
        "当出现某小语种上的性能显著高于之前版本模型时，将该语种的训练集保存到本地并作为\n",
        "current_best训练集\n",
        "这样从10000条模板和20000条实体间的组合中就能筛选出最优的训练集。\n",
        "不断迭代整个过程，训练就会彻底成功。\n",
        "将这整个流程组合在一起，就成了迭代得到当前最好模型的“系统优化代码”。"
      ],
      "metadata": {
        "id": "m03Z4MjNRtDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "这其实就是在建立一个强化学习系统的雏形：有了迭代目标（最优训练集），有评价指标（F1），有模型更新的简单策略（mean值差异>0.01），有充分随机的向量探索空间（训练模板+实体池=训练集）"
      ],
      "metadata": {
        "id": "5dTgKVi4x4we"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   策略更新1：软更新\n",
        "*   是在上一轮没有任何语言发生替换的情况下，为了防止模型停滞，针对已有的训练集做有约束范围的探索性小幅调整\n",
        "*   保留原训练集的 70%，引入新合成的 30% 样本。\n",
        "*   或者根据样本难度分数（loss高的样本）来替换掉最容易的样本。\n",
        "*   这种“软更新”在RL中相当于**经验回放(Experience Replay)**中加权采样——既保留有价值的历史经验，也引入新探索。\n"
      ],
      "metadata": {
        "id": "CGYbeZwUzF4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 安装依赖 =====\n",
        "!pip install -q evaluate seqeval plotly\n",
        "\n",
        "# ===== Step0: 全局设置 =====\n",
        "from google.colab import drive\n",
        "import os, json\n",
        "import shutil\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "drive.mount('/content/drive')\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter\n",
        "import random\n",
        "TOKENIZER_NAME = \"xlm-roberta-base\"\n",
        "\n",
        "# 全局复用 tokenizer（初始为预训练模型版本）\n",
        "GLOBAL_TOKENIZER = AutoTokenizer.from_pretrained(TOKENIZER_NAME, use_fast=True)\n",
        "# 全局路径\n",
        "TRAIN_DATA_DIR = \"/content/drive/MyDrive/NER/train_datasets_AC\"\n",
        "BEST_SAVE_DIR = \"/content/drive/MyDrive/NER/current_best\"\n",
        "BEST_RECORD_FILE = os.path.join(BEST_SAVE_DIR, \"best_scores.json\")\n",
        "MODEL_OUTPUT_DIR = \"/content/ner_xlmr_out\"\n",
        "TEST_DATA_ROOT = \"/content/drive/MyDrive/NER/datasets\"  # JSON测试集目录\n",
        "STOPWORDS_DIR = \"/content/drive/MyDrive/NER/stopwords\"\n",
        "\n",
        "# 全局参数\n",
        "ITERATIONS = 3 #@param\n",
        "IMPROVE_THRESHOLD = 0.0001 # @param\n",
        "keep_ratios=0.95 # @param 保留比例\n",
        "last_replace_happened = False  # 初始化为 False,直接开始寻找更优训练集\n",
        "ENABLE_SOFT_UPDATE = True # 软更新可拔插参数 True 时启用软更新限制探索范围\n",
        "# 全局标签定义\n",
        "LABELS = [\"O\", \"B-ADDRESS\", \"I-ADDRESS\", \"B-TIME\", \"I-TIME\"]\n",
        "\n",
        "# 语言列表\n",
        "LANGS = [\"简体中文\", \"繁体中文\", \"英语\", \"丹麦语\", \"俄语\", \"土耳其语\",\n",
        "         \"德语\", \"意大利语\", \"日语\", \"法语\", \"瑞典语\",\n",
        "         \"荷兰语\", \"葡萄牙语\", \"西班牙语\", \"韩语\"]\n",
        "\n",
        "os.makedirs(TRAIN_DATA_DIR, exist_ok=True)\n",
        "os.makedirs(BEST_SAVE_DIR, exist_ok=True)\n",
        "N_PER_LANG=500#@param\n",
        "\n",
        "########################################\n",
        "# === 工具函数：特征提取 ===\n",
        "########################################\n",
        "\n",
        "def compute_sample_embedding(model, tokenizer, text):\n",
        "    \"\"\"生成样本的 CLS 向量嵌入\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    cls_embed = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # [CLS] 向量\n",
        "    return cls_embed[0]  # shape: (hidden_dim,)\n",
        "\n",
        "\n",
        "def compute_confidence_map(model, tokenizer, dataset):\n",
        "    \"\"\"计算每个样本的平均预测置信度映射: sample_id -> confidence\"\"\"\n",
        "    conf_map = {}\n",
        "    for sample in dataset:\n",
        "        inputs = tokenizer(sample['text'], return_tensors=\"pt\", truncation=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        logits = outputs.logits  # (1, seq_len, num_labels)\n",
        "        probs = torch.softmax(logits, dim=-1).cpu().numpy()  # (1, L, C)\n",
        "        max_probs = probs.max(axis=-1).mean()  # 平均最大概率\n",
        "        conf_map[sample['id']] = float(max_probs)\n",
        "    return conf_map\n",
        "\n",
        "\n",
        "def compute_template_frequency(dataset):\n",
        "    \"\"\"统计模板使用频率\"\"\"\n",
        "    template_ids = [s['template_id'] for s in dataset]\n",
        "    freq = Counter(template_ids)\n",
        "    return freq\n",
        "\n",
        "\n",
        "def compute_validation_embeddings(model, tokenizer, val_dataset):\n",
        "    \"\"\"计算验证集所有样本的CLS嵌入\"\"\"\n",
        "    all_embeds = []\n",
        "    for sample in val_dataset:\n",
        "        emb = compute_sample_embedding(model, tokenizer, sample['text'])\n",
        "        all_embeds.append(emb)\n",
        "    return np.array(all_embeds)  # shape: (N_val, hidden_dim)\n",
        "\n",
        "def latest_checkpoint(base_dir: str) -> str:\n",
        "    if os.path.basename(base_dir).startswith(\"checkpoint-\") and os.path.isdir(base_dir):\n",
        "        return base_dir\n",
        "    cks = [d for d in os.listdir(base_dir) if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(base_dir, d))]\n",
        "    if not cks:\n",
        "        return base_dir\n",
        "    cks.sort(key=lambda x: int(x.split(\"-\")[-1]))\n",
        "    return os.path.join(base_dir, cks[-1])\n",
        "########################################\n",
        "# === 多维评分软更新 ===\n",
        "########################################\n",
        "\n",
        "def compute_confidence_change(sample, prev_conf_map, curr_conf_map):\n",
        "    prev_conf = prev_conf_map.get(sample['id'], 0.0)\n",
        "    curr_conf = curr_conf_map.get(sample['id'], 0.0)\n",
        "    return abs(curr_conf - prev_conf)\n",
        "\n",
        "\n",
        "def compute_validation_similarity(sample_embedding, val_embeddings):\n",
        "    sims = cosine_similarity(sample_embedding.reshape(1, -1), val_embeddings)\n",
        "    return sims.mean()\n",
        "\n",
        "\n",
        "def soft_update_with_multiscore(old_dataset, new_dataset, keep_ratio,\n",
        "                                 prev_conf_map, curr_conf_map,\n",
        "                                 template_frequency, val_embeddings,\n",
        "                                 alpha, beta, gamma, delta):\n",
        "\n",
        "    # loss 中位数用作难度计算基准\n",
        "    losses = np.array([s['loss'] for s in old_dataset])\n",
        "    median_loss = np.median(losses)\n",
        "\n",
        "    scored_samples = []\n",
        "    for sample in old_dataset:\n",
        "        # ① 置信度变化分\n",
        "        conf_score = compute_confidence_change(sample, prev_conf_map, curr_conf_map)\n",
        "\n",
        "        # ② 验证集相似度分\n",
        "        val_score = compute_validation_similarity(sample['embedding'], val_embeddings)\n",
        "\n",
        "        # ③ 模板多样性分\n",
        "        template_id = sample['template_id']\n",
        "        template_score = 1.0 / (template_frequency.get(template_id, 0) + 1)\n",
        "\n",
        "        # ④ 难度分\n",
        "        loss = sample['loss']\n",
        "        diff_score = 1.0 - abs(loss - median_loss) / median_loss\n",
        "\n",
        "        # 综合评分\n",
        "        total_score = (alpha * conf_score +\n",
        "                       beta * val_score +\n",
        "                       gamma * template_score +\n",
        "                       delta * diff_score)\n",
        "        scored_samples.append((total_score, sample))\n",
        "\n",
        "    # 按总分排序\n",
        "    scored_samples.sort(key=lambda x: x[0], reverse=True)\n",
        "    keep_count = int(len(scored_samples) * keep_ratio)\n",
        "\n",
        "    kept_samples = [s[1] for s in scored_samples[:keep_count]]\n",
        "    updated_dataset = kept_samples + new_dataset\n",
        "\n",
        "    print(f\"[Soft Update] Kept {keep_count} high-score samples, added {len(new_dataset)} new samples\")\n",
        "    return updated_dataset\n",
        "# ===== Step1: 优先用最佳集，否则合成数据 =====\n",
        "def generate_training_data_for_lang(lang, count):\n",
        "    \"\"\"\n",
        "    生成指定语言的训练样本，返回 tokens_list, labels_list\n",
        "    不写文件 — 由调用方决定是否保存\n",
        "    \"\"\"\n",
        "    import os, re, json, random, unicodedata\n",
        "    from transformers import AutoTokenizer\n",
        "\n",
        "    per_00 = 0.10\n",
        "    per_11 = 0.30\n",
        "    NEG_ADDR_PROB = 0.2\n",
        "    NEG_TIME_PROB = 0.2\n",
        "    COMBINE_TPL_ROOT = \"/content/drive/MyDrive/NER/combine_templates\"\n",
        "    REAL_ADDR_ROOT = \"/content/drive/MyDrive/NER/real_address\"\n",
        "    REAL_TIME_ROOT = \"/content/drive/MyDrive/NER/real_time\"\n",
        "    NON_ADDR_ROOT  = \"/content/drive/MyDrive/NER/non_address\"\n",
        "    NON_TIME_ROOT  = \"/content/drive/MyDrive/NER/non_time\"\n",
        "    TOKENIZER_NAME = \"xlm-roberta-base\"\n",
        "\n",
        "    tokenizer = GLOBAL_TOKENIZER\n",
        "\n",
        "    def read_lines(path):\n",
        "        if not os.path.exists(path):\n",
        "            return []\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return [ln.strip() for ln in f if ln.strip()]\n",
        "\n",
        "    def read_jsonl_templates(path):\n",
        "        items = []\n",
        "        if not os.path.exists(path):\n",
        "            return items\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for ln in f:\n",
        "                try:\n",
        "                    obj = json.loads(ln.strip())\n",
        "                    t = obj.get(\"template\")\n",
        "                    if isinstance(t, str) and t.strip():\n",
        "                        items.append(\" \".join(t.split()))\n",
        "                except:\n",
        "                    continue\n",
        "        return items\n",
        "\n",
        "    def load_templates_for_lang(lang):\n",
        "        pos_11_tpls, pos_other_tpls, neg_sent_tpls = [], [], []\n",
        "        filename = f\"{lang}_combine_templates.jsonl\"\n",
        "        for name in os.listdir(COMBINE_TPL_ROOT):\n",
        "            if not name.startswith(\"combine_templates_\"):\n",
        "                continue\n",
        "            subdir = os.path.join(COMBINE_TPL_ROOT, name)\n",
        "            fpath = os.path.join(subdir, filename)\n",
        "            items = read_jsonl_templates(fpath)\n",
        "            if not items:\n",
        "                continue\n",
        "            slot = name[len(\"combine_templates_\"):]\n",
        "            if slot.isdigit():\n",
        "                if int(slot) == 0:\n",
        "                    neg_sent_tpls.extend(items)\n",
        "                elif int(slot) == 11:\n",
        "                    pos_11_tpls.extend(items)\n",
        "                else:\n",
        "                    pos_other_tpls.extend(items)\n",
        "        return {\"pos_11\": pos_11_tpls, \"pos_other\": pos_other_tpls, \"neg_sent\": neg_sent_tpls}\n",
        "\n",
        "    def apply_noise(text, spans):\n",
        "        def toggle_case(s):\n",
        "            return \"\".join(c.upper() if \"a\"<=c<=\"z\" else c.lower() if \"A\"<=c<=\"Z\" else c for c in s)\n",
        "        def to_fullwidth(c):\n",
        "            return chr(ord(c) + 0xFEE0) if 33 <= ord(c) <= 126 else \"\\u3000\" if c==\" \" else c\n",
        "        def to_halfwidth(c):\n",
        "            return \" \" if c==\"\\u3000\" else chr(ord(c) - 0xFEE0) if 65281 <= ord(c) <= 65374 else c\n",
        "        def fullwidth_halfwidth_flip(s):\n",
        "            out = []\n",
        "            for ch in s:\n",
        "                if unicodedata.east_asian_width(ch) in (\"F\",\"W\") and random.random()<0.5:\n",
        "                    out.append(to_halfwidth(ch))\n",
        "                elif random.random() < 0.2:\n",
        "                    out.append(to_fullwidth(ch))\n",
        "                else:\n",
        "                    out.append(ch)\n",
        "            return \"\".join(out)\n",
        "        def replace_separators(s):\n",
        "            trans = {\"-\": random.choice([\"—\",\"-\",\"–\"]), \"/\": random.choice([\"/\",\"／\",\"∕\"]),\n",
        "                     \".\": random.choice([\".\",\"·\",\"．\"]), \":\": random.choice([\":\",\"：\"])}\n",
        "            return \"\".join(trans.get(ch, ch) for ch in s)\n",
        "\n",
        "        if random.random() < 0.15:\n",
        "            text = toggle_case(text)\n",
        "        if random.random() < 0.10:\n",
        "            text = fullwidth_halfwidth_flip(text)\n",
        "        if random.random() < 0.10:\n",
        "            text = replace_separators(text)\n",
        "        return text, spans\n",
        "\n",
        "    def fill_and_align_to_bio(tpl, lang, addrs_map, times_map, non_addr_map, non_time_map):\n",
        "        parts, entities = [], []\n",
        "        last_pos = 0\n",
        "        for m in re.finditer(r'\\{address\\}|\\{time\\}', tpl):\n",
        "            parts.append(tpl[last_pos:m.start()])\n",
        "            if m.group() == \"{address}\":\n",
        "                if random.random() < NEG_ADDR_PROB:\n",
        "                    rep = random.choice(non_addr_map.get(lang, [\"\"]))\n",
        "                else:\n",
        "                    rep = random.choice(addrs_map.get(lang, [\"Main Street\"]))\n",
        "                    start_char = sum(len(p) for p in parts)\n",
        "                    entities.append((start_char, start_char+len(rep), \"ADDRESS\"))\n",
        "                parts.append(rep)\n",
        "            elif m.group() == \"{time}\":\n",
        "                if random.random() < NEG_TIME_PROB:\n",
        "                    rep = random.choice(non_time_map.get(lang, [\"\"]))\n",
        "                else:\n",
        "                    rep = random.choice(times_map.get(lang, [\"2024-07-01\"]))\n",
        "                    start_char = sum(len(p) for p in parts)\n",
        "                    entities.append((start_char, start_char+len(rep), \"TIME\"))\n",
        "                parts.append(rep)\n",
        "            last_pos = m.end()\n",
        "        parts.append(tpl[last_pos:])\n",
        "        text = \"\".join(parts)\n",
        "        text, new_spans = apply_noise(text, [(s,e) for s,e,_ in entities])\n",
        "        for i, (_, _, typ) in enumerate(entities):\n",
        "            entities[i] = (new_spans[i][0], new_spans[i][1], typ)\n",
        "\n",
        "        enc = tokenizer(text, add_special_tokens=False, return_offsets_mapping=True)\n",
        "        tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"])\n",
        "        offsets = enc[\"offset_mapping\"]\n",
        "        labels = [\"O\"] * len(tokens)\n",
        "        for start_char, end_char, ent_type in entities:\n",
        "            first_flag = True\n",
        "            for i, (tok_s, tok_e) in enumerate(offsets):\n",
        "                if tok_e <= start_char or tok_s >= end_char:\n",
        "                    continue\n",
        "                labels[i] = f\"B-{ent_type}\" if first_flag else f\"I-{ent_type}\"\n",
        "                first_flag = False\n",
        "        return tokens, labels\n",
        "\n",
        "    # 加载资源\n",
        "    addrs_map    = {lang: read_lines(os.path.join(REAL_ADDR_ROOT,f\"{lang}.txt\"))}\n",
        "    times_map    = {lang: read_lines(os.path.join(REAL_TIME_ROOT,f\"{lang}.txt\"))}\n",
        "    non_addr_map = {lang: read_lines(os.path.join(NON_ADDR_ROOT,f\"{lang}.txt\"))}\n",
        "    non_time_map = {lang: read_lines(os.path.join(NON_TIME_ROOT,f\"{lang}.txt\"))}\n",
        "\n",
        "    templates = load_templates_for_lang(lang)\n",
        "    neg_target       = int(count * per_00)\n",
        "    pos_target_total = count - neg_target\n",
        "    pos_11_target    = int(pos_target_total * per_11)\n",
        "    pos_other_target = pos_target_total - pos_11_target\n",
        "    pos_11_tpls  = templates.get(\"pos_11\", [\"{address} {time}\"])\n",
        "    pos_other_tpls = templates.get(\"pos_other\", [\"{address} {time}\"])\n",
        "    neg_tpls    = templates.get(\"neg_sent\", [])\n",
        "\n",
        "    samples_tokens, samples_labels = [], []\n",
        "    for _ in range(pos_11_target):\n",
        "        toks, lbls = fill_and_align_to_bio(random.choice(pos_11_tpls), lang, addrs_map, times_map, non_addr_map, non_time_map)\n",
        "        samples_tokens.append(toks)\n",
        "        samples_labels.append(lbls)\n",
        "    for _ in range(pos_other_target):\n",
        "        toks, lbls = fill_and_align_to_bio(random.choice(pos_other_tpls), lang, addrs_map, times_map, non_addr_map, non_time_map)\n",
        "        samples_tokens.append(toks)\n",
        "        samples_labels.append(lbls)\n",
        "    for _ in range(neg_target):\n",
        "        if neg_tpls:\n",
        "            tpl = random.choice(neg_tpls)\n",
        "            toks = tpl.split()\n",
        "            lbls = [\"O\"]*len(toks)\n",
        "            samples_tokens.append(toks)\n",
        "            samples_labels.append(lbls)\n",
        "\n",
        "    return samples_tokens, samples_labels\n",
        "def soft_update_best_dataset(keep_ratio=0.6, w_midloss=0.7, w_conf=0.3):\n",
        "    import torch\n",
        "    import numpy as np\n",
        "    from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "    print(f\"[软更新] 保留loss接近中值的样本，比例={keep_ratio}\")\n",
        "\n",
        "    def read_conll(path):\n",
        "        toks_all, labs_all = [], []\n",
        "        toks, labs = [], []\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line=line.strip()\n",
        "                if not line:\n",
        "                    if toks:\n",
        "                        toks_all.append(toks)\n",
        "                        labs_all.append(labs)\n",
        "                        toks,labs=[],[]\n",
        "                    continue\n",
        "                t,y = (line.split(\" \",1)+[\"O\"])[:2]\n",
        "                toks.append(t); labs.append(y)\n",
        "        if toks:\n",
        "            toks_all.append(toks)\n",
        "            labs_all.append(labs)\n",
        "        return toks_all, labs_all\n",
        "\n",
        "    def compute_loss_conf(model, tokenizer, tokens, labels, label2id):\n",
        "        cls_id = tokenizer.cls_token_id\n",
        "        sep_id = tokenizer.sep_token_id\n",
        "        input_ids = [cls_id] + tokenizer.convert_tokens_to_ids(tokens) + [sep_id]\n",
        "        labels_ids = [-100] + [label2id.get(l, 0) for l in labels] + [-100]\n",
        "        attention_mask = [1]*len(input_ids)\n",
        "        inputs = {\n",
        "            \"input_ids\": torch.tensor([input_ids], dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor([attention_mask], dtype=torch.long),\n",
        "            \"labels\": torch.tensor([labels_ids], dtype=torch.long)\n",
        "        }\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.cuda() for k,v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs.loss.item()\n",
        "            probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "            max_probs = probs.max(axis=-1).mean()\n",
        "        return loss, max_probs\n",
        "\n",
        "    ckpt = latest_checkpoint(MODEL_OUTPUT_DIR)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ckpt, use_fast=True)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(ckpt)\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "    model.eval()\n",
        "    label2id = {l: i for i, l in enumerate(LABELS)}\n",
        "\n",
        "    updated_langs = []\n",
        "\n",
        "    for lang in LANGS:\n",
        "        best_file = os.path.join(BEST_SAVE_DIR, f\"{lang}.conll\")\n",
        "        if not os.path.isfile(best_file):\n",
        "            continue\n",
        "\n",
        "        all_tokens, all_labels = read_conll(best_file)\n",
        "        orig_count = len(all_tokens)\n",
        "        if orig_count == 0:\n",
        "            continue\n",
        "\n",
        "        losses, confs = [], []\n",
        "        for tks, lbs in zip(all_tokens, all_labels):\n",
        "            loss, conf = compute_loss_conf(model, tokenizer, tks, lbs, label2id)\n",
        "            losses.append(loss)\n",
        "            confs.append(conf)\n",
        "\n",
        "        # 归一化\n",
        "        loss_arr = np.array(losses)\n",
        "        conf_arr = np.array(confs)\n",
        "        loss_min, loss_max = loss_arr.min(), loss_arr.max()\n",
        "        loss_norm = (loss_arr - loss_min) / (loss_max - loss_min + 1e-8)\n",
        "        conf_norm = (conf_arr - conf_arr.min()) / (conf_arr.max() - conf_arr.min() + 1e-8)\n",
        "\n",
        "        # 找中值\n",
        "        median_loss_norm = np.median(loss_norm)\n",
        "\n",
        "        # 分数：loss接近中值 + 置信度低\n",
        "        scores = w_midloss * (1 - np.abs(loss_norm - median_loss_norm)) \\\n",
        "               + w_conf * (1 - conf_norm)\n",
        "\n",
        "        sorted_idx = np.argsort(scores)[::-1]\n",
        "        keep_count = int(len(sorted_idx) * keep_ratio)\n",
        "        keep_idx = set(sorted_idx[:keep_count])\n",
        "\n",
        "        kept_tokens = [all_tokens[i] for i in keep_idx]\n",
        "        kept_labels = [all_labels[i] for i in keep_idx]\n",
        "\n",
        "        new_count = orig_count - keep_count\n",
        "        new_tokens, new_labels = generate_training_data_for_lang(lang, count=new_count)\n",
        "        merged_tokens = kept_tokens + new_tokens\n",
        "        merged_labels = kept_labels + new_labels\n",
        "\n",
        "        train_file = os.path.join(TRAIN_DATA_DIR, f\"train_{lang}.conll\")\n",
        "        with open(train_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            for toks, labs in zip(merged_tokens, merged_labels):\n",
        "                for t, l in zip(toks, labs):\n",
        "                    f.write(f\"{t} {l}\\n\")\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "        updated_langs.append(lang)\n",
        "        print(f\"[软更新] {lang}: 保留{keep_count}条(中值loss优先), 新增{new_count}条 → {train_file}\")\n",
        "\n",
        "    print(f\"[软更新] 完成，更新语言: {', '.join(updated_langs) if updated_langs else '无'}\")\n",
        "def prepare_training_data(iteration, last_replace_happened):\n",
        "    \"\"\"\n",
        "    Step1: 数据准备逻辑（软更新可拔插 + 拔掉时用彻底随机生成）\n",
        "    iteration: 当前迭代轮次 (从1开始)\n",
        "    last_replace_happened: 上一轮是否有最佳集替换（性能提升）\n",
        "    \"\"\"\n",
        "    os.makedirs(TRAIN_DATA_DIR, exist_ok=True)\n",
        "    merge_path = os.path.join(TRAIN_DATA_DIR, \"train_all.conll\")\n",
        "\n",
        "    # 检查最佳集文件是否完整存在\n",
        "    best_files_exist = all(\n",
        "        os.path.isfile(os.path.join(BEST_SAVE_DIR, f\"{lang}.conll\"))\n",
        "        for lang in LANGS\n",
        "    )\n",
        "\n",
        "    # 情况 1：没有最佳集文件\n",
        "    if not best_files_exist:\n",
        "        print(f\"[Step1] 第{iteration}轮：无最佳集文件 → 全量随机生成数据\")\n",
        "        generate_training_data()\n",
        "\n",
        "    # 情况 2：第一轮 + 有最佳集文件\n",
        "    elif iteration == 1:\n",
        "        print(f\"[Step1] 第{iteration}轮：初始化为最佳集（不软更新、不随机生成）\")\n",
        "        for lang in LANGS:\n",
        "            src = os.path.join(BEST_SAVE_DIR, f\"{lang}.conll\")\n",
        "            dst = os.path.join(TRAIN_DATA_DIR, f\"train_{lang}.conll\")\n",
        "            shutil.copyfile(src, dst)\n",
        "\n",
        "    # 情况 3：非第一轮 + 上一轮无替换\n",
        "    elif not last_replace_happened:\n",
        "        if ENABLE_SOFT_UPDATE:\n",
        "            print(f\"[Step1] 第{iteration}轮：上一轮无替换 → 软更新探索\")\n",
        "            soft_update_best_dataset(keep_ratio=keep_ratios,w_midloss=1.0, w_conf=0)\n",
        "        else:\n",
        "            print(f\"[Step1] 第{iteration}轮：上一轮无替换 → 彻底随机生成数据\")\n",
        "            generate_training_data()\n",
        "\n",
        "    # 情况 4：非第一轮 + 上一轮有替换\n",
        "    else:\n",
        "        if ENABLE_SOFT_UPDATE:\n",
        "            print(f\"[Step1] 第{iteration}轮：上一轮有提升 → 软更新探索（不覆盖）\")\n",
        "            soft_update_best_dataset(keep_ratio=keep_ratios,w_midloss=1.0, w_conf=0)\n",
        "        else:\n",
        "            print(f\"[Step1] 第{iteration}轮：上一轮有提升 → 彻底随机生成数据\")\n",
        "            generate_training_data()\n",
        "\n",
        "    # 合并所有训练数据到 train_all.conll\n",
        "    with open(merge_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "        for lang in LANGS:\n",
        "            src = os.path.join(TRAIN_DATA_DIR, f\"train_{lang}.conll\")\n",
        "            if os.path.isfile(src):\n",
        "                with open(src, \"r\", encoding=\"utf-8\") as fin:\n",
        "                    fout.write(fin.read())\n",
        "                    fout.write(\"\\n\")\n",
        "\n",
        "    print(f\"[Step1] 第{iteration}轮：train_all.conll 文件已生成，共包含 {len(LANGS)} 个语言数据\")\n",
        "\n",
        "def generate_training_data():\n",
        "    \"\"\"\n",
        "    生成训练数据（多语言、多标签），输出到 TRAIN_DATA_DIR\n",
        "    \"\"\"\n",
        "    import os, re, json, random, unicodedata\n",
        "    from typing import List, Tuple, Dict\n",
        "    from transformers import AutoTokenizer\n",
        "\n",
        "    per_00 = 0.05#@param\n",
        "    per_11 = 0.25#@param\n",
        "    NEG_ADDR_PROB = 0.2\n",
        "    NEG_TIME_PROB = 0.2\n",
        "    PREVIEW_N = 3\n",
        "    INSERT_CHARS = [\",\", \".\", \"·\", \"—\", \"-\", \"/\", \":\", \"、\", \"・\",\n",
        "                    \"?\", \"？\", \"！\", \"!\", \"，\", \"。\", \"；\", \";\", \" \"]\n",
        "    COMBINE_TPL_ROOT = \"/content/drive/MyDrive/NER/combine_templates\"\n",
        "    REAL_ADDR_ROOT = \"/content/drive/MyDrive/NER/real_address\"\n",
        "    REAL_TIME_ROOT = \"/content/drive/MyDrive/NER/real_time\"\n",
        "    NON_ADDR_ROOT  = \"/content/drive/MyDrive/NER/non_address\"\n",
        "    NON_TIME_ROOT  = \"/content/drive/MyDrive/NER/non_time\"\n",
        "    TOKENIZER_NAME = \"xlm-roberta-base\"\n",
        "\n",
        "    PER_LANG_CONLL = lambda lang: os.path.join(TRAIN_DATA_DIR, f\"train_{lang}.conll\")\n",
        "    MERGED_CONLL = os.path.join(TRAIN_DATA_DIR, \"train_all.conll\")\n",
        "\n",
        "    tokenizer = GLOBAL_TOKENIZER\n",
        "\n",
        "    def read_lines(path: str) -> List[str]:\n",
        "        if not os.path.exists(path): return []\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return [ln.strip() for ln in f if ln.strip()]\n",
        "\n",
        "    def read_jsonl_templates(path: str) -> list:\n",
        "        items = []\n",
        "        if not os.path.exists(path): return items\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for ln in f:\n",
        "                try:\n",
        "                    obj = json.loads(ln.strip())\n",
        "                    t = obj.get(\"template\")\n",
        "                    if isinstance(t, str) and t.strip():\n",
        "                        items.append(\" \".join(t.split()))\n",
        "                except: continue\n",
        "        return items\n",
        "\n",
        "    def load_templates_for_lang(lang: str) -> Dict[str, list]:\n",
        "        pos_11_tpls, pos_other_tpls, neg_sent_tpls = [], [], []\n",
        "        filename = f\"{lang}_combine_templates.jsonl\"\n",
        "        for name in os.listdir(COMBINE_TPL_ROOT):\n",
        "            if not name.startswith(\"combine_templates_\"): continue\n",
        "            subdir = os.path.join(COMBINE_TPL_ROOT, name)\n",
        "            fpath = os.path.join(subdir, filename)\n",
        "            items = read_jsonl_templates(fpath)\n",
        "            if not items: continue\n",
        "            slot = name[len(\"combine_templates_\"):]\n",
        "            if slot.isdigit():\n",
        "                if int(slot) == 0:\n",
        "                    neg_sent_tpls.extend(items)\n",
        "                elif int(slot) == 11:\n",
        "                    pos_11_tpls.extend(items)\n",
        "                else:\n",
        "                    pos_other_tpls.extend(items)\n",
        "        return {\"pos_11\": pos_11_tpls, \"pos_other\": pos_other_tpls, \"neg_sent\": neg_sent_tpls}\n",
        "\n",
        "    def apply_noise(text: str, spans: List[Tuple[int,int]]) -> Tuple[str, List[Tuple[int,int]]]:\n",
        "        def toggle_case(s: str):\n",
        "            return \"\".join(c.upper() if \"a\"<=c<=\"z\" else c.lower() if \"A\"<=c<=\"Z\" else c for c in s)\n",
        "        def to_fullwidth(c: str):\n",
        "            return chr(ord(c) + 0xFEE0) if 33 <= ord(c) <= 126 else \"\\u3000\" if c==\" \" else c\n",
        "        def to_halfwidth(c: str):\n",
        "            return \" \" if c==\"\\u3000\" else chr(ord(c) - 0xFEE0) if 65281 <= ord(c) <= 65374 else c\n",
        "        def fullwidth_halfwidth_flip(s: str):\n",
        "            out=[]\n",
        "            for ch in s:\n",
        "                if unicodedata.east_asian_width(ch) in (\"F\",\"W\") and random.random()<0.5: out.append(to_halfwidth(ch))\n",
        "                elif random.random() < 0.2: out.append(to_fullwidth(ch))\n",
        "                else: out.append(ch)\n",
        "            return \"\".join(out)\n",
        "        def replace_separators(s: str):\n",
        "            trans = {\"-\": random.choice([\"—\",\"-\",\"–\"]), \"/\": random.choice([\"/\",\"／\",\"∕\"]),\n",
        "                     \".\": random.choice([\".\",\"·\",\"．\"]), \":\": random.choice([\":\",\"：\"])}\n",
        "            return \"\".join(trans.get(ch, ch) for ch in s)\n",
        "\n",
        "        if random.random() < 0.15: text = toggle_case(text)\n",
        "        if random.random() < 0.10: text = fullwidth_halfwidth_flip(text)\n",
        "        if random.random() < 0.10: text = replace_separators(text)\n",
        "        return text, spans\n",
        "\n",
        "    def fill_and_align_to_bio(tpl, lang, addrs_map, times_map, non_addr_map, non_time_map):\n",
        "        parts=[]; entities=[]; last_pos=0\n",
        "        for m in re.finditer(r'\\{address\\}|\\{time\\}', tpl):\n",
        "            parts.append(tpl[last_pos:m.start()])\n",
        "            if m.group() == \"{address}\":\n",
        "                if random.random() < NEG_ADDR_PROB: rep = random.choice(non_addr_map.get(lang, [\"\"]))\n",
        "                else:\n",
        "                    rep = random.choice(addrs_map.get(lang, [\"Main Street\"]))\n",
        "                    start_char = sum(len(p) for p in parts)\n",
        "                    entities.append((start_char, start_char+len(rep), \"ADDRESS\"))\n",
        "                parts.append(rep)\n",
        "            elif m.group() == \"{time}\":\n",
        "                if random.random() < NEG_TIME_PROB: rep = random.choice(non_time_map.get(lang, [\"\"]))\n",
        "                else:\n",
        "                    rep = random.choice(times_map.get(lang, [\"2024-07-01\"]))\n",
        "                    start_char = sum(len(p) for p in parts)\n",
        "                    entities.append((start_char, start_char+len(rep), \"TIME\"))\n",
        "                parts.append(rep)\n",
        "            last_pos = m.end()\n",
        "        parts.append(tpl[last_pos:])\n",
        "        text = \"\".join(parts)\n",
        "        text, new_spans = apply_noise(text, [(s,e) for s, e, _ in entities])\n",
        "        for i, (_, _, typ) in enumerate(entities):\n",
        "            entities[i] = (new_spans[i][0], new_spans[i][1], typ)\n",
        "\n",
        "        enc = tokenizer(text, add_special_tokens=False, return_offsets_mapping=True)\n",
        "        tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"])\n",
        "        offsets = enc[\"offset_mapping\"]\n",
        "        labels = [\"O\"] * len(tokens)\n",
        "        for start_char, end_char, ent_type in entities:\n",
        "            first_flag = True\n",
        "            for i, (tok_s, tok_e) in enumerate(offsets):\n",
        "                if tok_e <= start_char or tok_s >= end_char: continue\n",
        "                labels[i] = f\"B-{ent_type}\" if first_flag else f\"I-{ent_type}\"\n",
        "                first_flag = False\n",
        "        return tokens, labels\n",
        "\n",
        "    addrs_map={lang: read_lines(os.path.join(REAL_ADDR_ROOT,f\"{lang}.txt\")) for lang in LANGS}\n",
        "    times_map={lang: read_lines(os.path.join(REAL_TIME_ROOT,f\"{lang}.txt\")) for lang in LANGS}\n",
        "    non_addr_map={lang: read_lines(os.path.join(NON_ADDR_ROOT,f\"{lang}.txt\")) for lang in LANGS}\n",
        "    non_time_map={lang: read_lines(os.path.join(NON_TIME_ROOT,f\"{lang}.txt\")) for lang in LANGS}\n",
        "\n",
        "    merged=[]\n",
        "    for lang in LANGS:\n",
        "        templates = load_templates_for_lang(lang)\n",
        "        total_target = N_PER_LANG\n",
        "        neg_target = int(total_target * per_00)\n",
        "        pos_target_total = total_target - neg_target\n",
        "        pos_11_target = int(pos_target_total * per_11)\n",
        "        pos_other_target = pos_target_total - pos_11_target\n",
        "        pos_11_tpls = templates.get(\"pos_11\", [\"{address} {time}\"])\n",
        "        pos_other_tpls = templates.get(\"pos_other\", [\"{address} {time}\"])\n",
        "        neg_tpls = templates.get(\"neg_sent\", [])\n",
        "\n",
        "        samples=[]\n",
        "        for _ in range(pos_11_target):\n",
        "            toks, lbls = fill_and_align_to_bio(random.choice(pos_11_tpls), lang, addrs_map, times_map, non_addr_map, non_time_map)\n",
        "            samples.append((toks, lbls))\n",
        "        for _ in range(pos_other_target):\n",
        "            toks, lbls = fill_and_align_to_bio(random.choice(pos_other_tpls), lang, addrs_map, times_map, non_addr_map, non_time_map)\n",
        "            samples.append((toks, lbls))\n",
        "        for _ in range(neg_target):\n",
        "            if neg_tpls:\n",
        "                tpl = random.choice(neg_tpls)\n",
        "                toks = tpl.split(); lbls = [\"O\"]*len(toks)\n",
        "                samples.append((toks, lbls))\n",
        "        merged.extend(samples)\n",
        "\n",
        "        with open(PER_LANG_CONLL(lang), \"w\", encoding=\"utf-8\") as f:\n",
        "            for toks, lbls in samples:\n",
        "                for t, y in zip(toks, lbls): f.write(f\"{t} {y}\\n\")\n",
        "                f.write(\"\\n\")\n",
        "        print(f\"[生成]{lang}: {len(samples)} 条样本\")\n",
        "\n",
        "    import random; random.shuffle(merged)\n",
        "    with open(MERGED_CONLL, \"w\", encoding=\"utf-8\") as f:\n",
        "        for toks, lbls in merged:\n",
        "            for t,y in zip(toks,lbls): f.write(f\"{t} {y}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "    print(f\"[生成] 合并集 {len(merged)} 条 -> {MERGED_CONLL}\")\n",
        "\n",
        "\n",
        "# ===== Step2: 训练模型 =====\n",
        "def train_model():\n",
        "    import torch\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, set_seed\n",
        "    from datasets import Dataset\n",
        "    import evaluate\n",
        "    from packaging import version\n",
        "    import transformers\n",
        "\n",
        "    set_seed(42)\n",
        "    LABELS = [\"O\", \"B-ADDRESS\", \"I-ADDRESS\", \"B-TIME\", \"I-TIME\"]\n",
        "    label2id = {l: i for i, l in enumerate(LABELS)}\n",
        "    id2label = {i: l for l, i in label2id.items()}\n",
        "\n",
        "    def read_conll(path):\n",
        "        toks_all, labs_all = [], []\n",
        "        toks, labs = [], []\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line=line.strip()\n",
        "                if not line:\n",
        "                    if toks: toks_all.append(toks); labs_all.append(labs); toks,labs=[],[]\n",
        "                    continue\n",
        "                t,y = (line.split(\" \",1)+[\"O\"])[:2]\n",
        "                if y not in label2id: y=\"O\"\n",
        "                toks.append(t); labs.append(y)\n",
        "        if toks: toks_all.append(toks); labs_all.append(labs)\n",
        "        return toks_all, labs_all\n",
        "\n",
        "    tokens_all, tags_all = read_conll(os.path.join(TRAIN_DATA_DIR, \"train_all.conll\"))\n",
        "    tr_idx, va_idx = train_test_split(range(len(tokens_all)), test_size=0.1, random_state=42)\n",
        "    train_tokens = [tokens_all[i] for i in tr_idx]; train_tags = [tags_all[i] for i in tr_idx]\n",
        "    val_tokens   = [tokens_all[i] for i in va_idx]; val_tags   = [tags_all[i] for i in va_idx]\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\", use_fast=True)\n",
        "\n",
        "    def build_dataset(tokens_list, tags_list):\n",
        "        CLS_ID = tokenizer.cls_token_id\n",
        "        SEP_ID = tokenizer.sep_token_id\n",
        "        recs=[]\n",
        "        for toks, labs in zip(tokens_list, tags_list):\n",
        "            ids = tokenizer.convert_tokens_to_ids(toks)\n",
        "            input_ids = [CLS_ID] + ids + [SEP_ID]\n",
        "            labels_ids = [-100] + [label2id[l] for l in labs] + [-100]\n",
        "            attn = [1]*len(input_ids)\n",
        "            recs.append({\"input_ids\": input_ids, \"labels\": labels_ids, \"attention_mask\": attn})\n",
        "        return Dataset.from_list(recs)\n",
        "\n",
        "    class CollatorPT:\n",
        "        def __init__(self, pad_id): self.pad_id=pad_id\n",
        "        def __call__(self, batch):\n",
        "            import torch\n",
        "            maxlen = max(len(x[\"input_ids\"]) for x in batch)\n",
        "            ids=[x[\"input_ids\"]+[self.pad_id]*(maxlen-len(x[\"input_ids\"])) for x in batch]\n",
        "            attn=[x[\"attention_mask\"]+[0]*(maxlen-len(x[\"attention_mask\"])) for x in batch]\n",
        "            labs=[x[\"labels\"]+[-100]*(maxlen-len(x[\"labels\"])) for x in batch]\n",
        "            return {\"input_ids\": torch.tensor(ids), \"attention_mask\": torch.tensor(attn), \"labels\": torch.tensor(labs)}\n",
        "\n",
        "    ds_train=build_dataset(train_tokens, train_tags)\n",
        "    ds_val=build_dataset(val_tokens, val_tags)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-base\", num_labels=len(LABELS), id2label=id2label, label2id=label2id)\n",
        "    seqeval = evaluate.load(\"seqeval\")\n",
        "    def compute_metrics(p):\n",
        "        preds = p.predictions.argmax(-1)\n",
        "        labels = p.label_ids\n",
        "        preds_list, labels_list = [], []\n",
        "        for pl, gl in zip(preds, labels):\n",
        "            preds_list.append([id2label[i] for i,lab in zip(pl, gl) if lab != -100])\n",
        "            labels_list.append([id2label[lab] for i,lab in zip(pl, gl) if lab != -100])\n",
        "        res = seqeval.compute(predictions=preds_list, references=labels_list)\n",
        "        return {\"f1\": res[\"overall_f1\"], \"precision\": res[\"overall_precision\"], \"recall\": res[\"overall_recall\"]}\n",
        "\n",
        "    common_args = dict(\n",
        "        output_dir=MODEL_OUTPUT_DIR, num_train_epochs=3,\n",
        "        per_device_train_batch_size=128, per_device_eval_batch_size=128,\n",
        "        learning_rate=2.5e-5, logging_steps=50,\n",
        "        save_strategy=\"epoch\",load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\", greater_is_better=True,\n",
        "        report_to=\"none\", fp16=torch.cuda.is_available(), seed=42\n",
        "    )\n",
        "    if version.parse(transformers.__version__) >= version.parse(\"4.55.0\"):\n",
        "        args = TrainingArguments(**common_args, eval_strategy=\"epoch\")\n",
        "    else:\n",
        "        args = TrainingArguments(**common_args, evaluation_strategy=\"epoch\")\n",
        "\n",
        "    trainer = Trainer(model=model, args=args, train_dataset=ds_train, eval_dataset=ds_val,\n",
        "                      tokenizer=tokenizer, data_collator=CollatorPT(tokenizer.pad_token_id),\n",
        "                      compute_metrics=compute_metrics)\n",
        "    trainer.train()\n",
        "    trainer.save_model(MODEL_OUTPUT_DIR)\n",
        "    tokenizer.save_pretrained(MODEL_OUTPUT_DIR)\n",
        "    print(\"[训练] 模型已保存到:\", MODEL_OUTPUT_DIR)\n",
        "\n",
        "# ===== Step3: 测试、整体平均提升才更新最佳集 =====\n",
        "def test_and_update_best_dataset_per_lang():\n",
        "    import os, json, torch, regex as re, shutil\n",
        "    import plotly.graph_objects as go\n",
        "    from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "    replace_happened_this_round = False\n",
        "\n",
        "    # 历史最佳分数\n",
        "    if os.path.isfile(BEST_RECORD_FILE):\n",
        "        with open(BEST_RECORD_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            best_scores = json.load(f)\n",
        "    else:\n",
        "        best_scores = {\"overall_best_mean\": None, \"per_lang_scores\": {}}\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_OUTPUT_DIR, use_fast=True)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(MODEL_OUTPUT_DIR)\n",
        "    model.eval()\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "    id2label = {i: l for i, l in enumerate(LABELS)}\n",
        "\n",
        "    time_F1_list, addr_F1_list = [], []\n",
        "    per_lang_results = {}\n",
        "\n",
        "    for lang in LANGS:\n",
        "        json_file = os.path.join(TEST_DATA_ROOT, f\"{lang}.json\")\n",
        "        cur_time_f1, cur_addr_f1 = 0.0, 0.0\n",
        "\n",
        "        if os.path.isfile(json_file):\n",
        "            with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            metrics_time_tok, metrics_addr_tok = [], []\n",
        "\n",
        "            for sample in data:\n",
        "                sent = sample.get(\"sentence\")\n",
        "                gold_time = sample.get(\"time_tokens\", [])\n",
        "                gold_addr = sample.get(\"location_tokens\", [])\n",
        "\n",
        "                enc = tokenizer(sent, return_offsets_mapping=True, truncation=True,\n",
        "                                max_length=256, return_tensors=\"pt\")\n",
        "                offsets = enc.pop(\"offset_mapping\")\n",
        "                if torch.cuda.is_available():\n",
        "                    enc = {k: v.cuda() for k, v in enc.items()}\n",
        "                with torch.no_grad():\n",
        "                    logits = model(**enc).logits[0]\n",
        "                pred_ids = logits.argmax(-1).detach().cpu().numpy().tolist()\n",
        "                input_ids = enc[\"input_ids\"][0].detach().cpu().tolist()\n",
        "                offsets = offsets[0].detach().cpu().tolist()\n",
        "                toks = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "                keep = []\n",
        "                for tok, (st, ed) in zip(toks, offsets):\n",
        "                    if tok in (tokenizer.cls_token, tokenizer.sep_token, \"<s>\", \"</s>\", tokenizer.pad_token):\n",
        "                        keep.append(False)\n",
        "                    elif (st == 0 and ed == 0):\n",
        "                        keep.append(False)\n",
        "                    else:\n",
        "                        keep.append(True)\n",
        "                toks = [t for t, k in zip(toks, keep) if k]\n",
        "                labs = [id2label[i] for i, k in zip(pred_ids, keep) if k]\n",
        "\n",
        "                pred_time_toks = [t for t, l in zip(toks, labs) if l.endswith(\"TIME\")]\n",
        "                pred_addr_toks = [t for t, l in zip(toks, labs) if l.endswith(\"ADDRESS\")]\n",
        "\n",
        "                def token_level_metrics(pred_tokens, gold_tokens):\n",
        "                    pred_set, gold_set = set(pred_tokens), set(gold_tokens)\n",
        "                    if not gold_set: return None\n",
        "                    tp = len(pred_set & gold_set)\n",
        "                    precision = tp / len(pred_set) if pred_set else 0.0\n",
        "                    recall = tp / len(gold_set) if gold_set else 0.0\n",
        "                    f1 = 2 * precision * recall / (precision + recall) if (precision+recall) > 0 else 0.0\n",
        "                    return precision, recall, f1\n",
        "\n",
        "                m_time = token_level_metrics(pred_time_toks, sum([tokenizer.tokenize(gt) for gt in gold_time], []))\n",
        "                m_addr = token_level_metrics(pred_addr_toks, sum([tokenizer.tokenize(ga) for ga in gold_addr], []))\n",
        "                if m_time: metrics_time_tok.append(m_time)\n",
        "                if m_addr: metrics_addr_tok.append(m_addr)\n",
        "\n",
        "            def avg_metrics(metrics):\n",
        "                if not metrics: return (0,0,0)\n",
        "                p = sum(m[0] for m in metrics) / len(metrics)\n",
        "                r = sum(m[1] for m in metrics) / len(metrics)\n",
        "                f1 = sum(m[2] for m in metrics) / len(metrics)\n",
        "                return p, r, f1\n",
        "\n",
        "            cur_time_f1 = avg_metrics(metrics_time_tok)[2]\n",
        "            cur_addr_f1 = avg_metrics(metrics_addr_tok)[2]\n",
        "\n",
        "        time_F1_list.append(cur_time_f1)\n",
        "        addr_F1_list.append(cur_addr_f1)\n",
        "        per_lang_results[lang] = {\n",
        "            \"addr_f1\": cur_addr_f1,\n",
        "            \"time_f1\": cur_time_f1,\n",
        "            \"mean_f1\": (cur_time_f1 + cur_addr_f1) / 2\n",
        "        }\n",
        "\n",
        "    # 计算整体平均\n",
        "    overall_mean_f1 = sum([(t + a) / 2 for t, a in zip(time_F1_list, addr_F1_list)]) / len(LANGS)\n",
        "    prev_best_mean = best_scores.get(\"overall_best_mean\")\n",
        "\n",
        "    if prev_best_mean is None or overall_mean_f1 > prev_best_mean + IMPROVE_THRESHOLD:\n",
        "        print(f\"[整体更新] 平均F1提升: {prev_best_mean} → {overall_mean_f1}\")\n",
        "        # 全量替换 current_best\n",
        "        for lang in LANGS:\n",
        "            src_file = os.path.join(TRAIN_DATA_DIR, f\"train_{lang}.conll\")\n",
        "            if os.path.isfile(src_file):\n",
        "                shutil.copyfile(src_file, os.path.join(BEST_SAVE_DIR, f\"{lang}.conll\"))\n",
        "\n",
        "        # 更新记录\n",
        "        best_scores[\"overall_best_mean\"] = overall_mean_f1\n",
        "        best_scores[\"per_lang_scores\"] = per_lang_results\n",
        "        replace_happened_this_round = True\n",
        "    else:\n",
        "        print(f\"[保持] 整体平均F1无提升: {prev_best_mean} → {overall_mean_f1}\")\n",
        "        best_scores[\"per_lang_scores\"] = per_lang_results\n",
        "\n",
        "    with open(BEST_RECORD_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(best_scores, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return replace_happened_this_round\n",
        "# ===== 主迭代流程 =====\n",
        "for it in range(1, ITERATIONS+1):\n",
        "    print(f\"\\n=== 迭代 {it}/{ITERATIONS} ===\")\n",
        "    prepare_training_data(it, last_replace_happened)   # Step1\n",
        "    train_model()\n",
        "    last_replace_happened = test_and_update_best_dataset_per_lang()\n",
        "\n",
        "print(\"[完成] 最优训练集保存在:\", BEST_SAVE_DIR)\n",
        "# 迭代全部完成后可视化每个语言的F1\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "if os.path.isfile(BEST_RECORD_FILE):\n",
        "    with open(BEST_RECORD_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        best_scores = json.load(f)\n",
        "    per_lang = best_scores.get(\"per_lang_scores\", {})\n",
        "\n",
        "    langs = list(per_lang.keys())\n",
        "    addr_f1 = [per_lang[lang][\"addr_f1\"] for lang in langs]\n",
        "    time_f1 = [per_lang[lang][\"time_f1\"] for lang in langs]\n",
        "    mean_f1 = [per_lang[lang][\"mean_f1\"] for lang in langs]\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Bar(x=langs, y=addr_f1, name=\"Address F1\"))\n",
        "    fig.add_trace(go.Bar(x=langs, y=time_f1, name=\"Time F1\"))\n",
        "    fig.add_trace(go.Bar(x=langs, y=mean_f1, name=\"Mean F1\"))\n",
        "    fig.update_layout(title=\"各语言F1表现\", barmode=\"group\", xaxis_title=\"语言\", yaxis_title=\"F1分数\")\n",
        "    fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ae022b14f41d47949de8a341ee8cc763",
            "dbfef43be2b54c1cb17fc7d712b49139",
            "37ef7fab221043428fdce9bdcdc8f06b",
            "679b8f10389044e8b1e9589c64a65929",
            "fe31e34b4907456db80e003e16a92034",
            "17845c9a62a142c9ab71c852ade0bd9e",
            "b7dd1ab5adf3402e96450f8ab154b4e5",
            "4eec372bb2e84a72af86169c041b0205",
            "7d70bd28d08441ffa6669e8b3a096af3",
            "d43fd759cc734a57af728b8be2d65044",
            "daaecafe75c94d7bad0d590d4e7ed280",
            "f1a9d42bc9184adc9f825d27b6165635",
            "a9b3ce24f0084f8399a8be27e68f201c",
            "00cd5736fee2469e8b9b629ce3ebb01c",
            "944995c3da6e4d94b9964f29177309eb",
            "c258f41b12484169bb9779ae47dd2ebd",
            "0cdbf6c97748449ea5c0d45e3131667f",
            "4c47459e17f648a69a38998a3b7f2e87",
            "0d6572a43d28457097c446cef4043727",
            "8d9e38dea76949b688f2b89c7d4d9913",
            "966149eec5074d45846838774df4b353",
            "40888dbfc46043f7854b4c5b68695e38",
            "d5fa212761344a94949d335ae6330de1",
            "d5865940b61448fbbb42ec504f202d0c",
            "a2e639b925b8450a8ee68cf73dda26db",
            "1be77d01a11a4a35b8675f16151d0826",
            "c11ad014a37441d5a434873eb9f374e1",
            "1b606ca89a814a97bb09cd92300bcc64",
            "75024a0e6c28402d993bc9acc4ac824e",
            "69ba41a11be8452b9668b55234774bc6",
            "0272e9abc7df4f9aa19e2e284678fe3f",
            "465f8be96d74493baf10d9a32fa3b9a8",
            "01f507a275e346a9b930c6cefdad3555",
            "beb065e3b5e14e719d0070d184c2ba01",
            "94d5083a1c7e4a049daee091c1761db0",
            "8a9ab6a950174ffa89d07f1048cb6c42",
            "b092c23e4262461282c5daa0f93099a2",
            "f2e8dafdf5874c5e97f25dd0838c1ddd",
            "4e67e1d7f55c476b9657294e8574740b",
            "aacaad60c8c84b4aaf36e1c6163f045d",
            "11c2eaa7aec146c7ba2663703f9dda1a",
            "311a34502e054cf284663cb96a61c1e5",
            "914a49df50f24cddbef0060cb79821a6",
            "63789d87c8914da8acf4a9b56d0e91a8",
            "f06730fc86424629af153c693497c990",
            "f264177f388045bc83d275117fe73afb",
            "9330428cb5bc441d81ca343ef5ce2527",
            "a4e97d83a69d46bf881e0b06cb042f86",
            "d7db71d9be3349ba83e87515412d70f8",
            "4d5239557c954eb9bb4c76d8e9adef9c",
            "60f4f54c932345bbaf727a3a2726a890",
            "cf88f12857d64968909dd93fb7861dee",
            "1cc5c4fcc0234292a7e80a27b5c23a1e",
            "1fbe665b8f094e659affc626027a8d9e",
            "e8d6af4d5be040e3afff2ace2219f6f3",
            "507d41f535aa4d2fb1639c76bea3d83b",
            "163538dbd0d94a9baed654b70850292b",
            "708667775997411bbe80b9bdafee5795",
            "51a67709a78d49f397fd1b9398f43770",
            "f2c6f30ced504786af185540a59ee7e3",
            "bb0d18fa410944bbb3432d5bee8d03ed",
            "ab6efd65657b4585b16545bd7531f3f5",
            "8249abc6a6654f4b9f6dcd20ae3b0a62",
            "a5a4c111a95046e3a6af673329f5d338",
            "3c5779c75ed84616b290834cb59a401d",
            "40f275fc5b214abbb8ee158f025cdac8"
          ]
        },
        "id": "WiVixQuPRr3F",
        "outputId": "f60f64ad-83d5-42e3-9cd4-78a7de5b2e15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae022b14f41d47949de8a341ee8cc763"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1a9d42bc9184adc9f825d27b6165635"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5fa212761344a94949d335ae6330de1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "beb065e3b5e14e719d0070d184c2ba01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 迭代 1/3 ===\n",
            "[Step1] 第1轮：初始化为最佳集（不软更新、不随机生成）\n",
            "[Step1] 第1轮：train_all.conll 文件已生成，共包含 15 个语言数据\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f06730fc86424629af153c693497c990"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "507d41f535aa4d2fb1639c76bea3d83b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2007143435.py:742: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(model=model, args=args, train_dataset=ds_train, eval_dataset=ds_val,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='159' max='159' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [159/159 02:48, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.554300</td>\n",
              "      <td>0.067161</td>\n",
              "      <td>0.902040</td>\n",
              "      <td>0.877095</td>\n",
              "      <td>0.928445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.046900</td>\n",
              "      <td>0.011990</td>\n",
              "      <td>0.979062</td>\n",
              "      <td>0.976471</td>\n",
              "      <td>0.981668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.016200</td>\n",
              "      <td>0.009164</td>\n",
              "      <td>0.987909</td>\n",
              "      <td>0.985294</td>\n",
              "      <td>0.990538</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[训练] 模型已保存到: /content/ner_xlmr_out\n",
            "[保持] 整体平均F1无提升: 0.7215325783548443 → 0.7215325783548443\n",
            "\n",
            "=== 迭代 2/3 ===\n",
            "[Step1] 第2轮：上一轮无替换 → 软更新探索\n",
            "[软更新] 保留loss接近中值的样本，比例=0.95\n",
            "[软更新] 简体中文: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_简体中文.conll\n",
            "[软更新] 繁体中文: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_繁体中文.conll\n",
            "[软更新] 英语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_英语.conll\n",
            "[软更新] 丹麦语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_丹麦语.conll\n",
            "[软更新] 俄语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_俄语.conll\n",
            "[软更新] 土耳其语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_土耳其语.conll\n",
            "[软更新] 德语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_德语.conll\n",
            "[软更新] 意大利语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_意大利语.conll\n",
            "[软更新] 日语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_日语.conll\n",
            "[软更新] 法语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_法语.conll\n",
            "[软更新] 瑞典语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_瑞典语.conll\n",
            "[软更新] 荷兰语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_荷兰语.conll\n",
            "[软更新] 葡萄牙语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_葡萄牙语.conll\n",
            "[软更新] 西班牙语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_西班牙语.conll\n",
            "[软更新] 韩语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_韩语.conll\n",
            "[软更新] 完成，更新语言: 简体中文, 繁体中文, 英语, 丹麦语, 俄语, 土耳其语, 德语, 意大利语, 日语, 法语, 瑞典语, 荷兰语, 葡萄牙语, 西班牙语, 韩语\n",
            "[Step1] 第2轮：train_all.conll 文件已生成，共包含 15 个语言数据\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-2007143435.py:742: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(model=model, args=args, train_dataset=ds_train, eval_dataset=ds_val,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='159' max='159' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [159/159 07:11, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.536800</td>\n",
              "      <td>0.043118</td>\n",
              "      <td>0.939858</td>\n",
              "      <td>0.928363</td>\n",
              "      <td>0.951642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.038300</td>\n",
              "      <td>0.010884</td>\n",
              "      <td>0.984259</td>\n",
              "      <td>0.979314</td>\n",
              "      <td>0.989254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.013800</td>\n",
              "      <td>0.008294</td>\n",
              "      <td>0.991361</td>\n",
              "      <td>0.989298</td>\n",
              "      <td>0.993433</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[训练] 模型已保存到: /content/ner_xlmr_out\n",
            "[保持] 整体平均F1无提升: 0.7215325783548443 → 0.6871634003745675\n",
            "\n",
            "=== 迭代 3/3 ===\n",
            "[Step1] 第3轮：上一轮无替换 → 软更新探索\n",
            "[软更新] 保留loss接近中值的样本，比例=0.95\n",
            "[软更新] 简体中文: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_简体中文.conll\n",
            "[软更新] 繁体中文: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_繁体中文.conll\n",
            "[软更新] 英语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_英语.conll\n",
            "[软更新] 丹麦语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_丹麦语.conll\n",
            "[软更新] 俄语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_俄语.conll\n",
            "[软更新] 土耳其语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_土耳其语.conll\n",
            "[软更新] 德语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_德语.conll\n",
            "[软更新] 意大利语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_意大利语.conll\n",
            "[软更新] 日语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_日语.conll\n",
            "[软更新] 法语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_法语.conll\n",
            "[软更新] 瑞典语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_瑞典语.conll\n",
            "[软更新] 荷兰语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_荷兰语.conll\n",
            "[软更新] 葡萄牙语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_葡萄牙语.conll\n",
            "[软更新] 西班牙语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_西班牙语.conll\n",
            "[软更新] 韩语: 保留475条(中值loss优先), 新增25条 → /content/drive/MyDrive/NER/train_datasets_AC/train_韩语.conll\n",
            "[软更新] 完成，更新语言: 简体中文, 繁体中文, 英语, 丹麦语, 俄语, 土耳其语, 德语, 意大利语, 日语, 法语, 瑞典语, 荷兰语, 葡萄牙语, 西班牙语, 韩语\n",
            "[Step1] 第3轮：train_all.conll 文件已生成，共包含 15 个语言数据\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-2007143435.py:742: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(model=model, args=args, train_dataset=ds_train, eval_dataset=ds_val,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='159' max='159' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [159/159 08:33, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.529300</td>\n",
              "      <td>0.035312</td>\n",
              "      <td>0.933491</td>\n",
              "      <td>0.920163</td>\n",
              "      <td>0.947211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.032400</td>\n",
              "      <td>0.008390</td>\n",
              "      <td>0.982970</td>\n",
              "      <td>0.979167</td>\n",
              "      <td>0.986803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.013700</td>\n",
              "      <td>0.007809</td>\n",
              "      <td>0.988038</td>\n",
              "      <td>0.985092</td>\n",
              "      <td>0.991002</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[训练] 模型已保存到: /content/ner_xlmr_out\n",
            "[保持] 整体平均F1无提升: 0.7215325783548443 → 0.7035207090452592\n",
            "[完成] 最优训练集保存在: /content/drive/MyDrive/NER/current_best\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"9d3f7f63-dc17-4939-927a-34cece2a55d3\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9d3f7f63-dc17-4939-927a-34cece2a55d3\")) {                    Plotly.newPlot(                        \"9d3f7f63-dc17-4939-927a-34cece2a55d3\",                        [{\"name\":\"Address F1\",\"x\":[\"简体中文\",\"繁体中文\",\"英语\",\"丹麦语\",\"俄语\",\"土耳其语\",\"德语\",\"意大利语\",\"日语\",\"法语\",\"瑞典语\",\"荷兰语\",\"葡萄牙语\",\"西班牙语\",\"韩语\"],\"y\":[0.7214361523081458,0.7344079818631873,0.8244274372976444,0.5367306151231519,0.7258467069196792,0.9688093392341759,0.4766385513437371,0.4738811767317719,0.8627909985167758,0.48535785013816923,0.5035673736694145,0.2887573018291973,0.6937716995971029,0.3615886959764692,0.7945085354375606],\"type\":\"bar\"},{\"name\":\"Time F1\",\"x\":[\"简体中文\",\"繁体中文\",\"英语\",\"丹麦语\",\"俄语\",\"土耳其语\",\"德语\",\"意大利语\",\"日语\",\"法语\",\"瑞典语\",\"荷兰语\",\"葡萄牙语\",\"西班牙语\",\"韩语\"],\"y\":[0.9029142299715376,0.9032441284128317,0.9343824941160439,0.7426100464404558,0.7929244848515682,0.5567993053361819,0.8181704872165819,0.8756104572510823,0.8179066512399846,0.8913124471025556,0.6267845340950998,0.8291943989402101,0.6742883979916077,0.5947984749455337,0.6921603174603175],\"type\":\"bar\"},{\"name\":\"Mean F1\",\"x\":[\"简体中文\",\"繁体中文\",\"英语\",\"丹麦语\",\"俄语\",\"土耳其语\",\"德语\",\"意大利语\",\"日语\",\"法语\",\"瑞典语\",\"荷兰语\",\"葡萄牙语\",\"西班牙语\",\"韩语\"],\"y\":[0.8121751911398417,0.8188260551380095,0.8794049657068441,0.6396703307818039,0.7593855958856237,0.7628043222851789,0.6474045192801595,0.6747458169914271,0.8403488248783801,0.6883351486203624,0.5651759538822572,0.5589758503847037,0.6840300487943554,0.47819358546100144,0.743334426448939],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"各语言F1表现\"},\"barmode\":\"group\",\"xaxis\":{\"title\":{\"text\":\"语言\"}},\"yaxis\":{\"title\":{\"text\":\"F1分数\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9d3f7f63-dc17-4939-927a-34cece2a55d3');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **测试模型**"
      ],
      "metadata": {
        "id": "VBRn-o2bbPjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **使用json测试集测试**\n",
        "批量测试，得到准确率指标与错误集\n",
        "\n",
        "有后处理措施，非纯模型输出（比如去掉常见错识别字符等）"
      ],
      "metadata": {
        "id": "lo78jMVwbZjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "数据集格式如下，命名为语言名.txt(中文.txt)：\n",
        "\n",
        "```\n",
        "[\n",
        "  {\n",
        "    \"id\": 1,\n",
        "    \"sentence\": \"请确认我们的见面地点是在天凝镇政府。\",\n",
        "    \"time_tokens\": [],\n",
        "    \"location_tokens\": [\n",
        "      \"天凝镇政府\"\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    \"id\": 2,\n",
        "    \"sentence\": \"你能告诉我东川府铸钱局遗址（新局）的具体方向吗？\",\n",
        "    \"time_tokens\": [],\n",
        "    \"location_tokens\": [\n",
        "      \"东川府铸钱局遗址（新局）\"\n",
        "    ]\n",
        "  }\n",
        "\n",
        "]\n",
        "```\n",
        "输出各语言的测试结果；保存测试结果为txt文件；保存识别错误的部分为json。输出示例：\n",
        "\n",
        "```\n",
        "== Exact-match (entity text equal) ==\n",
        "ADDRESS   P=0.8924  R=0.7582  F1=0.8198  (#tp=2123, #fp=256, #fn=677)\n",
        "\n",
        "== Partial-match (char-overlap>0, greedy) ==\n",
        "ADDRESS   P=0.9996  R=0.8811  F1=0.9366  (#tp=2378, #fp=1, #fn=321)\n",
        "total:700, mismatch:359,acc:48.714285714285715%\n",
        "\n",
        "[Saved] non-exact matched cases -> /content/mismatches_4/意大利语_mismatches.json (count=359)\n",
        "\n",
        "== Address-specific Accuracies ==\n",
        "First-Address Exact Acc   : 0.9343  (654/700)\n",
        "First-Address Partial Acc : 0.9757  (683/700)\n",
        "First vs Any    Exact Acc : 0.9557  (669/700)\n",
        "First vs Any    Partial Acc: 1.0000 (700/700)\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CuZUXhrJrRRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用json数据集测试tflite模型\n",
        "\n",
        "import os, json, numpy as np, tensorflow as tf\n",
        "from typing import List, Tuple, Dict\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import io\n",
        "from contextlib import redirect_stdout\n",
        "\n",
        "# 测试集名字列表（请按实际情况修改，或把测试数据命名为\"语言名.txt\"）\n",
        "LANGUAGES = [\"中文\",\"英语\",\"丹麦语\",\"俄语\",\"土耳其语\",\"德语\",\"意大利语\",\"日语\",\"法语\",\"瑞典语\",\"荷兰语\",\"葡萄牙语\",\"西班牙语\",\"韩语\"]\n",
        "\n",
        "TEST_JSON_DIR  = \"/content/4\"            # 测试集所在目录\n",
        "OUTPUT_TXT_DIR = \"/content/eval_txt_4\"        # 各语言评测结果(txt)输出目录\n",
        "MISMATCH_DIR   = \"/content/mismatches_4\"      # 各语言错误项(mismatches.json)输出目录\n",
        "os.makedirs(OUTPUT_TXT_DIR, exist_ok=True)\n",
        "os.makedirs(MISMATCH_DIR,   exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "# --- Paths (edit as needed) ---\n",
        "TFLITE_PATH     = \"/content/drive/MyDrive/model_dy_quant.tflite\"     # TFLite模型\n",
        "TOKENIZER_DIRS  = [\"/content/drive/MyDrive/checkpoint-1235\", \"xlm-roberta-base\"]  # 优先从checkpoint加载\n",
        "\n",
        "# TEST_JSON_PATH  = \"/content/中文.json\"                            # 测试集JSON\n",
        "# MISMATCH_OUT    = \"/content/mismatches.json\"                          # 未完全匹配样本输出\n",
        "\n",
        "# # 错误统计输出（6个文件）\n",
        "# ERR_TIME_MISSED   = \"/content/errors_time_missed.json\"\n",
        "# ERR_TIME_PARTIAL  = \"/content/errors_time_partial.json\"\n",
        "# ERR_TIME_SPURIOUS = \"/content/errors_time_spurious.json\"\n",
        "# ERR_ADDR_MISSED   = \"/content/errors_address_missed.json\"\n",
        "# ERR_ADDR_PARTIAL  = \"/content/errors_address_partial.json\"\n",
        "# ERR_ADDR_SPURIOUS = \"/content/errors_address_spurious.json\"\n",
        "\n",
        "# --- Labels (fallback) ---\n",
        "LABELS = [\"O\",\"B-TIME\",\"I-TIME\",\"B-ADDRESS\",\"I-ADDRESS\"]\n",
        "ID2LABEL = {i: l for i, l in enumerate(LABELS)}\n",
        "\n",
        "# --- 1) Tokenizer ---\n",
        "from transformers import AutoTokenizer\n",
        "tok = None\n",
        "for p in TOKENIZER_DIRS:\n",
        "    try:\n",
        "        tok = AutoTokenizer.from_pretrained(p, use_fast=True)\n",
        "        # 尝试从 config 里取 id2label（若存在）\n",
        "        cfg_path = os.path.join(p, \"config.json\")\n",
        "        if os.path.isfile(cfg_path):\n",
        "            with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                cfg = json.load(f)\n",
        "            if \"id2label\" in cfg and isinstance(cfg[\"id2label\"], dict):\n",
        "                _id2 = {int(k): v for k, v in cfg[\"id2label\"].items()}\n",
        "                ID2LABEL = {i: _id2[i] for i in sorted(_id2.keys())}\n",
        "        print(f\"[OK] Loaded tokenizer from: {p} | labels: {[ID2LABEL[i] for i in range(len(ID2LABEL))]}\")\n",
        "        break\n",
        "    except Exception:\n",
        "        pass\n",
        "assert tok is not None, \"Tokenizer not found from TOKENIZER_DIRS.\"\n",
        "\n",
        "# --- 2) TFLite Interpreter ---\n",
        "try:\n",
        "    from ai_edge_litert.python.interpreter import Interpreter  # type: ignore\n",
        "    interpreter = Interpreter(model_path=TFLITE_PATH,num_threads=os.cpu_count())\n",
        "    print(f\"[OK] Using ai_edge_litert.Interpreter: {TFLITE_PATH}\")\n",
        "except Exception:\n",
        "    interpreter = tf.lite.Interpreter(model_path=TFLITE_PATH, num_threads=os.cpu_count())\n",
        "    print(f\"[OK] Using tf.lite.Interpreter: {TFLITE_PATH} (deprecated warning is OK)\")\n",
        "\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# --- 3) IO mapping helpers ---\n",
        "def map_io(interp):\n",
        "    inps = interp.get_input_details()\n",
        "    outs = interp.get_output_details()\n",
        "    idx = {\"input_ids\": None, \"attention_mask\": None, \"token_type_ids\": None}\n",
        "    for i, info in enumerate(inps):\n",
        "        name = info[\"name\"].lower()\n",
        "        if \"input_ids\" in name or \"ids\" in name:\n",
        "            if idx[\"input_ids\"] is None:\n",
        "                idx[\"input_ids\"] = i\n",
        "        elif \"attention_mask\" in name or \"mask\" in name:\n",
        "            idx[\"attention_mask\"] = i\n",
        "        elif \"token_type_ids\" in name or \"segment\" in name:\n",
        "            idx[\"token_type_ids\"] = i\n",
        "    if len(inps) == 1 and idx[\"input_ids\"] is None:\n",
        "        idx[\"input_ids\"] = 0\n",
        "    if len(inps) == 2 and idx[\"attention_mask\"] is None:\n",
        "        other = 1 if idx[\"input_ids\"] == 0 else 0\n",
        "        idx[\"attention_mask\"] = other\n",
        "    return inps, outs, idx\n",
        "\n",
        "INPS, OUTS, IDX = map_io(interpreter)\n",
        "print(\"[IO] Inputs:\", [d[\"name\"] for d in INPS])\n",
        "print(\"[IO] Outputs:\", [d[\"name\"] for d in OUTS])\n",
        "print(\"[IO] Mapped index:\", IDX)\n",
        "\n",
        "def get_fixed_len(interp, fallback=128) -> int:\n",
        "    inps_local = interp.get_input_details()\n",
        "    L = None\n",
        "    if IDX[\"input_ids\"] is not None:\n",
        "        shape = inps_local[IDX[\"input_ids\"]][\"shape\"]\n",
        "        if len(shape) == 2 and shape[1] > 0:\n",
        "            L = int(shape[1])\n",
        "    return int(L if L else fallback)\n",
        "\n",
        "FIX_LEN = get_fixed_len(interpreter, fallback=128)\n",
        "print(\"[IO] Model fixed/padded sequence length:\", FIX_LEN)\n",
        "\n",
        "# --- 4) Inference ---\n",
        "def tflite_infer_sentence(sentence: str):\n",
        "    enc = tok(\n",
        "        sentence,\n",
        "        return_tensors=\"np\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=FIX_LEN,\n",
        "        return_offsets_mapping=True,\n",
        "    )\n",
        "    input_ids = enc[\"input_ids\"].astype(np.int32)\n",
        "    attn      = enc[\"attention_mask\"].astype(np.int32)\n",
        "    ttids     = enc.get(\"token_type_ids\", np.zeros_like(input_ids, dtype=np.int32))\n",
        "\n",
        "\n",
        "    if IDX[\"input_ids\"] is not None:\n",
        "        interpreter.set_tensor(INPS[IDX[\"input_ids\"]][\"index\"], input_ids)\n",
        "    if IDX[\"attention_mask\"] is not None:\n",
        "        interpreter.set_tensor(INPS[IDX[\"attention_mask\"]][\"index\"], attn)\n",
        "    if IDX[\"token_type_ids\"] is not None:\n",
        "        interpreter.set_tensor(INPS[IDX[\"token_type_ids\"]][\"index\"], ttids)\n",
        "\n",
        "    interpreter.invoke()\n",
        "    logits = interpreter.get_tensor(OUTS[0][\"index\"])\n",
        "    if logits.ndim == 2:  # [L, C] -> [1, L, C]\n",
        "        logits = logits[None, ...]\n",
        "    return enc, logits\n",
        "\n",
        "# --- 5) Decode logits -> entity char-spans ---\n",
        "\n",
        "# ADDRESS 的前后去除字符\n",
        "\n",
        "TRIM_CHARS_ADDR = set([' ', '\\t', '在', '的', '.', ',', 'の', '，'])\n",
        "\n",
        "def trim_addr_edges(sentence: str, start: int, end: int) -> Tuple[int, int]:\n",
        "    \"\"\"收缩 span：去掉前后空格/在/的/./,（只作用于 ADDRESS）\"\"\"\n",
        "    while start < end and sentence[start] in TRIM_CHARS_ADDR:\n",
        "        start += 1\n",
        "    while end > start and sentence[end - 1] in TRIM_CHARS_ADDR:\n",
        "        end -= 1\n",
        "    return start, end\n",
        "\n",
        "\n",
        "def decode_entities_by_span(sentence: str, enc, logits) -> Dict[str, List[Tuple[int,int,str]]]:\n",
        "    \"\"\"\n",
        "    返回：\n",
        "      {\n",
        "        \"TIME\":    [(start,end,text), ...],\n",
        "        \"ADDRESS\": [(start,end,text), ...]\n",
        "      }\n",
        "    \"\"\"\n",
        "    offsets = enc[\"offset_mapping\"][0].tolist()\n",
        "    pred_ids = logits[0].argmax(-1).tolist()\n",
        "    labels = [ID2LABEL.get(i, \"O\") for i in pred_ids]\n",
        "\n",
        "    entities: Dict[str, List[Tuple[int,int,str]]] = {\"TIME\": [], \"ADDRESS\": []}\n",
        "    cur_type, cur_start, cur_end = None, None, None\n",
        "    last_label = \"O\"\n",
        "\n",
        "    for (s,e), y in zip(offsets, labels):\n",
        "        if (s == e) or (s is None) or (e is None):\n",
        "            if y.startswith(\"B-\") or y == \"O\":\n",
        "                if cur_type is not None:\n",
        "                    text = sentence[cur_start:cur_end]\n",
        "                    if text.strip():\n",
        "                        entities[cur_type].append((cur_start, cur_end, text))\n",
        "                cur_type, cur_start, cur_end = None, None, None\n",
        "            last_label = y\n",
        "            continue\n",
        "\n",
        "        if y.startswith(\"B-\") and y != last_label:\n",
        "            if cur_type is not None:\n",
        "                text = sentence[cur_start:cur_end]\n",
        "                if text.strip():\n",
        "                    entities[cur_type].append((cur_start, cur_end, text))\n",
        "            cur_type = y[2:]\n",
        "            cur_start, cur_end = s, e\n",
        "        elif (y.startswith(\"I-\") and cur_type == y[2:]) or (y.startswith(\"B-\") and y == last_label):\n",
        "            if cur_start is None:\n",
        "                cur_type = y[2:]\n",
        "                cur_start, cur_end = s, e\n",
        "            else:\n",
        "                cur_end = e\n",
        "        else:\n",
        "            if cur_type is not None:\n",
        "                text = sentence[cur_start:cur_end]\n",
        "                if text.strip():\n",
        "                    entities[cur_type].append((cur_start, cur_end, text))\n",
        "            cur_type, cur_start, cur_end = None, None, None\n",
        "\n",
        "        last_label = y\n",
        "\n",
        "    if cur_type is not None:\n",
        "        text = sentence[cur_start:cur_end]\n",
        "        if text.strip():\n",
        "            entities[cur_type].append((cur_start, cur_end, text))\n",
        "\n",
        "    # 对 ADDRESS 的 span 做边界收缩，并同步更新文本（去除\"在\"、\"的\"、\",\"等）\n",
        "    addr_trimmed = []\n",
        "    for (a, b, t) in entities[\"ADDRESS\"]:\n",
        "        a2, b2 = trim_addr_edges(sentence, a, b)\n",
        "        if a2 < b2:\n",
        "            addr_trimmed.append((a2, b2, sentence[a2:b2]))\n",
        "    entities[\"ADDRESS\"] = addr_trimmed\n",
        "\n",
        "    # 统一再过滤一次空串\n",
        "    for k in list(entities.keys()):\n",
        "        entities[k] = [(a, b, t) for (a, b, t) in entities[k] if t.strip() != \"\"]\n",
        "\n",
        "    return entities\n",
        "\n",
        "\n",
        "# --- 6) Metric helpers ---\n",
        "def normalize_text(s: str) -> str:\n",
        "    return \"\".join(s.split())\n",
        "\n",
        "def exact_match_sets(pred_texts: List[str], gold_texts: List[str]) -> Tuple[int,int,int]:\n",
        "    P = [normalize_text(x) for x in pred_texts]\n",
        "    G = [normalize_text(x) for x in gold_texts]\n",
        "    from collections import Counter\n",
        "    cP, cG = Counter(P), Counter(G)\n",
        "    tp = sum(min(cP[k], cG.get(k, 0)) for k in cP)\n",
        "    fp = sum(cP[k] - min(cP[k], cG.get(k, 0)) for k in cP)\n",
        "    fn = sum(cG[k] - min(cG[k], cP.get(k, 0)) for k in cG)\n",
        "    return tp, fp, fn\n",
        "\n",
        "def span_overlap(a: Tuple[int,int], b: Tuple[int,int]) -> int:\n",
        "    s = max(a[0], b[0]); e = min(a[1], b[1])\n",
        "    return max(0, e - s)\n",
        "\n",
        "def partial_match_tp(pred_spans: List[Tuple[int,int]], gold_spans: List[Tuple[int,int]]) -> int:\n",
        "    used = [False]*len(gold_spans)\n",
        "    tp = 0\n",
        "    for p in pred_spans:\n",
        "        for j, g in enumerate(gold_spans):\n",
        "            if not used[j] and span_overlap(p, g) > 0:\n",
        "                used[j] = True\n",
        "                tp += 1\n",
        "                break\n",
        "    return tp\n",
        "\n",
        "def prf(tp:int, fp:int, fn:int) -> Tuple[float,float,float]:\n",
        "    prec = tp / (tp + fp + 1e-9)\n",
        "    rec  = tp / (tp + fn + 1e-9)\n",
        "    f1   = 2*prec*rec / (prec+rec+1e-9) if (prec+rec)>0 else 0.0\n",
        "    return prec, rec, f1\n",
        "\n",
        "def evaluate_one_file(TEST_JSON_PATH: str, MISMATCH_OUT: str) -> str:\n",
        "    # --- 7) Load test set ---\n",
        "    with open(TEST_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        test_data = json.load(f)\n",
        "        print(f\"Loaded {len(test_data)} test records from {TEST_JSON_PATH}\")\n",
        "    assert isinstance(test_data, list), \"Test JSON must be a list of records.\"\n",
        "\n",
        "    # --- 8) Evaluate + error buckets ---\n",
        "    exact_tp = {\"TIME\":0, \"ADDRESS\":0}\n",
        "    exact_fp = {\"TIME\":0, \"ADDRESS\":0}\n",
        "    exact_fn = {\"TIME\":0, \"ADDRESS\":0}\n",
        "\n",
        "    part_tp  = {\"TIME\":0, \"ADDRESS\":0}\n",
        "    part_fp  = {\"TIME\":0, \"ADDRESS\":0}\n",
        "    part_fn  = {\"TIME\":0, \"ADDRESS\":0}\n",
        "\n",
        "    mismatches = []\n",
        "\n",
        "    # --- New: Acc counters for 2 address-specific metrics ---\n",
        "    first_exact_hits = 0\n",
        "    first_partial_hits = 0\n",
        "    first_denom = 0          # 仅统计“gold 至少有一个地址”的句子\n",
        "\n",
        "    first_any_exact_hits = 0\n",
        "    first_any_partial_hits = 0\n",
        "\n",
        "\n",
        "    # 错误桶（按频次）\n",
        "    missed_cnt   = {\"TIME\": Counter(), \"ADDRESS\": Counter()}               # gold 无重叠\n",
        "    spurious_cnt = {\"TIME\": Counter(), \"ADDRESS\": Counter()}               # pred 无重叠\n",
        "    partial_cnt_gold_pred = {\"TIME\": Counter(), \"ADDRESS\": Counter()}      # (gold_text, pred_text) 配对\n",
        "    partial_cnt_gold_only = {\"TIME\": Counter(), \"ADDRESS\": Counter()}      # 只有gold可定位但pred_text为空时的兜底\n",
        "\n",
        "    def find_spans(sentence: str, texts: List[str]) -> List[Tuple[int,int,str]]:\n",
        "        spans = []\n",
        "        cur = 0\n",
        "        for t in texts:\n",
        "            t_ = t.strip()\n",
        "            if not t_:  # 空串不计\n",
        "                continue\n",
        "            pos = sentence.find(t_, cur)\n",
        "            if pos < 0:\n",
        "                # 去空白再找（粗糙，若找到也无法精确映射，忽略）\n",
        "                s_norm = \"\".join(sentence.split())\n",
        "                t_norm = \"\".join(t_.split())\n",
        "                pos_n = s_norm.find(t_norm)\n",
        "                if pos_n >= 0:\n",
        "                    # 无法回映到原文精确位置，跳过\n",
        "                    pass\n",
        "            else:\n",
        "                spans.append((pos, pos+len(t_), t_))\n",
        "                cur = pos + len(t_)\n",
        "        return spans\n",
        "\n",
        "    i = 0\n",
        "    for rec in test_data:\n",
        "        if(i%1000 == 0):\n",
        "          print(i)\n",
        "        i+=1\n",
        "        sid = rec.get(\"id\")\n",
        "        sent = rec.get(\"sentence\", \"\")\n",
        "        gold_time = rec.get(\"time_tokens\", []) or []\n",
        "        gold_addr = rec.get(\"location_tokens\", []) or []\n",
        "\n",
        "        # 推理\n",
        "        enc, logits = tflite_infer_sentence(sent)\n",
        "        pred = decode_entities_by_span(sent, enc, logits)  # {\"TIME\":[(s,e,text)...], \"ADDRESS\":[...]}\n",
        "\n",
        "        # 预测文本\n",
        "        pred_time_texts = [t for (_,_,t) in pred[\"TIME\"]]\n",
        "        pred_addr_texts = [t for (_,_,t) in pred[\"ADDRESS\"]]\n",
        "\n",
        "        # ------ Exact by text ------\n",
        "        tp_t, fp_t, fn_t = exact_match_sets(pred_time_texts, gold_time)\n",
        "        tp_a, fp_a, fn_a = exact_match_sets(pred_addr_texts, gold_addr)\n",
        "        exact_tp[\"TIME\"] += tp_t; exact_fp[\"TIME\"] += fp_t; exact_fn[\"TIME\"] += fn_t\n",
        "        exact_tp[\"ADDRESS\"] += tp_a; exact_fp[\"ADDRESS\"] += fp_a; exact_fn[\"ADDRESS\"] += fn_a\n",
        "\n",
        "        # ------ Partial by span-overlap ------\n",
        "        gold_time_spans = find_spans(sent, gold_time)\n",
        "        gold_addr_spans = find_spans(sent, gold_addr)\n",
        "        pred_time_spans = [(s,e,t) for (s,e,t) in pred[\"TIME\"]]\n",
        "        pred_addr_spans = [(s,e,t) for (s,e,t) in pred[\"ADDRESS\"]]\n",
        "\n",
        "        # 首地址完全/部分匹配的准确率 ---\n",
        "        # 仅在 gold 至少有一个地址时计入分母\n",
        "        if len(gold_addr) > 0:\n",
        "            first_denom += 1\n",
        "            # exact：文本完全一致（去空白对齐）\n",
        "            if len(pred_addr_texts) > 0:\n",
        "                if normalize_text(pred_addr_texts[0]) == normalize_text(gold_addr[0]):\n",
        "                    first_exact_hits += 1\n",
        "\n",
        "            # partial：首地址的 span 有任意重叠（包含 exact 情况）\n",
        "            if len(pred_addr_spans) > 0 and len(gold_addr_spans) > 0:\n",
        "                p0 = (pred_addr_spans[0][0], pred_addr_spans[0][1])\n",
        "                g0 = (gold_addr_spans[0][0], gold_addr_spans[0][1])\n",
        "                if span_overlap(p0, g0) > 0:\n",
        "                    first_partial_hits += 1\n",
        "\n",
        "        # 预测首地址 vs 任意 gold 地址（exact / partial）\n",
        "        if len(gold_addr) > 0:\n",
        "          # 分母沿用 first_denom，不需要单独 any_denom\n",
        "          gold_set_norm = set(map(normalize_text, gold_addr))\n",
        "\n",
        "          # exact：预测首地址文本是否等于任一 gold 文本（去空白）\n",
        "          if len(pred_addr_texts) > 0:\n",
        "              if normalize_text(pred_addr_texts[0]) in gold_set_norm:\n",
        "                  first_any_exact_hits += 1\n",
        "\n",
        "          # partial：预测首地址的 span 是否与任一 gold span 有重叠\n",
        "          if len(pred_addr_spans) > 0 and len(gold_addr_spans) > 0:\n",
        "              ps, pe, _ = pred_addr_spans[0]\n",
        "              has_overlap = any(span_overlap((ps, pe), (gs, ge)) > 0 for (gs, ge, _) in gold_addr_spans)\n",
        "              if has_overlap:\n",
        "                  first_any_partial_hits += 1\n",
        "\n",
        "\n",
        "\n",
        "        # 计算TP，预测集合大小=TP+FP，标注集合大小=TP+FN\n",
        "        t_tp = partial_match_tp([(s,e) for (s,e,_) in pred_time_spans], [(s,e) for (s,e,_) in gold_time_spans])\n",
        "        a_tp = partial_match_tp([(s,e) for (s,e,_) in pred_addr_spans], [(s,e) for (s,e,_) in gold_addr_spans])\n",
        "        part_tp[\"TIME\"] += t_tp\n",
        "        part_tp[\"ADDRESS\"] += a_tp\n",
        "        part_fp[\"TIME\"] += max(0, len(pred_time_spans) - t_tp)\n",
        "        part_fp[\"ADDRESS\"] += max(0, len(pred_addr_spans) - a_tp)\n",
        "        part_fn[\"TIME\"] += max(0, len(gold_time_spans) - t_tp)\n",
        "        part_fn[\"ADDRESS\"] += max(0, len(gold_addr_spans) - a_tp)\n",
        "\n",
        "        # - 错误桶填充 -\n",
        "        # 1) missed: 对于每个 gold，没有与任何 pred 重叠\n",
        "        for (gs, ge, gt) in gold_time_spans:\n",
        "            if all(span_overlap((gs,ge), (ps,pe)) == 0 for (ps,pe,_) in pred_time_spans):\n",
        "                missed_cnt[\"TIME\"][gt] += 1\n",
        "        for (gs, ge, gt) in gold_addr_spans:\n",
        "            if all(span_overlap((gs,ge), (ps,pe)) == 0 for (ps,pe,_) in pred_addr_spans):\n",
        "                missed_cnt[\"ADDRESS\"][gt] += 1\n",
        "\n",
        "        # 2) spurious: 对于每个 pred，没有与任何 gold 重叠\n",
        "        for (ps, pe, pt) in pred_time_spans:\n",
        "            if all(span_overlap((ps,pe), (gs,ge)) == 0 for (gs,ge,_) in gold_time_spans):\n",
        "                spurious_cnt[\"TIME\"][pt] += 1\n",
        "        for (ps, pe, pt) in pred_addr_spans:\n",
        "            if all(span_overlap((ps,pe), (gs,ge)) == 0 for (gs,ge,_) in gold_addr_spans):\n",
        "                spurious_cnt[\"ADDRESS\"][pt] += 1\n",
        "\n",
        "        # 3) partial: gold 与 pred 有重叠但文本不完全一致（记录 (gold_text, pred_text) 对）\n",
        "        #    一个 gold 只取与之重叠的第一个 pred 作为代表（简单贪心）\n",
        "        def collect_partial(gold_spans, pred_spans, bucket_pair, bucket_gold_only):\n",
        "            used_pred = [False]*len(pred_spans)\n",
        "            for (gs,ge,gt) in gold_spans:\n",
        "                matched = False\n",
        "                for j,(ps,pe,pt) in enumerate(pred_spans):\n",
        "                    if not used_pred[j] and span_overlap((gs,ge), (ps,pe)) > 0:\n",
        "                        used_pred[j] = True\n",
        "                        matched = True\n",
        "                        if normalize_text(gt) != normalize_text(pt):\n",
        "                            bucket_pair[(gt, pt)] += 1\n",
        "                        break\n",
        "                if not matched:\n",
        "                    # 已在 missed 统计；这里不重复记\n",
        "                    pass\n",
        "\n",
        "        collect_partial(gold_time_spans, pred_time_spans,\n",
        "                        partial_cnt_gold_pred[\"TIME\"], partial_cnt_gold_only[\"TIME\"])\n",
        "        collect_partial(gold_addr_spans, pred_addr_spans,\n",
        "                        partial_cnt_gold_pred[\"ADDRESS\"], partial_cnt_gold_only[\"ADDRESS\"])\n",
        "\n",
        "        # 记录未“完全匹配”的样本（只要任一类型不完全匹配）\n",
        "        def as_set_norm(arr: List[str]): return set(\"\".join(x.split()) for x in arr)\n",
        "        if (as_set_norm(pred_time_texts) != as_set_norm(gold_time)) or \\\n",
        "          (as_set_norm(pred_addr_texts) != as_set_norm(gold_addr)):\n",
        "            mismatches.append({\n",
        "                \"id\": sid,\n",
        "                \"sentence\": sent,\n",
        "                \"gold\": {\"TIME\": gold_time, \"ADDRESS\": gold_addr},\n",
        "                \"pred\": {\"TIME\": pred_time_texts, \"ADDRESS\": pred_addr_texts},\n",
        "            })\n",
        "\n",
        "    # --- 9) Report ---\n",
        "\n",
        "    def show_metrics(title: str, tp: Dict[str,int], fp: Dict[str,int], fn: Dict[str,int]):\n",
        "        print(\"\\n==\", title, \"==\")\n",
        "        for k in [\"ADDRESS\"]:\n",
        "            if k == \"ALL\":\n",
        "                _tp = sum(tp.values()); _fp = sum(fp.values()); _fn = sum(fn.values())\n",
        "            else:\n",
        "                _tp, _fp, _fn = tp[k], fp[k], fn[k]\n",
        "            P, R, F = prf(_tp, _fp, _fn)\n",
        "            print(f\"{k:8s}  P={P:.4f}  R={R:.4f}  F1={F:.4f}  (#tp={_tp}, #fp={_fp}, #fn={_fn})\")\n",
        "\n",
        "    def _safe_div(a, b):\n",
        "        return a / b if b > 0 else 0.0\n",
        "\n",
        "\n",
        "    show_metrics(\"Exact-match (entity text equal)\", exact_tp, exact_fp, exact_fn)\n",
        "    show_metrics(\"Partial-match (char-overlap>0, greedy)\", part_tp, part_fp, part_fn)\n",
        "\n",
        "    with open(MISMATCH_OUT, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(mismatches, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"total:{len(test_data)}, mismatch:{len(mismatches)},acc:{(len(test_data)-len(mismatches))/len(test_data)*100}%\")\n",
        "    print(f\"\\n[Saved] non-exact matched cases -> {MISMATCH_OUT} (count={len(mismatches)})\")\n",
        "\n",
        "    print(\"\\n== Address-specific Accuracies ==\")\n",
        "    print(f\"First-Address Exact Acc   : {_safe_div(first_exact_hits, first_denom):.4f}  ({first_exact_hits}/{first_denom})\")\n",
        "    print(f\"First-Address Partial Acc : {_safe_div(first_partial_hits, first_denom):.4f}  ({first_partial_hits}/{first_denom})\")\n",
        "    print(f\"First vs Any    Exact Acc : {_safe_div(first_any_exact_hits, first_denom):.4f}  ({first_any_exact_hits}/{first_denom})\")\n",
        "    print(f\"First vs Any    Partial Acc: {_safe_div(first_any_partial_hits, first_denom):.4f} ({first_any_partial_hits}/{first_denom})\")\n",
        "\n",
        "\n",
        "# --- 10) Save six error files (sorted by frequency) ---\n",
        "def counter_to_sorted_list(cnt: Counter, as_pairs=False):\n",
        "    if as_pairs:\n",
        "        # cnt: Counter[(gold, pred)]\n",
        "        items = [ {\"gold\": g, \"pred\": p, \"count\": c}\n",
        "                  for (g,p), c in cnt.most_common() ]\n",
        "    else:\n",
        "        items = [ {\"text\": t, \"count\": c} for t, c in cnt.most_common() ]\n",
        "    return items\n",
        "\n",
        "\n",
        "\n",
        "# TIME\n",
        "# with open(ERR_TIME_MISSED, \"w\", encoding=\"utf-8\") as f:\n",
        "#     json.dump(counter_to_sorted_list(missed_cnt[\"TIME\"]), f, ensure_ascii=False, indent=2)\n",
        "# with open(ERR_TIME_SPURIOUS, \"w\", encoding=\"utf-8\") as f:\n",
        "#     json.dump(counter_to_sorted_list(spurious_cnt[\"TIME\"]), f, ensure_ascii=False, indent=2)\n",
        "# with open(ERR_TIME_PARTIAL, \"w\", encoding=\"utf-8\") as f:\n",
        "#     json.dump(counter_to_sorted_list(partial_cnt_gold_pred[\"TIME\"], as_pairs=True), f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# ADDRESS\n",
        "# # 写入文件\n",
        "# with open(ERR_ADDR_MISSED, \"w\", encoding=\"utf-8\") as f:\n",
        "#     json.dump(counter_to_sorted_list(missed_cnt[\"ADDRESS\"]), f, ensure_ascii=False, indent=2)\n",
        "# with open(ERR_ADDR_SPURIOUS, \"w\", encoding=\"utf-8\") as f:\n",
        "#     json.dump(counter_to_sorted_list(spurious_cnt[\"ADDRESS\"]), f, ensure_ascii=False, indent=2)\n",
        "# with open(ERR_ADDR_PARTIAL, \"w\", encoding=\"utf-8\") as f:\n",
        "#     json.dump(counter_to_sorted_list(partial_cnt_gold_pred[\"ADDRESS\"], as_pairs=True), f, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n",
        "# print(\"\\n[Saved error buckets]\")\n",
        "# print(\"  TIME   missed  ->\", ERR_TIME_MISSED)\n",
        "# print(\"  TIME   partial ->\", ERR_TIME_PARTIAL)\n",
        "# print(\"  TIME   spurious->\", ERR_TIME_SPURIOUS)\n",
        "# print(\"  ADDRESS missed  ->\", ERR_ADDR_MISSED)\n",
        "# print(\"  ADDRESS partial ->\", ERR_ADDR_PARTIAL)\n",
        "# print(\"  ADDRESS spurious->\", ERR_ADDR_SPURIOUS)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for lang in LANGUAGES:\n",
        "        test_json = os.path.join(TEST_JSON_DIR, f\"{lang}.json\")\n",
        "        if not os.path.isfile(test_json):\n",
        "            print(f\"[SKIP] {test_json} 不存在\")\n",
        "            continue\n",
        "\n",
        "        mismatch_out = os.path.join(MISMATCH_DIR, f\"{lang}_mismatches.json\")\n",
        "        txt_out      = os.path.join(OUTPUT_TXT_DIR, f\"{lang}_eval.txt\")\n",
        "\n",
        "        print(f\"\\n===== Evaluating: {lang} ({test_json}) =====\")\n",
        "        # 把本次评测的所有 print 同时写入 txt\n",
        "        with io.StringIO() as buf, redirect_stdout(buf):\n",
        "            _ = evaluate_one_file(test_json, mismatch_out)\n",
        "            text_log = buf.getvalue()\n",
        "\n",
        "        with open(txt_out, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(text_log)\n",
        "\n",
        "        # 同时也把摘要打印到控制台\n",
        "        print(text_log)\n",
        "        print(f\"[Saved] TXT -> {txt_out}\")\n",
        "        print(f\"[Saved] Mismatches -> {mismatch_out}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJ6RpqKYbShH",
        "outputId": "4e265bc5-dfac-41f4-da04-109594d73354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Loaded tokenizer from: /content/drive/MyDrive/checkpoint-1235 | labels: ['O', 'B-TIME', 'I-TIME', 'B-ADDRESS', 'I-ADDRESS']\n",
            "[OK] Using tf.lite.Interpreter: /content/drive/MyDrive/model_dy_quant.tflite (deprecated warning is OK)\n",
            "[IO] Inputs: ['serving_default_attention_mask:0', 'serving_default_input_ids:0']\n",
            "[IO] Outputs: ['StatefulPartitionedCall:0']\n",
            "[IO] Mapped index: {'input_ids': 1, 'attention_mask': 0, 'token_type_ids': None}\n",
            "[IO] Model fixed/padded sequence length: 128\n",
            "\n",
            "===== Evaluating: 中文 (/content/4/中文.json) =====\n",
            "Loaded 700 test records from /content/4/中文.json\n",
            "0\n",
            "\n",
            "== Exact-match (entity text equal) ==\n",
            "ADDRESS   P=0.8489  R=0.7607  F1=0.8024  (#tp=2130, #fp=379, #fn=670)\n",
            "\n",
            "== Partial-match (char-overlap>0, greedy) ==\n",
            "ADDRESS   P=0.9980  R=0.8943  F1=0.9433  (#tp=2504, #fp=5, #fn=296)\n",
            "total:700, mismatch:350,acc:50.0%\n",
            "\n",
            "[Saved] non-exact matched cases -> /content/mismatches_4/中文_mismatches.json (count=350)\n",
            "\n",
            "== Address-specific Accuracies ==\n",
            "First-Address Exact Acc   : 0.9257  (648/700)\n",
            "First-Address Partial Acc : 0.9943  (696/700)\n",
            "First vs Any    Exact Acc : 0.9314  (652/700)\n",
            "First vs Any    Partial Acc: 1.0000 (700/700)\n",
            "\n",
            "[Saved] TXT -> /content/eval_txt_4/中文_eval.txt\n",
            "[Saved] Mismatches -> /content/mismatches_4/中文_mismatches.json\n",
            "\n",
            "===== Evaluating: 英语 (/content/4/英语.json) =====\n",
            "Loaded 700 test records from /content/4/英语.json\n",
            "0\n",
            "\n",
            "== Exact-match (entity text equal) ==\n",
            "ADDRESS   P=0.8186  R=0.6561  F1=0.7284  (#tp=1837, #fp=407, #fn=963)\n",
            "\n",
            "== Partial-match (char-overlap>0, greedy) ==\n",
            "ADDRESS   P=1.0000  R=0.8014  F1=0.8898  (#tp=2244, #fp=0, #fn=556)\n",
            "total:700, mismatch:427,acc:39.0%\n",
            "\n",
            "[Saved] non-exact matched cases -> /content/mismatches_4/英语_mismatches.json (count=427)\n",
            "\n",
            "== Address-specific Accuracies ==\n",
            "First-Address Exact Acc   : 0.9229  (646/700)\n",
            "First-Address Partial Acc : 0.9914  (694/700)\n",
            "First vs Any    Exact Acc : 0.9286  (650/700)\n",
            "First vs Any    Partial Acc: 1.0000 (700/700)\n",
            "\n",
            "[Saved] TXT -> /content/eval_txt_4/英语_eval.txt\n",
            "[Saved] Mismatches -> /content/mismatches_4/英语_mismatches.json\n",
            "\n",
            "===== Evaluating: 丹麦语 (/content/4/丹麦语.json) =====\n",
            "Loaded 700 test records from /content/4/丹麦语.json\n",
            "0\n",
            "\n",
            "== Exact-match (entity text equal) ==\n",
            "ADDRESS   P=0.7812  R=0.6211  F1=0.6920  (#tp=1739, #fp=487, #fn=1061)\n",
            "\n",
            "== Partial-match (char-overlap>0, greedy) ==\n",
            "ADDRESS   P=1.0000  R=0.7950  F1=0.8858  (#tp=2226, #fp=0, #fn=574)\n",
            "total:700, mismatch:524,acc:25.142857142857146%\n",
            "\n",
            "[Saved] non-exact matched cases -> /content/mismatches_4/丹麦语_mismatches.json (count=524)\n",
            "\n",
            "== Address-specific Accuracies ==\n",
            "First-Address Exact Acc   : 0.9243  (647/700)\n",
            "First-Address Partial Acc : 0.9771  (684/700)\n",
            "First vs Any    Exact Acc : 0.9371  (656/700)\n",
            "First vs Any    Partial Acc: 1.0000 (700/700)\n",
            "\n",
            "[Saved] TXT -> /content/eval_txt_4/丹麦语_eval.txt\n",
            "[Saved] Mismatches -> /content/mismatches_4/丹麦语_mismatches.json\n",
            "\n",
            "===== Evaluating: 俄语 (/content/4/俄语.json) =====\n",
            "Loaded 700 test records from /content/4/俄语.json\n",
            "0\n",
            "\n",
            "== Exact-match (entity text equal) ==\n",
            "ADDRESS   P=0.8479  R=0.6968  F1=0.7649  (#tp=1951, #fp=350, #fn=849)\n",
            "\n",
            "== Partial-match (char-overlap>0, greedy) ==\n",
            "ADDRESS   P=1.0000  R=0.8244  F1=0.9038  (#tp=2301, #fp=0, #fn=490)\n",
            "total:700, mismatch:453,acc:35.285714285714285%\n",
            "\n",
            "[Saved] non-exact matched cases -> /content/mismatches_4/俄语_mismatches.json (count=453)\n",
            "\n",
            "== Address-specific Accuracies ==\n",
            "First-Address Exact Acc   : 0.8814  (617/700)\n",
            "First-Address Partial Acc : 0.9600  (672/700)\n",
            "First vs Any    Exact Acc : 0.9086  (636/700)\n",
            "First vs Any    Partial Acc: 1.0000 (700/700)\n",
            "\n",
            "[Saved] TXT -> /content/eval_txt_4/俄语_eval.txt\n",
            "[Saved] Mismatches -> /content/mismatches_4/俄语_mismatches.json\n",
            "\n",
            "===== Evaluating: 土耳其语 (/content/4/土耳其语.json) =====\n",
            "Loaded 700 test records from /content/4/土耳其语.json\n",
            "0\n",
            "\n",
            "== Exact-match (entity text equal) ==\n",
            "ADDRESS   P=0.7996  R=0.6654  F1=0.7263  (#tp=1863, #fp=467, #fn=937)\n",
            "\n",
            "== Partial-match (char-overlap>0, greedy) ==\n",
            "ADDRESS   P=1.0000  R=0.8321  F1=0.9084  (#tp=2330, #fp=0, #fn=470)\n",
            "total:700, mismatch:491,acc:29.85714285714286%\n",
            "\n",
            "[Saved] non-exact matched cases -> /content/mismatches_4/土耳其语_mismatches.json (count=491)\n",
            "\n",
            "== Address-specific Accuracies ==\n",
            "First-Address Exact Acc   : 0.8957  (627/700)\n",
            "First-Address Partial Acc : 0.9586  (671/700)\n",
            "First vs Any    Exact Acc : 0.9229  (646/700)\n",
            "First vs Any    Partial Acc: 1.0000 (700/700)\n",
            "\n",
            "[Saved] TXT -> /content/eval_txt_4/土耳其语_eval.txt\n",
            "[Saved] Mismatches -> /content/mismatches_4/土耳其语_mismatches.json\n",
            "\n",
            "===== Evaluating: 德语 (/content/4/德语.json) =====\n",
            "Loaded 700 test records from /content/4/德语.json\n",
            "0\n",
            "\n",
            "== Exact-match (entity text equal) ==\n",
            "ADDRESS   P=0.7726  R=0.5836  F1=0.6649  (#tp=1634, #fp=481, #fn=1166)\n",
            "\n",
            "== Partial-match (char-overlap>0, greedy) ==\n",
            "ADDRESS   P=1.0000  R=0.7567  F1=0.8615  (#tp=2115, #fp=0, #fn=680)\n",
            "total:700, mismatch:559,acc:20.142857142857142%\n",
            "\n",
            "[Saved] non-exact matched cases -> /content/mismatches_4/德语_mismatches.json (count=559)\n",
            "\n",
            "== Address-specific Accuracies ==\n",
            "First-Address Exact Acc   : 0.8743  (612/700)\n",
            "First-Address Partial Acc : 0.9300  (651/700)\n",
            "First vs Any    Exact Acc : 0.9214  (645/700)\n",
            "First vs Any    Partial Acc: 0.9986 (699/700)\n",
            "\n",
            "[Saved] TXT -> /content/eval_txt_4/德语_eval.txt\n",
            "[Saved] Mismatches -> /content/mismatches_4/德语_mismatches.json\n",
            "\n",
            "===== Evaluating: 意大利语 (/content/4/意大利语.json) =====\n",
            "Loaded 700 test records from /content/4/意大利语.json\n",
            "0\n",
            "\n",
            "== Exact-match (entity text equal) ==\n",
            "ADDRESS   P=0.8924  R=0.7582  F1=0.8198  (#tp=2123, #fp=256, #fn=677)\n",
            "\n",
            "== Partial-match (char-overlap>0, greedy) ==\n",
            "ADDRESS   P=0.9996  R=0.8811  F1=0.9366  (#tp=2378, #fp=1, #fn=321)\n",
            "total:700, mismatch:359,acc:48.714285714285715%\n",
            "\n",
            "[Saved] non-exact matched cases -> /content/mismatches_4/意大利语_mismatches.json (count=359)\n",
            "\n",
            "== Address-specific Accuracies ==\n",
            "First-Address Exact Acc   : 0.9343  (654/700)\n",
            "First-Address Partial Acc : 0.9757  (683/700)\n",
            "First vs Any    Exact Acc : 0.9557  (669/700)\n",
            "First vs Any    Partial Acc: 1.0000 (700/700)\n",
            "\n",
            "[Saved] TXT -> /content/eval_txt_4/意大利语_eval.txt\n",
            "[Saved] Mismatches -> /content/mismatches_4/意大利语_mismatches.json\n",
            "\n",
            "===== Evaluating: 日语 (/content/4/日语.json) =====\n",
            "Loaded 700 test records from /content/4/日语.json\n",
            "0\n",
            "\n",
            "== Exact-match (entity text equal) ==\n",
            "ADDRESS   P=0.8886  R=0.8061  F1=0.8453  (#tp=2257, #fp=283, #fn=543)\n",
            "\n",
            "== Partial-match (char-overlap>0, greedy) ==\n",
            "ADDRESS   P=0.9980  R=0.9063  F1=0.9500  (#tp=2535, #fp=5, #fn=262)\n",
            "total:700, mismatch:358,acc:48.857142857142854%\n",
            "\n",
            "[Saved] non-exact matched cases -> /content/mismatches_4/日语_mismatches.json (count=358)\n",
            "\n",
            "== Address-specific Accuracies ==\n",
            "First-Address Exact Acc   : 0.9129  (639/700)\n",
            "First-Address Partial Acc : 0.9629  (674/700)\n",
            "First vs Any    Exact Acc : 0.9471  (663/700)\n",
            "First vs Any    Partial Acc: 1.0000 (700/700)\n",
            "\n",
            "[Saved] TXT -> /content/eval_txt_4/日语_eval.txt\n",
            "[Saved] Mismatches -> /content/mismatches_4/日语_mismatches.json\n",
            "\n",
            "===== Evaluating: 法语 (/content/4/法语.json) =====\n",
            "Loaded 700 test records from /content/4/法语.json\n",
            "0\n",
            "\n",
            "== Exact-match (entity text equal) ==\n",
            "ADDRESS   P=0.8473  R=0.6936  F1=0.7628  (#tp=1942, #fp=350, #fn=858)\n",
            "\n",
            "== Partial-match (char-overlap>0, greedy) ==\n",
            "ADDRESS   P=1.0000  R=0.8186  F1=0.9002  (#tp=2292, #fp=0, #fn=508)\n",
            "total:700, mismatch:425,acc:39.285714285714285%\n",
            "\n",
            "[Saved] non-exact matched cases -> /content/mismatches_4/法语_mismatches.json (count=425)\n",
            "\n",
            "== Address-specific Accuracies ==\n",
            "First-Address Exact Acc   : 0.9400  (658/700)\n",
            "First-Address Partial Acc : 0.9800  (686/700)\n",
            "First vs Any    Exact Acc : 0.9586  (671/700)\n",
            "First vs Any    Partial Acc: 1.0000 (700/700)\n",
            "\n",
            "[Saved] TXT -> /content/eval_txt_4/法语_eval.txt\n",
            "[Saved] Mismatches -> /content/mismatches_4/法语_mismatches.json\n",
            "\n",
            "===== Evaluating: 瑞典语 (/content/4/瑞典语.json) =====\n",
            "Loaded 700 test records from /content/4/瑞典语.json\n",
            "0\n",
            "\n",
            "== Exact-match (entity text equal) ==\n",
            "ADDRESS   P=0.7797  R=0.6143  F1=0.6872  (#tp=1720, #fp=486, #fn=1080)\n",
            "\n",
            "== Partial-match (char-overlap>0, greedy) ==\n",
            "ADDRESS   P=1.0000  R=0.7879  F1=0.8813  (#tp=2206, #fp=0, #fn=594)\n",
            "total:700, mismatch:509,acc:27.285714285714285%\n",
            "\n",
            "[Saved] non-exact matched cases -> /content/mismatches_4/瑞典语_mismatches.json (count=509)\n",
            "\n",
            "== Address-specific Accuracies ==\n",
            "First-Address Exact Acc   : 0.9429  (660/700)\n",
            "First-Address Partial Acc : 0.9786  (685/700)\n",
            "First vs Any    Exact Acc : 0.9557  (669/700)\n",
            "First vs Any    Partial Acc: 1.0000 (700/700)\n",
            "\n",
            "[Saved] TXT -> /content/eval_txt_4/瑞典语_eval.txt\n",
            "[Saved] Mismatches -> /content/mismatches_4/瑞典语_mismatches.json\n",
            "\n",
            "===== Evaluating: 荷兰语 (/content/4/荷兰语.json) =====\n",
            "Loaded 700 test records from /content/4/荷兰语.json\n",
            "0\n",
            "\n",
            "== Exact-match (entity text equal) ==\n",
            "ADDRESS   P=0.7867  R=0.6046  F1=0.6838  (#tp=1693, #fp=459, #fn=1107)\n",
            "\n",
            "== Partial-match (char-overlap>0, greedy) ==\n",
            "ADDRESS   P=0.9995  R=0.7690  F1=0.8693  (#tp=2151, #fp=1, #fn=646)\n",
            "total:700, mismatch:513,acc:26.71428571428571%\n",
            "\n",
            "[Saved] non-exact matched cases -> /content/mismatches_4/荷兰语_mismatches.json (count=513)\n",
            "\n",
            "== Address-specific Accuracies ==\n",
            "First-Address Exact Acc   : 0.8400  (588/700)\n",
            "First-Address Partial Acc : 0.9414  (659/700)\n",
            "First vs Any    Exact Acc : 0.8843  (619/700)\n",
            "First vs Any    Partial Acc: 1.0000 (700/700)\n",
            "\n",
            "[Saved] TXT -> /content/eval_txt_4/荷兰语_eval.txt\n",
            "[Saved] Mismatches -> /content/mismatches_4/荷兰语_mismatches.json\n",
            "\n",
            "===== Evaluating: 葡萄牙语 (/content/4/葡萄牙语.json) =====\n",
            "Loaded 700 test records from /content/4/葡萄牙语.json\n",
            "0\n",
            "\n",
            "== Exact-match (entity text equal) ==\n",
            "ADDRESS   P=0.8477  R=0.6896  F1=0.7605  (#tp=1931, #fp=347, #fn=869)\n",
            "\n",
            "== Partial-match (char-overlap>0, greedy) ==\n",
            "ADDRESS   P=1.0000  R=0.8136  F1=0.8972  (#tp=2278, #fp=0, #fn=522)\n",
            "total:700, mismatch:423,acc:39.57142857142858%\n",
            "\n",
            "[Saved] non-exact matched cases -> /content/mismatches_4/葡萄牙语_mismatches.json (count=423)\n",
            "\n",
            "== Address-specific Accuracies ==\n",
            "First-Address Exact Acc   : 0.9271  (649/700)\n",
            "First-Address Partial Acc : 0.9743  (682/700)\n",
            "First vs Any    Exact Acc : 0.9471  (663/700)\n",
            "First vs Any    Partial Acc: 1.0000 (700/700)\n",
            "\n",
            "[Saved] TXT -> /content/eval_txt_4/葡萄牙语_eval.txt\n",
            "[Saved] Mismatches -> /content/mismatches_4/葡萄牙语_mismatches.json\n",
            "\n",
            "===== Evaluating: 西班牙语 (/content/4/西班牙语.json) =====\n",
            "Loaded 700 test records from /content/4/西班牙语.json\n",
            "0\n",
            "\n",
            "== Exact-match (entity text equal) ==\n",
            "ADDRESS   P=0.8547  R=0.7061  F1=0.7733  (#tp=1977, #fp=336, #fn=823)\n",
            "\n",
            "== Partial-match (char-overlap>0, greedy) ==\n",
            "ADDRESS   P=0.9996  R=0.8257  F1=0.9044  (#tp=2312, #fp=1, #fn=488)\n",
            "total:700, mismatch:387,acc:44.714285714285715%\n",
            "\n",
            "[Saved] non-exact matched cases -> /content/mismatches_4/西班牙语_mismatches.json (count=387)\n",
            "\n",
            "== Address-specific Accuracies ==\n",
            "First-Address Exact Acc   : 0.9100  (637/700)\n",
            "First-Address Partial Acc : 0.9800  (686/700)\n",
            "First vs Any    Exact Acc : 0.9243  (647/700)\n",
            "First vs Any    Partial Acc: 1.0000 (700/700)\n",
            "\n",
            "[Saved] TXT -> /content/eval_txt_4/西班牙语_eval.txt\n",
            "[Saved] Mismatches -> /content/mismatches_4/西班牙语_mismatches.json\n",
            "\n",
            "===== Evaluating: 韩语 (/content/4/韩语.json) =====\n",
            "Loaded 700 test records from /content/4/韩语.json\n",
            "0\n",
            "\n",
            "== Exact-match (entity text equal) ==\n",
            "ADDRESS   P=0.8605  R=0.7689  F1=0.8121  (#tp=2153, #fp=349, #fn=647)\n",
            "\n",
            "== Partial-match (char-overlap>0, greedy) ==\n",
            "ADDRESS   P=1.0000  R=0.8968  F1=0.9456  (#tp=2502, #fp=0, #fn=288)\n",
            "total:700, mismatch:397,acc:43.28571428571429%\n",
            "\n",
            "[Saved] non-exact matched cases -> /content/mismatches_4/韩语_mismatches.json (count=397)\n",
            "\n",
            "== Address-specific Accuracies ==\n",
            "First-Address Exact Acc   : 0.9071  (635/700)\n",
            "First-Address Partial Acc : 0.9614  (673/700)\n",
            "First vs Any    Exact Acc : 0.9400  (658/700)\n",
            "First vs Any    Partial Acc: 1.0000 (700/700)\n",
            "\n",
            "[Saved] TXT -> /content/eval_txt_4/韩语_eval.txt\n",
            "[Saved] Mismatches -> /content/mismatches_4/韩语_mismatches.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**“正确匹配”判定：**\n",
        "\n",
        "实体级：\n",
        "\n",
        "* **Exact-match:**\n",
        "预测出的实体文本要和标注的实体文本完全一致才算正确\n",
        "\n",
        "* **Partial-match:**\n",
        "预测出的实体文本和标注的实体文本有重叠就算正确\n",
        "\n",
        "\n",
        "句子级：\n",
        "\n",
        "* **first-first:** 模型输出的第一个地址和真实的第一个地址匹配就算正确\n",
        "\n",
        "\n",
        "* **first-any:** 模型输出的第一个地址和真实地址中的任意一个地址匹配就算正确\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**指标：**\n",
        "\n",
        "\n",
        "实体级：\n",
        "\n",
        "  * **TP：** 预测得到的实体和标注的完全相同\n",
        "\n",
        "  * **FP：** 预测出标注中没有的实体\n",
        "\n",
        "  * **FN：** 标注里有但预测里没有的\n",
        "\n",
        "  * **p（精确率）:** tp/(tp+fp),所有预测值中正确的比例\n",
        "\n",
        "  * **r（召回率）:** tp/(tp+fn)所有标注的实体中有多少被预测出来\n",
        "\n",
        "  * **f1:** 2pr/(p+r) p和r的综合指标\n",
        "\n",
        "\n",
        "句子级：\n",
        "  * **acc（准确率）：** 识别正确的句子数/所有句子数\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wsg18JZdSe_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **自定义数据查看tflite模型输出**\n",
        "用于简单查看模型表现，无后处理，无准确率指标输出"
      ],
      "metadata": {
        "id": "ABpXBPPPe6UR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 自定义数据查看tflite模型输出\n",
        "\n",
        "import os, numpy as np, torch\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer\n",
        "import json, os\n",
        "\n",
        "# 要测试的句子\n",
        "samples = [\n",
        "    \"The packages have been delivered to Elmton Court, Woodside Primary School, and Picardy Manorway / Belvedere Station.\",\n",
        "    \"明天上午9:30在上海市徐汇区漕溪北路398号开会。\",\n",
        "    \"我今天十点去广州塔\",\n",
        "    \"下午两点去电影院\",\n",
        "    \"我要去影院\",\n",
        "    \"2024年6月15日早上8:10，计划去北京天坛公园跑步\",\n",
        "    \"导航去公司上班\",\n",
        "    \"最近有一家新的咖啡店开在了南京玄武湖，要不要去试试？\",\n",
        "    \"这家影院的空调是真猛，看电影到一半突然打了个喷嚏，幸好剧情够精彩。\",\n",
        "    \"电影院\",\"公园\",\"牧场\",\"超市\",\"便利店\",\"电影\",\"长隆欢乐世界\",\"深业上城\",\"深圳野生动物园\",\"国际E城\",\"同乐村\",\"乐尚林居\",\n",
        "    \"保利五和电影院\",\"重庆解放碑的炉鼎火锅\",\n",
        "    \"晚上9:50才下班,回到家发现楼下便利店刚打烊。\",\n",
        "    \"2024年6月10日早上7:50在南京玄武湖跑圈,跑完一肚子饿\",\n",
        "    \"山姆会员店\",\n",
        "    \"Late afternoon on 圣诞夜\",\n",
        "    \"今天上午去南山图书馆，下午去山姆会员店\",\n",
        "    \"3:00\",\n",
        "    \"导航去万象天地\",\n",
        "    \"活动一：next Wednesday，活动二：今天, 活动三：九点半\",\n",
        "    \"下午去深圳世界之窗和妹妹玩\",\n",
        "    \"亚马逊州\",\n",
        "    \"小谷围镇穗石村的美食好好吃\",\n",
        "    \"4月30号，也就是星期三，来和我见面\",\n",
        "    \"明天上午9:30在上海市徐汇区漕溪北路398号开会。\",\n",
        "    \"Meet me at 221B Baker Street at 7 pm tomorrow.\",\n",
        "    \"Vi mødes kl. 1979-W07-6 ved DK-6000 Kolding, Østergade 16.\",\n",
        "    \"6月5日（月曜日）に新潟県新潟市中央区万代3丁目7-8で集合するのはいかがですか？\",\n",
        "    \"Perşembe 22 Ağustos 2024 saatinde bir kahve molası verelim.\",\n",
        "    \"The party kicks off at October 12th, 1986 at Vicolo della Cancelleria 6, 00186 Roma RM, Italia.\",\n",
        "    \"下周三上午九点提醒我去保利万和国际影城看电源\",\n",
        "    \"【活动提醒】6月18日10:00在杭州西湖音乐喷泉集合拍照，下午换到柳浪闻莺野餐，不要迟到。\",\n",
        "    \"据交通运输部通告，假期返程高峰预计出现在最后两天。\",\n",
        "    \"上午8:50到桂林象鼻山，10:45爬到象眼平台，下午3:35沿江散步回酒店。\",\n",
        "    \"我要去电影院50米外的小摊\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "TFLITE_PATH = \"/content/drive/MyDrive/model_dy_quant.tflite\"   # 模型路径\n",
        "# 训练时保存的 tokenizer 目录（若没有则回退到基座）\n",
        "TOKENIZER_DIRS = [\"/content/drive/MyDrive/checkpoint-1235\", \"xlm-roberta-base\"] # tokenizer路径（也可不设置）\n",
        "\n",
        "# 固定映射逻辑，请勿修改\n",
        "# O: 不是时间也不是地址的token\n",
        "# B-TIME:一个时间token串的第一个token\n",
        "# I-TIME：一个时间token串的非开头的部分\n",
        "# B-ADDRESS:一个地址token串的第一个token\n",
        "# I-ADDRESS：一个地址token串的非开头的部分\n",
        "LABELS = [\"O\",\"B-TIME\",\"I-TIME\",\"B-ADDRESS\",\"I-ADDRESS\"]\n",
        "ID2LABEL = {i:l for i,l in enumerate(LABELS)}\n",
        "\n",
        "# tokenizer\n",
        "tok = None\n",
        "for p in TOKENIZER_DIRS:\n",
        "    try:\n",
        "        tok = AutoTokenizer.from_pretrained(p, use_fast=True)\n",
        "        print(f\"Loaded tokenizer from: {p}\")\n",
        "        break\n",
        "    except Exception:\n",
        "        pass\n",
        "assert tok is not None, \"Tokenizer not found.\"\n",
        "\n",
        "print(f\"tokenizer model: {tok.backend_tokenizer.model}\")\n",
        "\n",
        "# Interpreter（优先新版 LiteRT，不存在则用 tf.lite）\n",
        "try:\n",
        "    from ai_edge_litert.python.interpreter import Interpreter  # type: ignore\n",
        "    interpreter = Interpreter(model_path=TFLITE_PATH)\n",
        "    print(f\"Using ai_edge_litert.Interpreter:{TFLITE_PATH}\")\n",
        "\n",
        "except Exception:\n",
        "    interpreter = tf.lite.Interpreter(model_path=TFLITE_PATH)\n",
        "    print(f\"Using tf.lite.Interpreter:{TFLITE_PATH} (deprecated warning is OK)\")\n",
        "\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "\n",
        "def load_id2label_from_config(model_dir: str):\n",
        "    # 既可以读 checkpoint-1235 下的 config.json，也可以读 SavedModel 导出目录旁边的 config.json\n",
        "    cfg_path = os.path.join(model_dir, \"config.json\")\n",
        "    with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        cfg = json.load(f)\n",
        "    id2label = {int(k): v for k, v in cfg[\"id2label\"].items()}\n",
        "    label2id = {v: int(k) for k, v in id2label.items()}\n",
        "    labels_order = [id2label[i] for i in range(len(id2label))]\n",
        "    return id2label, label2id, labels_order\n",
        "\n",
        "\n",
        "\n",
        "# 解析输入输出张量信息（自动匹配名称）\n",
        "def _io_info(interp):\n",
        "    inps = interp.get_input_details()\n",
        "    outs = interp.get_output_details()\n",
        "\n",
        "    # 记录索引\n",
        "    idx = {\"input_ids\": None, \"attention_mask\": None, \"token_type_ids\": None}\n",
        "    for i, info in enumerate(inps):\n",
        "        name = info[\"name\"].lower()\n",
        "        if \"input_ids\" in name or \"ids\" in name:\n",
        "            if idx[\"input_ids\"] is None: idx[\"input_ids\"] = i\n",
        "        elif \"attention_mask\" in name or \"mask\" in name:\n",
        "            idx[\"attention_mask\"] = i\n",
        "        elif \"token_type_ids\" in name or \"segment\" in name:\n",
        "            idx[\"token_type_ids\"] = i\n",
        "    # 若只有一个输入，则当作 input_ids；若两个，多半是 ids+mask\n",
        "    if len(inps) == 1 and idx[\"input_ids\"] is None:\n",
        "        idx[\"input_ids\"] = 0\n",
        "    if len(inps) == 2 and idx[\"attention_mask\"] is None:\n",
        "        other = 1 if idx[\"input_ids\"] == 0 else 0\n",
        "        idx[\"attention_mask\"] = other\n",
        "    return inps, outs, idx\n",
        "\n",
        "inps, outs, IDX = _io_info(interpreter)\n",
        "print(\"Inputs:\", [d[\"name\"] for d in inps])\n",
        "print(\"Outputs:\", [d[\"name\"] for d in outs])\n",
        "print(\"Mapped index:\", IDX)\n",
        "\n",
        "# 组装推理函数（动态长度 resize）\n",
        "def infer(text, fallback_len=128):\n",
        "    # 读取模型输入的固定长度（[1, 128]）\n",
        "    inps_local = interpreter.get_input_details()\n",
        "    # 通过 input_ids 这个输入拿到固定长度；如果拿不到就用 fallback\n",
        "    fixed_len = None\n",
        "    if IDX[\"input_ids\"] is not None:\n",
        "        shape = inps_local[IDX[\"input_ids\"]][\"shape\"]\n",
        "        if len(shape) == 2 and shape[1] > 0:\n",
        "            fixed_len = int(shape[1])\n",
        "    if fixed_len is None:\n",
        "        fixed_len = int(fallback_len)\n",
        "\n",
        "    # 关键：在 tokenizer 侧做 padding='max_length' + truncation 到 fixed_len\n",
        "    enc = tok(\n",
        "        text,\n",
        "        return_tensors=\"np\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=fixed_len,\n",
        "    )\n",
        "    print(\"PY ids:\", enc[\"input_ids\"][0][:64].tolist())\n",
        "    print(\"PY mask:\", enc[\"attention_mask\"][0][:64].tolist())\n",
        "    print(\"tokens:\", tok.convert_ids_to_tokens(enc[\"input_ids\"][0])[:32])\n",
        "\n",
        "    input_ids = enc[\"input_ids\"].astype(np.int32)          # [1, fixed_len]\n",
        "    attn      = enc[\"attention_mask\"].astype(np.int32)     # [1, fixed_len]\n",
        "    ttids     = enc.get(\"token_type_ids\", np.zeros_like(input_ids, dtype=np.int32))\n",
        "\n",
        "    # 不要 resize，直接 allocate + set_tensor\n",
        "    interpreter.allocate_tensors()\n",
        "    if IDX[\"input_ids\"] is not None:\n",
        "        interpreter.set_tensor(inps_local[IDX[\"input_ids\"]][\"index\"], input_ids)\n",
        "    if IDX[\"attention_mask\"] is not None:\n",
        "        interpreter.set_tensor(inps_local[IDX[\"attention_mask\"]][\"index\"], attn)\n",
        "    if IDX[\"token_type_ids\"] is not None:\n",
        "        interpreter.set_tensor(inps_local[IDX[\"token_type_ids\"]][\"index\"], ttids)\n",
        "\n",
        "    interpreter.invoke()\n",
        "    logits = interpreter.get_tensor(interpreter.get_output_details()[0][\"index\"])\n",
        "    return enc, logits\n",
        "\n",
        "\n",
        "# 将 logits -> BIO 标签，并合并子词为实体片段\n",
        "def decode_entities(enc, logits):\n",
        "    ids = enc[\"input_ids\"][0].tolist()\n",
        "    toks = tok.convert_ids_to_tokens(ids)\n",
        "    pred_ids = logits[0].argmax(-1).tolist()\n",
        "    labels = [ID2LABEL[i] for i in pred_ids]\n",
        "\n",
        "    # 去掉特殊符号（<s>, </s>, <pad>）\n",
        "    clean = []\n",
        "    for t, y in zip(toks, labels):\n",
        "        if t in (tok.cls_token, tok.sep_token, tok.pad_token, \"<s>\", \"</s>\"):\n",
        "            continue\n",
        "        clean.append((t, y))\n",
        "\n",
        "    # 合并为实体\n",
        "    entities, cur_type, cur_text = [], None, \"\"\n",
        "    for t, y in clean:\n",
        "        if t == \"▁\":\n",
        "            continue\n",
        "\n",
        "        surf = t.replace(\"▁\", \" \").strip()\n",
        "\n",
        "        if y.startswith(\"B-\"):\n",
        "            if cur_type:\n",
        "                entities.append((cur_type, cur_text.strip()))\n",
        "            cur_type, cur_text = y[2:], surf\n",
        "        elif y.startswith(\"I-\") and cur_type == y[2:]:\n",
        "            cur_text += surf\n",
        "        else:\n",
        "            if cur_type:\n",
        "                entities.append((cur_type, cur_text.strip()))\n",
        "            cur_type, cur_text = None, \"\"\n",
        "    if cur_type:\n",
        "        entities.append((cur_type, cur_text.strip()))\n",
        "\n",
        "    return [t for t,_ in clean], [y for _,y in clean], entities\n",
        "\n",
        "\n",
        "\n",
        "# 跑并打印\n",
        "for i, s in enumerate(samples, 1):\n",
        "    enc, logits = infer(s)      # 固定128\n",
        "    toks, labs, ents = decode_entities(enc, logits)\n",
        "    print(\"=\"*70)\n",
        "    print(f\"[{i}] Text:\", s)\n",
        "    print(\"TOK:\", \" \".join(toks[:120]))\n",
        "    print(\"PRD:\", \" \".join(labs[:120]))\n",
        "    print(\"ENTS:\", ents)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-H-6cjkXD20O",
        "outputId": "c1ebde9a-65f3-4003-fff6-8e9f7738f2d3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded tokenizer from: /content/drive/MyDrive/checkpoint-1235\n",
            "tokenizer model: Unigram(unk_id=3, vocab=[(\"<s>\", 0), (\"<pad>\", 0), (\"</s>\", 0), (\"<unk>\", 0), (\",\", -3.4635426998138428), ...], byte_fallback=False)\n",
            "Using tf.lite.Interpreter:/content/drive/MyDrive/model_dy_quant.tflite (deprecated warning is OK)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: ['serving_default_attention_mask:0', 'serving_default_input_ids:0']\n",
            "Outputs: ['StatefulPartitionedCall:0']\n",
            "Mapped index: {'input_ids': 1, 'attention_mask': 0, 'token_type_ids': None}\n",
            "PY ids: [0, 581, 98169, 7, 765, 2809, 75060, 297, 47, 540, 39, 1507, 52341, 4, 39076, 8752, 19012, 1294, 19188, 4, 136, 27860, 147, 1459, 1572, 748, 7514, 248, 12628, 187796, 38488, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁The', '▁package', 's', '▁have', '▁been', '▁deliver', 'ed', '▁to', '▁El', 'm', 'ton', '▁Court', ',', '▁Wood', 'side', '▁Prima', 'ry', '▁School', ',', '▁and', '▁Pic', 'ar', 'dy', '▁Man', 'or', 'way', '▁/', '▁Bel', 'vedere', '▁Station', '.']\n",
            "======================================================================\n",
            "[1] Text: The packages have been delivered to Elmton Court, Woodside Primary School, and Picardy Manorway / Belvedere Station.\n",
            "TOK: ▁The ▁package s ▁have ▁been ▁deliver ed ▁to ▁El m ton ▁Court , ▁Wood side ▁Prima ry ▁School , ▁and ▁Pic ar dy ▁Man or way ▁/ ▁Bel vedere ▁Station .\n",
            "PRD: O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "ENTS: [('ADDRESS', 'ElmtonCourt,WoodsidePrimarySchool,andPicardyManorway/BelvedereStation')]\n",
            "PY ids: [0, 6, 72938, 66323, 1126, 12, 1197, 213, 106321, 38216, 47360, 3624, 246192, 52551, 5929, 3136, 151616, 5730, 4185, 1147, 30, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '明天', '上午', '9', ':', '30', '在', '上海市', '徐', '汇', '区', '漕', '溪', '北', '路', '398', '号', '开', '会', '。', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[2] Text: 明天上午9:30在上海市徐汇区漕溪北路398号开会。\n",
            "TOK: ▁ 明天 上午 9 : 30 在 上海市 徐 汇 区 漕 溪 北 路 398 号 开 会 。\n",
            "PRD: B-TIME B-TIME I-TIME I-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O\n",
            "ENTS: [('TIME', '明天上午9:30'), ('ADDRESS', '上海市徐汇区漕溪北路398号')]\n",
            "PY ids: [0, 13129, 7461, 11196, 2391, 1677, 47591, 18248, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁我', '今天', '十', '点', '去', '广州', '塔', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[3] Text: 我今天十点去广州塔\n",
            "TOK: ▁我 今天 十 点 去 广州 塔\n",
            "PRD: O B-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS\n",
            "ENTS: [('TIME', '今天十点'), ('ADDRESS', '广州塔')]\n",
            "PY ids: [0, 6, 33253, 6442, 2391, 1677, 14781, 6105, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '下午', '两', '点', '去', '电影', '院', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[4] Text: 下午两点去电影院\n",
            "TOK: ▁ 下午 两 点 去 电影 院\n",
            "PRD: B-TIME B-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS\n",
            "ENTS: [('TIME', '下午两点'), ('ADDRESS', '电影院')]\n",
            "PY ids: [0, 13129, 96154, 188621, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁我', '要去', '影院', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[5] Text: 我要去影院\n",
            "TOK: ▁我 要去 影院\n",
            "PRD: O O O\n",
            "ENTS: []\n",
            "PY ids: [0, 387, 2357, 470, 910, 630, 1837, 635, 48356, 1019, 15110, 4, 11578, 1677, 8732, 1906, 159083, 77493, 171621, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁20', '24', '年', '6', '月', '15', '日', '早上', '8', ':10', ',', '计划', '去', '北京', '天', '坛', '公园', '跑步', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[6] Text: 2024年6月15日早上8:10，计划去北京天坛公园跑步\n",
            "TOK: ▁20 24 年 6 月 15 日 早上 8 :10 , 计划 去 北京 天 坛 公园 跑步\n",
            "PRD: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "ENTS: [('TIME', '2024年6月15日早上8:10'), ('ADDRESS', '北京天坛公园')]\n",
            "PY ids: [0, 6, 199592, 1677, 1903, 74375, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '导航', '去', '公司', '上班', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[7] Text: 导航去公司上班\n",
            "TOK: ▁ 导航 去 公司 上班\n",
            "PRD: O O O O O\n",
            "ENTS: []\n",
            "PY ids: [0, 47504, 465, 18994, 12122, 22018, 3381, 4185, 111429, 53174, 111075, 14635, 19664, 4, 1050, 7402, 1677, 202920, 32, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁最近', '有', '一家', '新的', '咖啡', '店', '开', '在了', '南京', '玄', '武', '湖', ',', '要', '不要', '去', '试试', '?', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[8] Text: 最近有一家新的咖啡店开在了南京玄武湖，要不要去试试？\n",
            "TOK: ▁最近 有 一家 新的 咖啡 店 开 在了 南京 玄 武 湖 , 要 不要 去 试试 ?\n",
            "PRD: O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O\n",
            "ENTS: [('ADDRESS', '南京玄武湖')]\n",
            "PY ids: [0, 6, 112727, 188621, 43, 109612, 354, 5364, 49426, 4, 2112, 14781, 789, 74491, 18503, 2554, 128457, 130305, 245451, 4, 35754, 1322, 165438, 112059, 52479, 30, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '这家', '影院', '的', '空调', '是', '真', '猛', ',', '看', '电影', '到', '一半', '突然', '打', '了个', '喷', '嚏', ',', '幸', '好', '剧情', '够', '精彩', '。', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[9] Text: 这家影院的空调是真猛，看电影到一半突然打了个喷嚏，幸好剧情够精彩。\n",
            "TOK: ▁ 这家 影院 的 空调 是 真 猛 , 看 电影 到 一半 突然 打 了个 喷 嚏 , 幸 好 剧情 够 精彩 。\n",
            "PRD: O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "ENTS: []\n",
            "PY ids: [0, 6, 14781, 6105, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '电影', '院', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[10] Text: 电影院\n",
            "TOK: ▁ 电影 院\n",
            "PRD: B-ADDRESS B-ADDRESS I-ADDRESS\n",
            "ENTS: [('ADDRESS', '电影院')]\n",
            "PY ids: [0, 6, 77493, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '公园', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[11] Text: 公园\n",
            "TOK: ▁ 公园\n",
            "PRD: B-ADDRESS B-ADDRESS\n",
            "ENTS: [('ADDRESS', '公园')]\n",
            "PY ids: [0, 6, 62352, 6778, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '牧', '场', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[12] Text: 牧场\n",
            "TOK: ▁ 牧 场\n",
            "PRD: B-ADDRESS B-ADDRESS I-ADDRESS\n",
            "ENTS: [('ADDRESS', '牧场')]\n",
            "PY ids: [0, 6, 107156, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '超市', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[13] Text: 超市\n",
            "TOK: ▁ 超市\n",
            "PRD: B-ADDRESS B-ADDRESS\n",
            "ENTS: [('ADDRESS', '超市')]\n",
            "PY ids: [0, 6, 27094, 3381, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '便利', '店', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[14] Text: 便利店\n",
            "TOK: ▁ 便利 店\n",
            "PRD: B-ADDRESS B-ADDRESS O\n",
            "ENTS: [('ADDRESS', '便利')]\n",
            "PY ids: [0, 6, 14781, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '电影', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[15] Text: 电影\n",
            "TOK: ▁ 电影\n",
            "PRD: O O\n",
            "ENTS: []\n",
            "PY ids: [0, 6, 3846, 38003, 216350, 3221, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '长', '隆', '欢乐', '世界', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[16] Text: 长隆欢乐世界\n",
            "TOK: ▁ 长 隆 欢乐 世界\n",
            "PRD: B-ADDRESS B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS\n",
            "ENTS: [('ADDRESS', '长隆欢乐世界')]\n",
            "PY ids: [0, 6, 6728, 9886, 575, 4741, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '深', '业', '上', '城', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[17] Text: 深业上城\n",
            "TOK: ▁ 深 业 上 城\n",
            "PRD: B-ADDRESS B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS\n",
            "ENTS: [('ADDRESS', '深业上城')]\n",
            "PY ids: [0, 6, 34670, 10344, 2026, 71733, 33151, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '深圳', '野', '生', '动物', '园', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[18] Text: 深圳野生动物园\n",
            "TOK: ▁ 深圳 野 生 动物 园\n",
            "PRD: B-ADDRESS B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS\n",
            "ENTS: [('ADDRESS', '深圳野生动物园')]\n",
            "PY ids: [0, 6, 5559, 647, 4741, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '国际', 'E', '城', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[19] Text: 国际E城\n",
            "TOK: ▁ 国际 E 城\n",
            "PRD: B-ADDRESS B-ADDRESS I-ADDRESS I-ADDRESS\n",
            "ENTS: [('ADDRESS', '国际E城')]\n",
            "PY ids: [0, 6, 3169, 15240, 8616, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '同', '乐', '村', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[20] Text: 同乐村\n",
            "TOK: ▁ 同 乐 村\n",
            "PRD: B-ADDRESS B-ADDRESS I-ADDRESS I-ADDRESS\n",
            "ENTS: [('ADDRESS', '同乐村')]\n",
            "PY ids: [0, 6, 15240, 34484, 3942, 10721, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '乐', '尚', '林', '居', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[21] Text: 乐尚林居\n",
            "TOK: ▁ 乐 尚 林 居\n",
            "PRD: B-ADDRESS B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS\n",
            "ENTS: [('ADDRESS', '乐尚林居')]\n",
            "PY ids: [0, 6, 6082, 3908, 5137, 264, 14781, 6105, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '保', '利', '五', '和', '电影', '院', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[22] Text: 保利五和电影院\n",
            "TOK: ▁ 保 利 五 和 电影 院\n",
            "PRD: B-ADDRESS B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS\n",
            "ENTS: [('ADDRESS', '保利五和电影院')]\n",
            "PY ids: [0, 6, 62823, 78322, 182535, 43, 135873, 108281, 5505, 138300, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '重庆', '解放', '碑', '的', '炉', '鼎', '火', '锅', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[23] Text: 重庆解放碑的炉鼎火锅\n",
            "TOK: ▁ 重庆 解放 碑 的 炉 鼎 火 锅\n",
            "PRD: B-ADDRESS B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS\n",
            "ENTS: [('ADDRESS', '重庆解放碑的炉鼎火锅')]\n",
            "PY ids: [0, 6, 23730, 1126, 23837, 4395, 159759, 4, 210304, 8261, 14843, 1130, 27094, 3381, 26471, 2554, 249730, 30, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '晚上', '9', ':50', '才', '下班', ',', '回到家', '发现', '楼', '下', '便利', '店', '刚', '打', '烊', '。', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[24] Text: 晚上9:50才下班,回到家发现楼下便利店刚打烊。\n",
            "TOK: ▁ 晚上 9 :50 才 下班 , 回到家 发现 楼 下 便利 店 刚 打 烊 。\n",
            "PRD: B-TIME B-TIME I-TIME I-TIME O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O O O O O\n",
            "ENTS: [('TIME', '晚上9:50'), ('ADDRESS', '楼下便利')]\n",
            "PY ids: [0, 387, 2357, 470, 910, 630, 963, 635, 48356, 966, 23837, 213, 53174, 111075, 14635, 19664, 11659, 18453, 4, 11659, 8943, 684, 120475, 184120, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁20', '24', '年', '6', '月', '10', '日', '早上', '7', ':50', '在', '南京', '玄', '武', '湖', '跑', '圈', ',', '跑', '完', '一', '肚子', '饿', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[25] Text: 2024年6月10日早上7:50在南京玄武湖跑圈,跑完一肚子饿\n",
            "TOK: ▁20 24 年 6 月 10 日 早上 7 :50 在 南京 玄 武 湖 跑 圈 , 跑 完 一 肚子 饿\n",
            "PRD: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O\n",
            "ENTS: [('TIME', '2024年6月10日早上7:50'), ('ADDRESS', '南京玄武湖')]\n",
            "PY ids: [0, 6, 2272, 26027, 73470, 3381, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '山', '姆', '会员', '店', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[26] Text: 山姆会员店\n",
            "TOK: ▁ 山 姆 会员 店\n",
            "PRD: B-ADDRESS B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS\n",
            "ENTS: [('ADDRESS', '山姆会员店')]\n",
            "PY ids: [0, 239, 67, 157109, 98, 6, 26546, 249962, 8097, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁La', 'te', '▁afternoon', '▁on', '▁', '圣', '诞', '夜', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[27] Text: Late afternoon on 圣诞夜\n",
            "TOK: ▁La te ▁afternoon ▁on ▁ 圣 诞 夜\n",
            "PRD: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME\n",
            "ENTS: [('TIME', 'Lateafternoonon圣诞夜')]\n",
            "PY ids: [0, 61168, 66323, 1677, 4617, 2272, 160115, 4, 33253, 1677, 2272, 26027, 73470, 3381, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁今天', '上午', '去', '南', '山', '图书馆', ',', '下午', '去', '山', '姆', '会员', '店', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[28] Text: 今天上午去南山图书馆，下午去山姆会员店\n",
            "TOK: ▁今天 上午 去 南 山 图书馆 , 下午 去 山 姆 会员 店\n",
            "PRD: B-TIME O O B-ADDRESS I-ADDRESS I-ADDRESS O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS\n",
            "ENTS: [('TIME', '今天'), ('ADDRESS', '南山图书馆'), ('ADDRESS', '山姆会员店')]\n",
            "PY ids: [0, 138, 6632, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁3', ':00', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[29] Text: 3:00\n",
            "TOK: ▁3 :00\n",
            "PRD: B-TIME I-TIME\n",
            "ENTS: [('TIME', '3:00')]\n",
            "PY ids: [0, 6, 199592, 1677, 3895, 24082, 124198, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '导航', '去', '万', '象', '天地', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[30] Text: 导航去万象天地\n",
            "TOK: ▁ 导航 去 万 象 天地\n",
            "PRD: O O O B-ADDRESS I-ADDRESS I-ADDRESS\n",
            "ENTS: [('ADDRESS', '万象天地')]\n",
            "PY ids: [0, 6, 7005, 684, 12, 86, 29062, 64227, 4, 7005, 3195, 12, 7461, 4, 6, 7005, 1971, 12, 11669, 2391, 6193, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '活动', '一', ':', 'ne', 'xt', '▁Wednesday', ',', '活动', '二', ':', '今天', ',', '▁', '活动', '三', ':', '九', '点', '半', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[31] Text: 活动一：next Wednesday，活动二：今天, 活动三：九点半\n",
            "TOK: ▁ 活动 一 : ne xt ▁Wednesday , 活动 二 : 今天 , ▁ 活动 三 : 九 点 半\n",
            "PRD: O O O O B-TIME I-TIME I-TIME O O O O O O O O O O O I-TIME O\n",
            "ENTS: [('TIME', 'nextWednesday')]\n",
            "PY ids: [0, 6, 33253, 1677, 34670, 3221, 1420, 31271, 264, 91627, 6484, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '下午', '去', '深圳', '世界', '之', '窗', '和', '妹妹', '玩', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[32] Text: 下午去深圳世界之窗和妹妹玩\n",
            "TOK: ▁ 下午 去 深圳 世界 之 窗 和 妹妹 玩\n",
            "PRD: O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O\n",
            "ENTS: [('ADDRESS', '深圳世界之窗')]\n",
            "PY ids: [0, 6, 166741, 7800, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '亚马逊', '州', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[33] Text: 亚马逊州\n",
            "TOK: ▁ 亚马逊 州\n",
            "PRD: B-ADDRESS B-ADDRESS I-ADDRESS\n",
            "ENTS: [('ADDRESS', '亚马逊州')]\n",
            "PY ids: [0, 57960, 24081, 63699, 35368, 243567, 6451, 8616, 43, 27841, 1322, 70012, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁小', '谷', '围', '镇', '穗', '石', '村', '的', '美食', '好', '好吃', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[34] Text: 小谷围镇穗石村的美食好好吃\n",
            "TOK: ▁小 谷 围 镇 穗 石 村 的 美食 好 好吃\n",
            "PRD: B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O\n",
            "ENTS: [('ADDRESS', '小谷围镇穗石村')]\n",
            "PY ids: [0, 201, 630, 1197, 5730, 4, 47474, 32999, 1971, 4, 1589, 101981, 157563, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁4', '月', '30', '号', ',', '也就是', '星期', '三', ',', '来', '和我', '见面', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[35] Text: 4月30号，也就是星期三，来和我见面\n",
            "TOK: ▁4 月 30 号 , 也就是 星期 三 , 来 和我 见面\n",
            "PRD: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O\n",
            "ENTS: [('TIME', '4月30号,也就是星期三')]\n",
            "PY ids: [0, 6, 72938, 66323, 1126, 12, 1197, 213, 106321, 38216, 47360, 3624, 246192, 52551, 5929, 3136, 151616, 5730, 4185, 1147, 30, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '明天', '上午', '9', ':', '30', '在', '上海市', '徐', '汇', '区', '漕', '溪', '北', '路', '398', '号', '开', '会', '。', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[36] Text: 明天上午9:30在上海市徐汇区漕溪北路398号开会。\n",
            "TOK: ▁ 明天 上午 9 : 30 在 上海市 徐 汇 区 漕 溪 北 路 398 号 开 会 。\n",
            "PRD: B-TIME B-TIME I-TIME I-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O\n",
            "ENTS: [('TIME', '明天上午9:30'), ('ADDRESS', '上海市徐汇区漕溪北路398号')]\n",
            "PY ids: [0, 72626, 163, 99, 128811, 571, 133840, 15130, 99, 361, 3592, 127773, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁Meet', '▁me', '▁at', '▁221', 'B', '▁Baker', '▁Street', '▁at', '▁7', '▁pm', '▁tomorrow', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[37] Text: Meet me at 221B Baker Street at 7 pm tomorrow.\n",
            "TOK: ▁Meet ▁me ▁at ▁221 B ▁Baker ▁Street ▁at ▁7 ▁pm ▁tomorrow .\n",
            "PRD: O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-TIME B-TIME I-TIME O O\n",
            "ENTS: [('ADDRESS', '221BBakerStreet'), ('TIME', '7pm')]\n",
            "PY ids: [0, 582, 39711, 7, 4434, 5, 24849, 9, 1456, 8368, 13545, 1226, 64275, 9, 78263, 186808, 4, 155664, 25183, 6849, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁Vi', '▁møde', 's', '▁kl', '.', '▁1979', '-', 'W', '07', '-6', '▁ved', '▁DK', '-', '6000', '▁Kolding', ',', '▁Øster', 'gade', '▁16.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[38] Text: Vi mødes kl. 1979-W07-6 ved DK-6000 Kolding, Østergade 16.\n",
            "TOK: ▁Vi ▁møde s ▁kl . ▁1979 - W 07 -6 ▁ved ▁DK - 6000 ▁Kolding , ▁Øster gade ▁16.\n",
            "PRD: O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS\n",
            "ENTS: [('TIME', '1979-W07-6'), ('ADDRESS', 'DK-6000Kolding,Østergade16.')]\n",
            "PY ids: [0, 305, 630, 758, 635, 132, 630, 102766, 635, 16, 327, 211737, 19275, 211737, 2128, 13244, 3624, 3895, 5260, 363, 191585, 181216, 507, 130293, 72915, 62319, 281, 60369, 32, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁6', '月', '5', '日', '(', '月', '曜', '日', ')', 'に', '新潟', '県', '新潟', '市', '中央', '区', '万', '代', '3', '丁目', '7-8', 'で', '集合', 'するのは', 'いか', 'が', 'ですか', '?', '</s>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[39] Text: 6月5日（月曜日）に新潟県新潟市中央区万代3丁目7-8で集合するのはいかがですか？\n",
            "TOK: ▁6 月 5 日 ( 月 曜 日 ) に 新潟 県 新潟 市 中央 区 万 代 3 丁目 7-8 で 集合 するのは いか が ですか ?\n",
            "PRD: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O\n",
            "ENTS: [('TIME', '6月5日(月曜日)'), ('ADDRESS', '新潟県新潟市中央区万代3丁目7-8')]\n",
            "PY ids: [0, 199517, 1039, 83281, 387, 2357, 2013, 5012, 263, 140104, 20467, 16561, 493, 40251, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁Perşembe', '▁22', '▁Ağustos', '▁20', '24', '▁saat', 'inde', '▁bir', '▁kahve', '▁mol', 'ası', '▁ver', 'elim', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[40] Text: Perşembe 22 Ağustos 2024 saatinde bir kahve molası verelim.\n",
            "TOK: ▁Perşembe ▁22 ▁Ağustos ▁20 24 ▁saat inde ▁bir ▁kahve ▁mol ası ▁ver elim .\n",
            "PRD: B-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O\n",
            "ENTS: [('TIME', 'Perşembe22Ağustos2024')]\n",
            "PY ids: [0, 581, 19085, 101630, 7, 5773, 99, 18374, 427, 927, 4, 21735, 99, 582, 22796, 832, 80392, 1341, 11, 305, 4, 6, 34479, 15276, 8321, 12174, 4, 7694, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁The', '▁party', '▁kick', 's', '▁off', '▁at', '▁October', '▁12', 'th', ',', '▁1986', '▁at', '▁Vi', 'colo', '▁della', '▁Cancel', 'leri', 'a', '▁6', ',', '▁', '001', '86', '▁Roma', '▁RM', ',', '▁Italia', '.', '</s>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[41] Text: The party kicks off at October 12th, 1986 at Vicolo della Cancelleria 6, 00186 Roma RM, Italia.\n",
            "TOK: ▁The ▁party ▁kick s ▁off ▁at ▁October ▁12 th , ▁1986 ▁at ▁Vi colo ▁della ▁Cancel leri a ▁6 , ▁ 001 86 ▁Roma ▁RM , ▁Italia .\n",
            "PRD: O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "ENTS: [('TIME', 'October12th,1986'), ('ADDRESS', 'VicolodellaCancelleria6,00186RomaRM,Italia')]\n",
            "PY ids: [0, 6, 1130, 6271, 1971, 66323, 11669, 2391, 35027, 125753, 6082, 3908, 3895, 264, 5559, 12235, 4741, 2112, 8312, 9730, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '下', '周', '三', '上午', '九', '点', '提醒', '我去', '保', '利', '万', '和', '国际', '影', '城', '看', '电', '源', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[42] Text: 下周三上午九点提醒我去保利万和国际影城看电源\n",
            "TOK: ▁ 下 周 三 上午 九 点 提醒 我去 保 利 万 和 国际 影 城 看 电 源\n",
            "PRD: B-TIME B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O\n",
            "ENTS: [('TIME', '下周三上午九点'), ('ADDRESS', '保利万和国际影城')]\n",
            "PY ids: [0, 5946, 7005, 35027, 2728, 910, 630, 1819, 635, 963, 6632, 213, 60620, 3891, 19664, 35766, 130305, 41318, 130293, 97386, 4, 33253, 19543, 789, 83652, 61237, 63198, 246492, 10344, 19814, 4, 7402, 85564, 789, 30, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁【', '活动', '提醒', '】', '6', '月', '18', '日', '10', ':00', '在', '杭州', '西', '湖', '音乐', '喷', '泉', '集合', '拍照', ',', '下午', '换', '到', '柳', '浪', '闻', '莺', '野', '餐', ',', '不要']\n",
            "======================================================================\n",
            "[43] Text: 【活动提醒】6月18日10:00在杭州西湖音乐喷泉集合拍照，下午换到柳浪闻莺野餐，不要迟到。\n",
            "TOK: ▁【 活动 提醒 】 6 月 18 日 10 :00 在 杭州 西 湖 音乐 喷 泉 集合 拍照 , 下午 换 到 柳 浪 闻 莺 野 餐 , 不要 迟 到 。\n",
            "PRD: O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O\n",
            "ENTS: [('TIME', '6月18日10:00'), ('ADDRESS', '杭州西湖音乐喷泉'), ('ADDRESS', '柳浪闻莺')]\n",
            "PY ids: [0, 39733, 228468, 2649, 3909, 22292, 4, 85803, 15527, 11072, 111960, 54769, 111081, 15657, 121395, 30, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁据', '交通运输', '部', '通', '告', ',', '假期', '返', '程', '高峰', '预计', '出现在', '最后', '两天', '。', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[44] Text: 据交通运输部通告，假期返程高峰预计出现在最后两天。\n",
            "TOK: ▁据 交通运输 部 通 告 , 假期 返 程 高峰 预计 出现在 最后 两天 。\n",
            "PRD: O O O O O O O O O O O O B-TIME I-TIME O\n",
            "ENTS: [('TIME', '最后两天')]\n",
            "PY ids: [0, 6, 66323, 1019, 23837, 789, 86974, 3942, 24082, 48563, 2272, 4, 963, 15989, 64297, 789, 24082, 5003, 7488, 4, 33253, 363, 27144, 33073, 9751, 188955, 2490, 6989, 30, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁', '上午', '8', ':50', '到', '桂', '林', '象', '鼻', '山', ',', '10', ':45', '爬', '到', '象', '眼', '平台', ',', '下午', '3', ':35', '沿', '江', '散步', '回', '酒店', '。', '</s>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[45] Text: 上午8:50到桂林象鼻山，10:45爬到象眼平台，下午3:35沿江散步回酒店。\n",
            "TOK: ▁ 上午 8 :50 到 桂 林 象 鼻 山 , 10 :45 爬 到 象 眼 平台 , 下午 3 :35 沿 江 散步 回 酒店 。\n",
            "PRD: B-TIME B-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O B-TIME I-TIME O O B-ADDRESS I-ADDRESS I-ADDRESS O B-TIME I-TIME I-TIME I-ADDRESS I-ADDRESS O O O O\n",
            "ENTS: [('TIME', '上午8:50'), ('ADDRESS', '桂林象鼻山'), ('TIME', '10:45'), ('ADDRESS', '象眼平台'), ('TIME', '下午3:35')]\n",
            "PY ids: [0, 13129, 96154, 14781, 6105, 2525, 5473, 1998, 16394, 188819, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "PY mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tokens: ['<s>', '▁我', '要去', '电影', '院', '50', '米', '外', '的小', '摊', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "======================================================================\n",
            "[46] Text: 我要去电影院50米外的小摊\n",
            "TOK: ▁我 要去 电影 院 50 米 外 的小 摊\n",
            "PRD: O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O\n",
            "ENTS: [('ADDRESS', '电影院50米外')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 其他代码"
      ],
      "metadata": {
        "id": "yBvKXyPpEWmw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "检查训练数据集"
      ],
      "metadata": {
        "id": "LOdRqoY_LwFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# 数据文件路径\n",
        "DATA_PATH = \"/content/drive/MyDrive/NER/train_datasets_AC/train_葡萄牙语.conll\"\n",
        "\n",
        "# 标签定义\n",
        "LABELS = [\"O\", \"B-ADDRESS\", \"I-ADDRESS\", \"B-TIME\", \"I-TIME\"]\n",
        "\n",
        "def read_conll(path):\n",
        "    \"\"\"读取 CoNLL 文件 -> [(tokens, labels), ...]\"\"\"\n",
        "    sents = []\n",
        "    toks, labs = [], []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if toks:\n",
        "                    sents.append((toks, labs))\n",
        "                    toks, labs = [], []\n",
        "                continue\n",
        "            if \" \" in line:\n",
        "                t, y = line.split(\" \", 1)\n",
        "            else:\n",
        "                t, y = line, \"O\"\n",
        "            if y not in LABELS:\n",
        "                y = \"O\"\n",
        "            toks.append(t)\n",
        "            labs.append(y)\n",
        "    if toks:\n",
        "        sents.append((toks, labs))\n",
        "    return sents\n",
        "\n",
        "\n",
        "def extract_entities(tokens, labels):\n",
        "    \"\"\"从 tokens + BIO labels 中抽取 time/address 实体\"\"\"\n",
        "    entities = {\"TIME\": [], \"ADDRESS\": []}\n",
        "    cur_type, cur_tokens = None, []\n",
        "\n",
        "    for t, y in zip(tokens, labels):\n",
        "        if y == \"O\":\n",
        "            if cur_type:\n",
        "                entities[cur_type].append(\"\".join(cur_tokens))\n",
        "                cur_type, cur_tokens = None, []\n",
        "            continue\n",
        "\n",
        "        prefix, etype = y.split(\"-\", 1)\n",
        "        if prefix == \"B\":\n",
        "            if cur_type:\n",
        "                entities[cur_type].append(\"\".join(cur_tokens))\n",
        "            cur_type, cur_tokens = etype, [t]\n",
        "        elif prefix == \"I\" and cur_type == etype:\n",
        "            cur_tokens.append(t)\n",
        "        else:\n",
        "            if cur_type:\n",
        "                entities[cur_type].append(\"\".join(cur_tokens))\n",
        "            cur_type, cur_tokens = etype, [t]\n",
        "\n",
        "    if cur_type:\n",
        "        entities[cur_type].append(\"\".join(cur_tokens))\n",
        "\n",
        "    return entities\n",
        "\n",
        "\n",
        "# 读取训练集\n",
        "sents = read_conll(DATA_PATH)\n",
        "print(f\"Loaded {len(sents)} sentences from {DATA_PATH}\")\n",
        "\n",
        "# 随机抽取 100 条\n",
        "samples = random.sample(sents, 100)\n",
        "\n",
        "for i, (tokens, labels) in enumerate(samples, 1):\n",
        "    ents = extract_entities(tokens, labels)\n",
        "    print(\"=\"*60)\n",
        "    print(f\"[{i}] TOK:\", \" \".join(tokens))\n",
        "    print(\"LBL:\", \" \".join(labels))\n",
        "    print(\"TIME:\", ents[\"TIME\"])\n",
        "    print(\"ADDR:\", ents[\"ADDRESS\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSlnu_MnLyde",
        "outputId": "587b9c2d-83dd-48c4-b0e9-ae8dc3662e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1000 sentences from /content/drive/MyDrive/NER/train_datasets_AC/train_葡萄牙语.conll\n",
            "============================================================\n",
            "[1] TOK: ▁25 ▁de ▁agosto ▁às ▁22 h 15 ▁visite i ▁o ▁Está dio ▁do ▁Campo ▁Futebol ▁Club e ▁para ▁comprar ▁ingredientes ▁fresco s .\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O\n",
            "TIME: ['▁25▁de▁agosto▁às▁22h15']\n",
            "ADDR: ['▁Estádio▁do▁Campo▁Futebol▁Clube']\n",
            "============================================================\n",
            "[2] TOK: ▁Che gamos ▁a ▁Edi fíci o ▁29 ▁2 ▁de ▁mai . ▁de ▁2021 , ▁explora mos ▁os ▁arredor es , ▁e ▁depois ▁seguimos ▁para ▁22 # ▁No ite ▁de ▁31 ▁de ▁outubro , ▁volta ndo ▁para ▁casa ▁18 ▁de ▁janeiro ▁de ▁20 34 ▁às ▁06 :36 .\n",
            "LBL: O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁2▁de▁mai.▁de▁2021', '▁Noite▁de▁31▁de▁outubro', '▁18▁de▁janeiro▁de▁2034▁às▁06:36']\n",
            "ADDR: []\n",
            "============================================================\n",
            "[3] TOK: ▁10 ▁de ▁outubro ▁de ▁2023 ▁explore i ▁as ▁galeria s ▁de ▁arte ▁no ▁Pelo u rinho ▁de ▁Po mbal inho ▁e ▁depois ▁relax ei ▁no ▁parque ▁próximo .\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O\n",
            "TIME: ['▁10▁de▁outubro▁de▁2023']\n",
            "ADDR: ['▁Pelourinho▁de▁Pombalinho']\n",
            "============================================================\n",
            "[4] TOK: ▁A ▁maraton a ▁começou ▁em ▁Parque ▁Nossa ▁Senhor a ▁dos ▁Mila gres ▁durante ▁Aman hã ▁ao ▁cair ▁da ▁tarde ▁às ▁19 h 25 , ▁passou ▁por ▁Fra tel ▁e ▁termino u ▁em ▁Ca ci lhas .\n",
            "LBL: O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O B-ADDRESS I-ADDRESS O O O O B-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TIME: ['▁Amanhã▁ao▁cair▁da▁tarde▁às▁19h25']\n",
            "ADDR: ['▁Parque▁Nossa▁Senhora▁dos▁Milagres', '▁Fratel', '▁Cacilhas']\n",
            "============================================================\n",
            "[5] TOK: ▁Os ▁alunos ▁devem ▁enviar ▁seus ▁trabalhos ▁Segunda - feira ▁da ▁semana ▁que ▁vem ▁às ▁18 h 15 , ▁e ▁os ▁resultados ▁estar ão ▁disponíveis ▁Da qui ▁a ▁três ▁dias ▁às ▁13 h 15 .\n",
            "LBL: O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁Segunda-feira▁da▁semana▁que▁vem▁às▁18h15', '▁Daqui▁a▁três▁dias▁às▁13h15']\n",
            "ADDR: []\n",
            "============================================================\n",
            "[6] TOK: ▁2 ▁de ▁abril ▁de ▁20 35 ▁às ▁07 :35 ▁descobri ▁um ▁novo ▁restaurante ▁no ▁Centro ▁Social ▁e ▁Paro qui al ▁de ▁Ci b ões ▁e ▁adorei ▁a ▁comida .\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O\n",
            "TIME: ['▁2▁de▁abril▁de▁2035▁às▁07:35']\n",
            "ADDR: ['▁Centro▁Social▁e▁Paroquial▁de▁Cibões']\n",
            "============================================================\n",
            "[7] TOK: ▁O ▁castelo ▁de ▁Alma da ▁( Pra gal ) ▁é ▁um ▁lugar ▁fascinant e ▁para ▁aprender ▁sobre ▁história ▁local .\n",
            "LBL: O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: ['▁Almada▁(Pragal)']\n",
            "============================================================\n",
            "[8] TOK: ▁As ▁ruas ▁estavam ▁decora das ▁com ▁luze s ▁colori das ▁para ▁o ▁festival .\n",
            "LBL: O O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[9] TOK: ▁Che gamos ▁a ▁Edi fíci o ▁43 ▁3 ▁de ▁março ▁de ▁2030 , ▁descans amos ▁Hoje ▁às ▁19 h 45 ▁e ▁parti mos ▁novamente ▁Da qui ▁a ▁cinco ▁dias ▁às ▁18 h 55 .\n",
            "LBL: O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME O O O B-TIME I-TIME I-TIME I-TIME I-TIME O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁3▁de▁março▁de▁2030', '▁Hoje▁às▁19h45', '▁Daqui▁a▁cinco▁dias▁às▁18h55']\n",
            "ADDR: []\n",
            "============================================================\n",
            "[10] TOK: ▁As ▁montanha s ▁oferece m ▁vistas ▁de slu mbra ntes ▁para ▁os ▁aventure iros .\n",
            "LBL: O O O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[11] TOK: ▁Passe i ▁pelo ▁Edi fíci o ▁Est rela ▁do ▁Norte ▁Me ados ▁de ▁março ▁antes ▁de ▁me ▁encontrar ▁com ▁meus ▁pais ▁no ▁Entrada ▁Nú cle o ▁Rural ▁17 ▁de ▁dezembro ▁de ▁2011.\n",
            "LBL: O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME\n",
            "TIME: ['▁Meados▁de▁março', '▁17▁de▁dezembro▁de▁2011.']\n",
            "ADDR: ['▁Edifício▁Estrela▁do▁Norte', '▁Entrada▁Núcleo▁Rural']\n",
            "============================================================\n",
            "[12] TOK: ▁Durante ▁20 ▁de ▁mai . ▁de ▁20 35 , ▁escr evi ▁um ▁artigo ▁sobre ▁ sustentabilidade .\n",
            "LBL: O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O\n",
            "TIME: ['▁20▁de▁mai.▁de▁2035']\n",
            "ADDR: []\n",
            "============================================================\n",
            "[13] TOK: ▁A ▁feira ▁gastro nôm ica ▁abre ▁suas ▁portas ▁6 ▁de ▁abril ▁às ▁10 h 00 ▁e ▁fecha ▁oficialmente ▁14 ▁de ▁agosto ▁de ▁2030 .\n",
            "LBL: O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O B-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁6▁de▁abril▁às▁10h00', '▁14▁de▁agosto▁de▁2030']\n",
            "ADDR: []\n",
            "============================================================\n",
            "[14] TOK: ▁Entre ▁algum ▁dia , ▁com ▁o ▁tempo ▁e ▁2 ▁de ▁março ▁de ▁20 27 , ▁passa mos ▁por ▁Centro ▁Social ▁de ▁Nossa ▁Senhor a ▁da ▁A legri a , ▁Edi fíci o ▁4 ▁e ▁Conte nça s ▁Gar e ▁( Esta ção ).\n",
            "LBL: O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS\n",
            "TIME: ['▁2▁de▁março▁de▁2027']\n",
            "ADDR: ['▁Centro▁Social▁de▁Nossa▁Senhora▁da▁Alegria', '▁Contenças▁Gare▁(Estação).']\n",
            "============================================================\n",
            "[15] TOK: ▁Sa í mos ▁de ▁Edi fíci o ▁41 ▁Depois ▁de ▁amanhã ▁às ▁18 h 30 , ▁passa mos ▁por ▁Lar ▁de ▁Id osos ▁Aman hã ▁pela ▁manhã ▁às ▁06 h 50 ▁e ▁termina mos ▁em ▁Av ▁Principal ▁Rou co ▁( X ) ▁Rua ▁4 ▁( MAR L ) ▁8 ▁de ▁mai . ▁de ▁1978 .\n",
            "LBL: O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁Depois▁de▁amanhã▁às▁18h30', '▁Amanhã▁pela▁manhã▁às▁06h50', '▁8▁de▁mai.▁de▁1978']\n",
            "ADDR: ['▁Lar▁de▁Idosos', '▁Av▁Principal▁Rouco▁(X)▁Rua▁4▁(MARL)']\n",
            "============================================================\n",
            "[16] TOK: ▁Passe i ▁eventualmente ▁no ▁Campo ▁de ▁Futebol ▁Ferreiro s ▁revi s ando ▁alguns ▁documentos ▁importantes .\n",
            "LBL: O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O\n",
            "TIME: []\n",
            "ADDR: ['▁Campo▁de▁Futebol▁Ferreiros']\n",
            "============================================================\n",
            "[17] TOK: ▁5 ▁de ▁fevereiro ▁de ▁20 31 ▁o ▁artigo ▁foi ▁red i gido , ▁mais ▁tarde ▁passou ▁por ▁revi sões ▁e ▁18 ▁de ▁janeiro ▁de ▁1994 ▁este ve ▁pronto ▁para ▁publicação .\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O\n",
            "TIME: ['▁5▁de▁fevereiro▁de▁2031', '▁18▁de▁janeiro▁de▁1994']\n",
            "ADDR: []\n",
            "============================================================\n",
            "[18] TOK: ▁30 ▁de ▁outubro ▁de ▁20 26 , ▁parti mos ▁de ▁Mur al ▁dos ▁tu bar ões ▁de ▁Mar vila , ▁fiz emos ▁uma ▁visita ▁a ▁Centro ▁Comercial ▁Pin hal ▁do ▁Mo inho ▁e ▁termina mos ▁em ▁Con jun to ▁Habita cional ▁da ▁Var ze a .\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TIME: ['▁30▁de▁outubro▁de▁2026']\n",
            "ADDR: ['▁Mural▁dos▁tubarões▁de▁Marvila', '▁Centro▁Comercial▁Pinhal▁do▁Moinho', '▁Conjunto▁Habitacional▁da▁Varzea']\n",
            "============================================================\n",
            "[19] TOK: ▁N / o ▁Fan al , ▁em ▁breve ▁descobri ▁uma ▁liv r aria ▁com ▁livros ▁antigo s .\n",
            "LBL: O O O B-ADDRESS I-ADDRESS O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: ['▁Fanal']\n",
            "============================================================\n",
            "[20] TOK: ▁O ▁grupo ▁começou ▁o ▁passeio ▁em ▁Segurança ▁Social ▁de ▁A rgan il ▁Aman hã ▁pela ▁manhã ▁às ▁08 h 15 , ▁chegou ▁a ▁Segurança ▁Social ▁de ▁Al just rel ▁Da qui ▁a ▁dois ▁dias ▁às ▁18 h 25 ▁e ▁di sper sou - se ▁Segunda — feira ▁da ▁próxima ▁semana ▁às ▁10 h 30 .\n",
            "LBL: O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁Amanhã▁pela▁manhã▁às▁08h15', '▁Daqui▁a▁dois▁dias▁às▁18h25', '▁Segunda—feira▁da▁próxima▁semana▁às▁10h30']\n",
            "ADDR: ['▁Segurança▁Social▁de▁Arganil', '▁Segurança▁Social▁de▁Aljustrel']\n",
            "============================================================\n",
            "[21] TOK: ▁Fu i ▁ao ▁3 # ▁Mei o ▁da ▁tarde , ▁15 h 00 , ▁18 ▁de ▁setembro ▁e ▁aproveite i ▁para ▁experimentar ▁prato s ▁típico s ▁antes ▁de ▁sair ▁4 ▁de ▁fevereiro ▁às ▁10 h 50 .\n",
            "LBL: O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁Meio▁da▁tarde,▁15h00,▁18▁de▁setembro', '▁4▁de▁fevereiro▁às▁10h50']\n",
            "ADDR: []\n",
            "============================================================\n",
            "[22] TOK: ▁Assist ir ▁ao ▁pô r ▁do ▁sol ▁é ▁um ▁ato ▁de ▁pura ▁contempla ção .\n",
            "LBL: O O O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[23] TOK: ▁no ▁devido ▁tempo ▁realiza mos ▁uma ▁reunião ▁importante ▁no ▁Com ando ▁da ▁Zona ▁Militar ▁da ▁Madeira ▁para ▁discutir ▁o ▁projeto .\n",
            "LBL: O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O\n",
            "TIME: []\n",
            "ADDR: ['▁Comando▁da▁Zona▁Militar▁da▁Madeira']\n",
            "============================================================\n",
            "[24] TOK: ▁Eu ▁pre fir o ▁tomar ▁café ▁1 o ▁de ▁outubro ▁às ▁09 h 25 ▁e ▁al mo çar ▁algo ▁leve ▁algum ▁dia .\n",
            "LBL: O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O\n",
            "TIME: ['▁1o▁de▁outubro▁às▁09h25']\n",
            "ADDR: []\n",
            "============================================================\n",
            "[25] TOK: ▁Durante ▁eventualmente , ▁compra mos ▁artes an ato ▁local ▁no ▁Estrada ▁Nacional ▁125 , ▁KM ▁12 , ▁Ta vira , ▁Portugal .\n",
            "LBL: O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TIME: []\n",
            "ADDR: ['▁Estrada▁Nacional▁125,▁KM▁12,▁Tavira,▁Portugal']\n",
            "============================================================\n",
            "[26] TOK: ▁Uma ▁caminha da ▁pela ▁trilha ▁revelou ▁pais agens ▁de ▁tirar ▁o ▁f ô le go .\n",
            "LBL: O O O O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[27] TOK: ▁Visit ei ▁o ▁Comissão ▁de ▁Co orden ação ▁e ▁Desenvolvimento ▁Regional ▁do ▁Al ente jo ▁Segunda ▁quarta - feira ▁de ▁outubro , ▁comecei ▁um ▁projeto ▁Dia ▁dos ▁Nam o rados , ▁14 ▁de ▁fevereiro ▁e ▁final ize i ▁o ▁dia ▁por ▁lá ▁Depois ▁de ▁amanhã ▁às ▁14 h 45 .\n",
            "LBL: O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁Segunda▁quarta-feira▁de▁outubro', '▁Dia▁dos▁Namorados,▁14▁de▁fevereiro', '▁Depois▁de▁amanhã▁às▁14h45']\n",
            "ADDR: ['▁Comissão▁de▁Coordenação▁e▁Desenvolvimento▁Regional▁do▁Alentejo']\n",
            "============================================================\n",
            "[28] TOK: ▁Entre ▁Hospital ▁e ▁Está dio ▁da ▁Luz , ▁fiz emos ▁uma ▁pausa ▁para ▁relaxar ▁antes ▁de ▁finalmente ▁chegar mos ▁a ▁Centro ▁Comercial ▁Bez er ra ▁3 ▁de ▁fevereiro ▁às ▁17 h 30 .\n",
            "LBL: O B-ADDRESS O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁3▁de▁fevereiro▁às▁17h30']\n",
            "ADDR: ['▁Hospital', '▁Estádio▁da▁Luz', '▁Centro▁Comercial▁Bezerra']\n",
            "============================================================\n",
            "[29] TOK: ▁eventualmente ▁fiz ▁uma ▁trilha ▁no ▁Edi fíci o ▁47 ▁para ▁relaxar ▁e ▁medita r .\n",
            "LBL: O O O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[30] TOK: ▁Fu i ▁ao ▁Taip as ▁( Com and ante ▁Carvalho ▁Cra to ) ▁8 o ▁dia ▁do ▁terceiro ▁mês ▁lunar ▁e ▁depois ▁aproveite i ▁para ▁passe ar ▁no ▁parque ▁próximo ▁ao ▁Espaço ▁Nature za ·\n",
            "LBL: O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TIME: ['▁8o▁dia▁do▁terceiro▁mês▁lunar']\n",
            "ADDR: ['▁Taipas▁(Comandante▁Carvalho▁Crato)', '▁Espaço▁Natureza']\n",
            "============================================================\n",
            "[31] TOK: ▁Minh a ▁caminha da ▁começou ▁no ▁Departamento ▁de ▁Investiga ção ▁e ▁A ção ▁Penal ▁( S ec ções ▁2 , ▁4 , ▁5 , ▁6 ▁e ▁7) ▁22 ▁de ▁agosto ▁de ▁2023 ▁e ▁termino u ▁no ▁34 # ▁19 ▁de ▁ago . ▁de ▁1971 .\n",
            "LBL: O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁22▁de▁agosto▁de▁2023', '▁19▁de▁ago.▁de▁1971']\n",
            "ADDR: ['▁Departamento▁de▁Investigação▁e▁Ação▁Penal▁(Secções▁2,▁4,▁5,▁6▁e▁7)']\n",
            "============================================================\n",
            "[32] TOK: ▁Come ça mos ▁no ▁Avenida ▁da ▁República ▁- ▁Cava quin has , ▁para mos ▁rapidamente ▁em ▁Rua ▁Vasco ▁da ▁Gam a , ▁No ▁23, ▁4 o ▁Anda r , ▁2 900 -4 21 ▁Se tú bal ▁e ▁por ▁fim , ▁23 o ▁dia ▁do ▁déc imo ▁primeiro ▁mês ▁lunar , ▁che gamos ▁ao ▁Av ▁Roma ▁98 .\n",
            "LBL: O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O B-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TIME: ['▁23o▁dia▁do▁décimo▁primeiro▁mês▁lunar']\n",
            "ADDR: ['▁Avenida▁da▁República▁-▁Cavaquinhas', '▁Rua▁Vasco▁da▁Gama,▁No▁23,▁4o▁Andar,▁2900-421▁Setúbal', '▁Av▁Roma▁98']\n",
            "============================================================\n",
            "[33] TOK: ▁O ▁evento ▁começou ▁em ▁19 # ▁14 ▁de ▁agosto ▁de ▁20 27 , ▁houve ▁uma ▁pausa ▁para ▁almoço ▁em ▁Estrada ▁dos ▁Pesca dores , ▁KM ▁5 , ▁No ▁13, ▁Ap t . ▁1 E , ▁29 70 - 123 ▁Se si mbra ▁Últim a ▁quinta - feira ▁de ▁maio ▁às ▁16 h 50 , ▁e ▁en cer rou - se ▁em ▁Jardim ▁Fi al ho ▁de ▁Almeida ▁Fi m ▁de ▁novembro .\n",
            "LBL: O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁14▁de▁agosto▁de▁2027', '▁Última▁quinta-feira▁de▁maio▁às▁16h50', '▁Fim▁de▁novembro']\n",
            "ADDR: ['▁Estrada▁dos▁Pescadores,▁KM▁5,▁No▁13,▁Apt.▁1E,▁2970-123▁Sesimbra', '▁Jardim▁Fialho▁de▁Almeida']\n",
            "============================================================\n",
            "[34] TOK: ▁Fiz emos ▁um ▁interval o ▁no ▁Alam eda ▁Shop ▁& ▁Spot ▁9 ▁de ▁julho ▁de ▁1987 ▁e ▁seguimos ▁para ▁o ▁Az ambu ja ▁10 ▁de ▁fevereiro ▁de ▁1986 . ?\n",
            "LBL: O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME O O O O B-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME O O\n",
            "TIME: ['▁9▁de▁julho▁de▁1987', '▁10▁de▁fevereiro▁de▁1986']\n",
            "ADDR: ['▁Alameda▁Shop▁&▁Spot', '▁Azambuja']\n",
            "============================================================\n",
            "[35] TOK: ▁Os ▁volunt ários ▁trabalhar am ▁juntos ▁para ▁ajudar ▁as ▁família s ▁da ▁região .\n",
            "LBL: O O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[36] TOK: ▁Eles ▁se ▁sentir am ▁inspirado s ▁a ▁criar ▁algo ▁completamente ▁novo .\n",
            "LBL: O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[37] TOK: ▁Sa í mos ▁de ▁Serviço s ▁Parti lhado s ▁do ▁Ministério ▁da ▁Saúde ▁logo ▁Fer i ado ▁de ▁Pá sco a ▁às ▁09 h 30 , ▁fiz emos ▁um ▁interval o ▁em ▁Fin ança s ▁- ▁Terra s ▁de ▁Bour o ▁17 ▁de ▁maio ▁de ▁1986 ▁e ▁volta mos ▁a ▁Edi fíci o ▁36 ▁Segunda ▁quinta - feira ▁de ▁outubro .\n",
            "LBL: O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁Feriado▁de▁Páscoa▁às▁09h30', '▁17▁de▁maio▁de▁1986', '▁Segunda▁quinta-feira▁de▁outubro']\n",
            "ADDR: ['▁Serviços▁Partilhados▁do▁Ministério▁da▁Saúde', '▁Finanças▁-▁Terras▁de▁Bouro']\n",
            "============================================================\n",
            "[38] TOK: ▁Durante ▁o ▁evento ▁em ▁Nova dis ▁Camar ate , ▁que ▁começou ▁Hoje ▁às ▁20 h 30 ▁em ▁ponto , ▁tivemos ▁uma ▁pausa ▁Hoje ▁de ▁madrugada ▁às ▁2 h 45 ▁antes ▁de ▁encerra r ▁em ▁Edi fíci o ▁Santiago ▁em ▁breve .\n",
            "LBL: O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O\n",
            "TIME: ['▁Hoje▁às▁20h30▁em▁ponto', '▁Hoje▁de▁madrugada▁às▁2h45']\n",
            "ADDR: ['▁Novadis▁Camarate', '▁Edifício▁Santiago']\n",
            "============================================================\n",
            "[39] TOK: ▁A ▁vista ▁de ▁3 S ▁- ▁Sol va y ▁Share d ▁Services ▁para ▁Infine on ▁Technologie s ▁Business ▁Solutions , ▁Uni p esso al ▁L da . ▁é ▁incrível , ▁especialmente ▁ao ▁chegar ▁em ▁Trav essa ▁do ▁Mar , ▁No ▁3 , ▁Lo te ▁8 , ▁8 200 -300 ▁Al bu feira , ▁Faro , ▁PT .\n",
            "LBL: O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TIME: []\n",
            "ADDR: ['▁3S▁-▁Solvay▁Shared▁Services', '▁Infineon▁Technologies▁Business▁Solutions,▁Unipessoal▁Lda.', '▁Travessa▁do▁Mar,▁No▁3,▁Lote▁8,▁8200-300▁Albufeira,▁Faro,▁PT']\n",
            "============================================================\n",
            "[40] TOK: ▁Aman hã ▁às ▁11 h 30 ▁fui ▁ao ▁Centro ▁Comercial ▁81 , ▁cum pri ▁uma ▁tarefa ▁com ▁o ▁tempo ▁e ▁final ize i ▁tudo ▁Final ▁de ▁fevereiro .\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O B-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O O O B-TIME I-TIME I-TIME O\n",
            "TIME: ['▁Amanhã▁às▁11h30', '▁Final▁de▁fevereiro']\n",
            "ADDR: ['▁Centro▁Comercial▁81']\n",
            "============================================================\n",
            "[41] TOK: ▁Conclu í ▁algumas ▁tarefas ▁pendent es ▁em ▁breve ▁e ▁fiquei ▁ali vi ado .\n",
            "LBL: O O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[42] TOK: ▁- Da qui ▁a ▁três ▁dias ▁ao ▁meio - dia ▁às ▁12 h 00 ▁aproveite i ▁para ▁organizar ▁os ▁arquivo s ▁no ▁computador .\n",
            "LBL: O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O O\n",
            "TIME: ['Daqui▁a▁três▁dias▁ao▁meio-dia▁às▁12h00']\n",
            "ADDR: []\n",
            "============================================================\n",
            "[43] TOK: ▁Che gamos ▁ao ▁Bela vista ▁Lo te ▁3 ▁Da qui ▁a ▁cinco ▁dias ▁às ▁10 h 15 , ▁al mo ça mos ▁no ▁Far robi m ▁do ▁Norte , ▁e ▁22 ▁de ▁abril ▁de ▁2006 ▁fomos ▁ao ▁Sport ing ▁Shopping ▁Center .\n",
            "LBL: O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O B-TIME I-TIME I-TIME I-TIME I-TIME O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TIME: ['▁Daqui▁a▁cinco▁dias▁às▁10h15', '▁22▁de▁abril▁de▁2006']\n",
            "ADDR: ['▁Belavista▁Lote▁3', '▁Farrobim▁do▁Norte', '▁Sporting▁Shopping▁Center']\n",
            "============================================================\n",
            "[44] TOK: ▁A ▁ biodiversi dade ▁encontra da ▁em ▁Ponte ▁do ▁Gal ante ▁( Re sta uran te ▁Pa que te ) ▁e ▁Centro ▁Escolar ▁de ▁Co vas ▁é ▁surpreende nte .\n",
            "LBL: O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O\n",
            "TIME: []\n",
            "ADDR: ['▁Ponte▁do▁Galante▁(Restaurante▁Paquete)', '▁Centro▁Escolar▁de▁Covas']\n",
            "============================================================\n",
            "[45] TOK: ▁A ▁viagem ▁começou ▁em ▁Edi fíci o ▁18 ▁e ▁se ▁este nde u ▁até ▁Edi fíci o ▁Vega , ▁explora ndo ▁o ▁melhor ▁da ▁região .\n",
            "LBL: O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: ['▁Edifício▁Vega']\n",
            "============================================================\n",
            "[46] TOK: ▁A ▁peça ▁teatral ▁recebeu ▁ótima s ▁críticas ▁por ▁sua ▁original idade .\n",
            "LBL: O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[47] TOK: ▁Durante ▁com ▁o ▁tempo , ▁explore i ▁os ▁arredor es ▁do ▁Av . ▁Dom ▁Manuel ▁I ▁105 , ▁S / N , ▁7000 - 123 ▁É vora , ▁Portugal ▁com ▁um ▁grupo ▁de ▁turistas .\n",
            "LBL: O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O\n",
            "TIME: []\n",
            "ADDR: ['▁Av.▁Dom▁Manuel▁I▁105,▁S/N,▁7000-123▁Évora,▁Portugal']\n",
            "============================================================\n",
            "[48] TOK: ▁Es cre ver ▁poema s ▁sobre ▁a ▁vida ▁cotidian a ▁pode ▁ser ▁tera pê u tico .\n",
            "LBL: O O O O O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[49] TOK: ▁Visita r ▁ex posições ▁de ▁arte ▁pode ▁ser ▁uma ▁experiência ▁enriquece dora .\n",
            "LBL: O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[50] TOK: ▁O ▁evento ▁reuni u ▁pessoas ▁com ▁os ▁mesmos ▁interesse s ▁e ▁paix ões .\n",
            "LBL: O O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[51] TOK: ▁Passe i ▁8 ▁de ▁junho ▁às ▁14 h 50 ▁no ▁Catal ão ▁organiza ndo ▁uma ▁exposição ▁de ▁fotografia s .\n",
            "LBL: O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS O O O O O O O O\n",
            "TIME: ['▁8▁de▁junho▁às▁14h50']\n",
            "ADDR: ['▁Catalão']\n",
            "============================================================\n",
            "[52] TOK: ▁A ▁con fer ência ▁começou ▁algum ▁dia ▁em ▁Silva ▁Ara ú jo , ▁continuo u ▁em ▁31 # ▁14 ▁de ▁julho ▁às ▁20 h 15 ▁e ▁foi ▁encerra da ▁oficialmente ▁Festival ▁de ▁primavera ▁às ▁19 h 55 .\n",
            "LBL: O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁14▁de▁julho▁às▁20h15', '▁Festival▁de▁primavera▁às▁19h55']\n",
            "ADDR: ['▁Silva▁Araújo']\n",
            "============================================================\n",
            "[53] TOK: ▁Sa í mos ▁de ▁casa ▁para ▁Edi fíci o ▁7 ▁Dia ▁1 o ▁de ▁março ▁ao ▁meio - dia , ▁explora mos ▁a ▁região ▁até ▁10 ▁de ▁outubro ▁às ▁11 h 10 ▁e ▁volta mos ▁ao ▁ponto ▁de ▁partida ▁Media dos ▁de ▁março ▁às ▁13 h 55 .\n",
            "LBL: O O O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁Dia▁1o▁de▁março▁ao▁meio-dia', '▁10▁de▁outubro▁às▁11h10', '▁Mediados▁de▁março▁às▁13h55']\n",
            "ADDR: []\n",
            "============================================================\n",
            "[54] TOK: ▁A ▁cer im ônia ▁contou ▁com ▁atividades ▁em ▁A gência ▁para ▁a ▁Integra ção , ▁Migra ções ▁e ▁A silo ▁( A IMA ), ▁seguida s ▁por ▁uma ▁recep ção ▁em ▁Riba s ▁Baix o ▁( Esc ), ▁encerra ndo - se ▁em ▁Edi fíci o ▁44 ▁13 ▁de ▁novembro ▁às ▁11 h 00 .\n",
            "LBL: O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁13▁de▁novembro▁às▁11h00']\n",
            "ADDR: ['▁Agência▁para▁a▁Integração,▁Migrações▁e▁Asilo▁(AIMA),', '▁Ribas▁Baixo▁(Esc),']\n",
            "============================================================\n",
            "[55] TOK: ▁Entre ▁Edi fíci o ▁Alfa ▁e ▁Cas cata s , ▁há ▁um ▁ rio ▁que ▁de se mbo ca ▁em ▁Parque ▁Ja ime ▁Filip e ▁da ▁Fon se ca .\n",
            "LBL: O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O B-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TIME: []\n",
            "ADDR: ['▁Edifício▁Alfa', '▁Cascatas', '▁Parque▁Jaime▁Filipe▁da▁Fonseca']\n",
            "============================================================\n",
            "[56] TOK: ▁A ▁peça ▁de ▁teatro ▁começar á ▁da qui ▁a ▁pouco , ▁com ▁programação ▁especial ▁este ndi da ▁logo ▁depois .\n",
            "LBL: O O O O O O O O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[57] TOK: ▁Media dos ▁de ▁setembro ▁visite i ▁o ▁AP DL ▁- ▁Administração ▁dos ▁Porto s ▁do ▁Do uro , ▁Le ix ões ▁e ▁Vi ana ▁do ▁Castel o ▁e ▁fiquei ▁encantado ▁até ▁Aman hã ▁às ▁7 h 40 .\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁Mediados▁de▁setembro', '▁Amanhã▁às▁7h40']\n",
            "ADDR: ['▁APDL▁-▁Administração▁dos▁Portos▁do▁Douro,▁Leixões▁e▁Viana▁do▁Castelo']\n",
            "============================================================\n",
            "[58] TOK: ▁Hoje ▁ao ▁meio - dia ▁e ▁qua renta ▁começa mos ▁no ▁X ▁Z é ▁Gal o , ▁seguimos ▁para ▁Avenida ▁Dom ▁Nu no ▁Ál vare s ▁Pereira ▁e ▁fecha mos ▁o ▁dia ▁em ▁Pelo u rinho ▁de ▁Alma da .\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TIME: ['▁Hoje▁ao▁meio-dia▁e▁quarenta']\n",
            "ADDR: ['▁X▁Zé▁Galo', '▁Avenida▁Dom▁Nuno▁Álvares▁Pereira', '▁Pelourinho▁de▁Almada']\n",
            "============================================================\n",
            "[59] TOK: ▁Os ▁moradores ▁de ▁Campo ▁Municipal ▁da ▁Vista ▁Alegre ▁recomenda m ▁uma ▁visita ▁a ▁Pro bran ca , ▁especialmente ▁quando ▁se ▁está ▁perto ▁de ▁O ▁balo i ço ▁do ▁Ze quin ha ▁FC .\n",
            "LBL: O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TIME: []\n",
            "ADDR: ['▁Campo▁Municipal▁da▁Vista▁Alegre', '▁Probranca', '▁O▁baloiço▁do▁Zequinha▁FC']\n",
            "============================================================\n",
            "[60] TOK: ▁! Vi site i ▁o ▁Complex o ▁De sportiv o ▁1 o ▁de ▁Maio ▁ce do ▁ou ▁tarde ▁e ▁fiquei ▁encantado ▁com ▁sua ▁história .\n",
            "LBL: O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: ['▁Complexo▁Desportivo▁1o▁de▁Maio']\n",
            "============================================================\n",
            "[61] TOK: ▁Os ▁participantes ▁do ▁curso ▁estavam ▁anima dos ▁com ▁o ▁conteúdo ▁apresentado .\n",
            "LBL: O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[62] TOK: ▁As ▁flores ▁no ▁ja rdim ▁estão ▁desa bro ch ando .\n",
            "LBL: O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[63] TOK: ▁Passe i ▁20 ▁de ▁outubro ▁de ▁20 26 ▁explora ndo ▁receita s ▁novas ▁na ▁cozinha .\n",
            "LBL: O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O\n",
            "TIME: ['▁20▁de▁outubro▁de▁2026']\n",
            "ADDR: []\n",
            "============================================================\n",
            "[64] TOK: ▁Uma ▁tarde ▁tranquila ▁foi ▁aproveita da ▁com ▁boa ▁música ▁e ▁companhia ▁agradável .\n",
            "LBL: O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[65] TOK: ▁Dia ▁15 ▁do ▁mês ▁lunar ▁7 ▁fomos ▁ao ▁Fá bric a ▁Vi são , ▁explora mos ▁os ▁arredor es ▁do ▁Av . ▁dos ▁Ali ados , ▁No ▁88 , ▁Porto , ▁4000 -06 5 , ▁PT ▁e ▁conclu í mos ▁a ▁jornada ▁Segunda - feira ▁antes ▁do ▁Carnaval ▁às ▁16 h 20 ▁com ▁uma ▁surpresa ▁especial ▁em ▁breve .\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O\n",
            "TIME: ['▁Dia▁15▁do▁mês▁lunar▁7', '▁Segunda-feira▁antes▁do▁Carnaval▁às▁16h20']\n",
            "ADDR: ['▁Fábrica▁Visão', '▁Av.▁dos▁Aliados,▁No▁88,▁Porto,▁4000-065,▁PT']\n",
            "============================================================\n",
            "[66] TOK: ▁Entre ▁com ▁o ▁tempo ▁e ▁o ▁final ▁do ▁dia , ▁visita mos ▁o ▁Estrada ▁Nova , ▁No ▁80 , ▁Se tú bal , ▁Portugal , ▁2 900 - 350 ▁e ▁tira mos ▁fotos ▁no ▁Lar go ▁São ▁Domingo s ▁9 , ▁4000 -5 45 ▁Porto .\n",
            "LBL: O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TIME: []\n",
            "ADDR: ['▁Estrada▁Nova,▁No▁80,▁Setúbal,▁Portugal,▁2900-350', '▁Largo▁São▁Domingos▁9,▁4000-545▁Porto']\n",
            "============================================================\n",
            "[67] TOK: ▁Ela ▁passou ▁horas ▁desen ha ndo ▁pais agens ▁imagin árias ▁em ▁seu ▁ca der no .\n",
            "LBL: O O O O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[68] TOK: ▁Os ▁melhores ▁museu s ▁estão ▁localizado s ▁em ▁É vo J ard ins , ▁mas ▁você ▁também ▁deve ▁explorar ▁Rua ▁da ▁Universidade ▁101 , ▁Edi fíci o ▁Norte , ▁3000 - 492 ▁Co i mbra , ▁PT ▁e ▁conhecer ▁Maria ▁Mura lha .\n",
            "LBL: O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O B-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TIME: []\n",
            "ADDR: ['▁ÉvoJardins', '▁Rua▁da▁Universidade▁101,▁Edifício▁Norte,▁3000-492▁Coimbra,▁PT', '▁Maria▁Muralha']\n",
            "============================================================\n",
            "[69] TOK: ▁? As ▁reuni ões ▁acontecer ão ▁Dia ▁dos ▁namorado s , ▁14 ▁de ▁fevereiro ▁no ▁Brit o ▁( Jun ta ▁de ▁Fre gues ia ) ▁e ▁seguir ão ▁Aman hã ▁às ▁10 h 10 ▁para ▁o ▁Centro ▁Comercial ▁Con tinen te ▁Port im ão , ▁finaliza ndo ▁Aman hã ▁de ▁manhã ▁às ▁09 h 45 .\n",
            "LBL: O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁Dia▁dos▁namorados,▁14▁de▁fevereiro', '▁Amanhã▁às▁10h10', '▁Amanhã▁de▁manhã▁às▁09h45']\n",
            "ADDR: ['▁Brito▁(Junta▁de▁Freguesia)', '▁Centro▁Comercial▁Continente▁Portimão']\n",
            "============================================================\n",
            "[70] TOK: ▁Primeiro ▁domingo ▁do ▁mês ▁de ▁maio ▁participe i ▁de ▁um ▁evento ▁cultural ▁no ▁Avenida ▁do ▁At lân tico , ▁No ▁35 , ▁C asca is , ▁Lisboa , ▁PT .\n",
            "LBL: B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TIME: ['▁Primeiro▁domingo▁do▁mês▁de▁maio']\n",
            "ADDR: ['▁Avenida▁do▁Atlântico,▁No▁35,▁Cascais,▁Lisboa,▁PT']\n",
            "============================================================\n",
            "[71] TOK: ▁O ▁trabalho ▁foi ▁realizado ▁Hoje ▁pela ▁manhã ▁às ▁06 h 45 , ▁e ▁a ▁entrega ▁final ▁ocorreu ▁perfeita mente ▁Dia ▁2 ▁de ▁março ▁às ▁10 h 25 .\n",
            "LBL: O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁Hoje▁pela▁manhã▁às▁06h45', '▁Dia▁2▁de▁março▁às▁10h25']\n",
            "ADDR: []\n",
            "============================================================\n",
            "[72] TOK: ▁Che gamos ▁ao ▁4 # ▁Fi m ▁de ▁fevereiro , ▁caminha mos ▁até ▁Espaço ▁do ▁C idad ão ▁de ▁Ponte ▁da ▁Barca ▁e ▁termina mos ▁no ▁CR OA ▁- ▁Centro ▁de ▁Reco lha ▁Oficial ▁de ▁Anima is ▁Dia ▁3 ▁de ▁novembro , ▁às ▁23 h 15 .\n",
            "LBL: O O O O O B-TIME I-TIME I-TIME I-TIME O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁Fim▁de▁fevereiro', '▁Dia▁3▁de▁novembro,▁às▁23h15']\n",
            "ADDR: ['▁Espaço▁do▁Cidadão▁de▁Ponte▁da▁Barca', '▁CROA▁-▁Centro▁de▁Recolha▁Oficial▁de▁Animais']\n",
            "============================================================\n",
            "[73] TOK: ▁No ▁Edi fíci o ▁29, ▁no ▁futuro ▁prove i ▁prato s ▁ex ó ticos ▁e ▁delicioso s .\n",
            "LBL: O O O O O O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[74] TOK: ▁A ▁caminha da ▁inicio u - se ▁pont u almente ▁24 ▁de ▁abril ▁de ▁1998 , ▁fiz emos ▁uma ▁pausa ▁para ▁descanso ▁em ▁breve ▁e ▁retorn amos ▁ao ▁ponto ▁de ▁encontro ▁6 ▁de ▁julho ▁de ▁2025 .\n",
            "LBL: O O O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁24▁de▁abril▁de▁1998', '▁6▁de▁julho▁de▁2025']\n",
            "ADDR: []\n",
            "============================================================\n",
            "[75] TOK: ▁No ▁Está dio ▁Municipal ▁de ▁Sever ▁do ▁Vou ga , ▁com ▁o ▁tempo ▁participe i ▁de ▁uma ▁cer im ônia ▁e ▁depois ▁explore i ▁o ▁Câmara ▁Mu nic pal ▁do ▁Nord este ▁- ▁Se vi ço ▁de ▁Obra s ▁e ▁Urban ismo .\n",
            "LBL: O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TIME: []\n",
            "ADDR: ['▁Estádio▁Municipal▁de▁Sever▁do▁Vouga', '▁Câmara▁Municpal▁do▁Nordeste▁-▁Seviço▁de▁Obras▁e▁Urbanismo']\n",
            "============================================================\n",
            "[76] TOK: ▁A ▁antiga ▁estrada ▁que ▁conecta ▁New comp ▁- ▁Com ponent es ▁para ▁Cal ça do ▁a ▁Parque ▁do ▁Rio ▁Ferreira ▁é ▁um ▁destino ▁turístico ▁muito ▁popula ; r ▁que ▁leva ▁a ▁Bre ia .\n",
            "LBL: O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O B-ADDRESS I-ADDRESS O\n",
            "TIME: []\n",
            "ADDR: ['▁Newcomp▁-▁Componentes▁para▁Calçado', '▁Parque▁do▁Rio▁Ferreira', '▁Breia']\n",
            "============================================================\n",
            "[77] TOK: ▁O ▁evento ▁foi ▁planeja do ▁14 ▁de ▁dezembro ▁de ▁20 28 , ▁organizado ▁4 ▁de ▁fevereiro ▁às ▁14 h 00 , ▁e ▁realizado ▁com ▁sucesso ▁7 ▁de ▁novembro ▁de ▁20 24.\n",
            "LBL: O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME\n",
            "TIME: ['▁14▁de▁dezembro▁de▁2028', '▁4▁de▁fevereiro▁às▁14h00', '▁7▁de▁novembro▁de▁2024.']\n",
            "ADDR: []\n",
            "============================================================\n",
            "[78] TOK: ▁Come ce i ▁o ▁dia ▁em ▁Complex o ▁De sportiv o ▁da ▁Co vil hã ▁9 ▁de ▁outubro ▁às ▁18 h 30 , ▁tra bal hei ▁até ▁Segunda ▁quarta - feira ▁do ▁mês , ▁às ▁7 h 50 , ▁e ▁final ize i ▁tudo ▁em ▁Parque ▁De sportiv o ▁do ▁At lân tico ▁em ▁breve .\n",
            "LBL: O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O\n",
            "TIME: ['▁9▁de▁outubro▁às▁18h30', '▁Segunda▁quarta-feira▁do▁mês,▁às▁7h50']\n",
            "ADDR: ['▁Complexo▁Desportivo▁da▁Covilhã', '▁Parque▁Desportivo▁do▁Atlântico']\n",
            "============================================================\n",
            "[79] TOK: ▁Os ▁alunos ▁saír am ▁do ▁Escola ▁Básica ▁de ▁Ros ário ▁1 ▁d . e ▁janeiro ▁de ▁20 24 , ▁explorar am ▁Escola ▁Básica ▁Maria ▁Adelaide ▁Silva ▁e ▁retorn aram ▁ao ▁Ne st ▁Collect ive ▁por ▁volta ▁de ▁Ter ceiro ▁fim ▁de ▁semana ▁de ▁setembro .\n",
            "LBL: O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁1▁d.e▁janeiro▁de▁2024', '▁Terceiro▁fim▁de▁semana▁de▁setembro']\n",
            "ADDR: ['▁Escola▁Básica▁de▁Rosário', '▁Escola▁Básica▁Maria▁Adelaide▁Silva', '▁Nest▁Collective']\n",
            "============================================================\n",
            "[80] TOK: ▁No ▁Parque ▁Bot ân ico ▁Ar but us ▁do ▁Demo , ▁3 ▁de ▁outubro ▁de ▁2025 ▁participe i ▁de ▁um ▁evento ▁de ▁música ▁ao ▁vivo .\n",
            "LBL: O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O B-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O O\n",
            "TIME: ['▁3▁de▁outubro▁de▁2025']\n",
            "ADDR: ['▁Parque▁Botânico▁Arbutus▁do▁Demo']\n",
            "============================================================\n",
            "[81] TOK: ▁Passe i ▁pelo ▁Centro ▁de ▁Bem ▁Esta r ▁Infantil ▁de ▁Monte ▁Real ▁26 o ▁dia ▁do ▁8 o ▁mês ▁lunar ▁e ▁depois ▁fui ▁dire to ▁ao ▁Lar ▁de ▁La mas ▁7 ▁de ▁setembro ▁de ▁1983 ▁às ▁19 :57 ▁para ▁resolver ▁algumas ▁coisas .\n",
            "LBL: O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O\n",
            "TIME: ['▁26o▁dia▁do▁8o▁mês▁lunar', '▁7▁de▁setembro▁de▁1983▁às▁19:57']\n",
            "ADDR: ['▁Centro▁de▁Bem▁Estar▁Infantil▁de▁Monte▁Real', '▁Lar▁de▁Lamas']\n",
            "============================================================\n",
            "[82] TOK: ▁No ▁Unidade ▁de ▁Cu idad os ▁Continua dos ▁Integra dos , ▁da qui ▁a ▁pouco , ▁ encontre i ▁um ▁amigo ▁que ▁não ▁via ▁há ▁anos .\n",
            "LBL: O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: ['▁Unidade▁de▁Cuidados▁Continuados▁Integrados']\n",
            "============================================================\n",
            "[83] TOK: ▁A ▁aula ▁será ▁ministra da ▁em ▁breve ▁e ▁encerra remos ▁as ▁atividades ▁Quart a – feira ▁da ▁segunda ▁semana ▁de ▁maio ·\n",
            "LBL: O O O O O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁Quarta–feira▁da▁segunda▁semana▁de▁maio']\n",
            "ADDR: []\n",
            "============================================================\n",
            "[84] TOK: ▁Entre ▁20 ▁de ▁set . ▁de ▁2022 ▁e ▁Meta de ▁de ▁dezembro , ▁vamos ▁explorar ▁Escola ▁Básica ▁de ▁Casal ▁de ▁Malta , ▁conhecer ▁Centro ▁Comercial ▁Verde ▁Sin tra ▁e ▁registrar ▁momentos ▁em ▁Av . ▁da ▁Constituição , ▁No ▁99 , ▁Porto , ▁PT .\n",
            "LBL: O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O B-TIME I-TIME I-TIME I-TIME O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TIME: ['▁20▁de▁set.▁de▁2022', '▁Metade▁de▁dezembro']\n",
            "ADDR: ['▁Escola▁Básica▁de▁Casal▁de▁Malta', '▁Centro▁Comercial▁Verde▁Sintra', '▁Av.▁da▁Constituição,▁No▁99,▁Porto,▁PT']\n",
            "============================================================\n",
            "[85] TOK: ▁As ▁discuss ões ▁sobre ▁ sustentabilidade ▁estão ▁cada ▁vez ▁mais ▁presentes .\n",
            "LBL: O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[86] TOK: ▁Os ▁sabor es ▁ex ó ticos ▁chamar am ▁a ▁atenção ▁dos ▁visitantes .\n",
            "LBL: O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[87] TOK: ▁Uma ▁discussão ▁aca dê mica ▁foi ▁promo vida ▁21 ▁de ▁outubro ▁às ▁23 h 30 , ▁com ▁a ▁publicação ▁dos ▁artigos ▁em ▁breve .\n",
            "LBL: O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O O O\n",
            "TIME: ['▁21▁de▁outubro▁às▁23h30']\n",
            "ADDR: []\n",
            "============================================================\n",
            "[88] TOK: ▁com ▁o ▁tempo ▁os ▁convidados ▁chegara m ▁ao ▁26 # , ▁assistir am ▁à ▁cer im ônia ▁em ▁Edi fíci o ▁31 ▁e ▁celebrar am ▁no ▁R . ▁das ▁Oliveira s , ▁Sin tra , ▁Portugal .\n",
            "LBL: O O O O O O O O O O O O O O O O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TIME: []\n",
            "ADDR: ['▁R.▁das▁Oliveiras,▁Sintra,▁Portugal']\n",
            "============================================================\n",
            "[89] TOK: ▁da qui ▁a ▁pouco ▁recebi ▁e logio s ▁por ▁um ▁trabalho ▁que ▁final ize i ▁recentemente .\n",
            "LBL: O O O O O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[90] TOK: ▁Os ▁artistas ▁locais ▁ex põe m ▁suas ▁obras ▁regular mente ▁no ▁Edi fíci o ▁Crist al ·\n",
            "LBL: O O O O O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O\n",
            "TIME: []\n",
            "ADDR: ['▁Edifício▁Cristal']\n",
            "============================================================\n",
            "[91] TOK: ▁Fu i ▁ao ▁Edi fíci o ▁14 ▁22 ▁de ▁outubro ▁de ▁2009, ▁participe i ▁do ▁workshop ▁3 ▁de ▁ago . ▁de ▁1988 , ▁e ▁sa í ▁Da qui ▁a ▁três ▁dias ▁às ▁08 h 00 ▁da ▁manhã ▁com ▁novas ▁ideias .\n",
            "LBL: O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O\n",
            "TIME: ['▁22▁de▁outubro▁de▁2009,', '▁3▁de▁ago.▁de▁1988', '▁Daqui▁a▁três▁dias▁às▁08h00▁da▁manhã']\n",
            "ADDR: []\n",
            "============================================================\n",
            "[92] TOK: ▁Durante ▁o ▁dia ▁da ▁competição , ▁os ▁participantes ▁começar am ▁os ▁prepara tivos ▁Segunda - feira , ▁9 ▁de ▁março ▁às ▁08 h 00 , ▁competir am ▁intensa mente ▁da qui ▁a ▁três ▁semanas ▁e ▁receber am ▁os ▁prêmio s ▁Pen últim a ▁terça - feira ▁de ▁maio .\n",
            "LBL: O O O O O O O O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O\n",
            "TIME: ['▁Segunda-feira,▁9▁de▁março▁às▁08h00', '▁daqui▁a▁três▁semanas', '▁Penúltima▁terça-feira▁de▁maio']\n",
            "ADDR: []\n",
            "============================================================\n",
            "[93] TOK: ▁A ▁feira ▁em ▁EN ▁2 ▁Km ▁271 ▁atra iu ▁visitantes ▁que ▁também ▁explorar am ▁Lo te ▁26 ▁e ▁Bad oca ▁Safari ▁Park ▁na ▁região .\n",
            "LBL: O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O O O O O B-ADDRESS I-ADDRESS I-ADDRESS O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS O O O\n",
            "TIME: []\n",
            "ADDR: ['▁EN▁2▁Km▁271', '▁Lote▁26', '▁Badoca▁Safari▁Park']\n",
            "============================================================\n",
            "[94] TOK: ▁A ▁aula ▁de ▁yoga ▁foi ▁relax ante ▁e ▁a ju dou ▁a ▁ali vi ar ▁o ▁est resse .\n",
            "LBL: O O O O O O O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[95] TOK: ▁O ▁filme ▁trouxe ▁uma ▁mensagem ▁poderosa ▁e ▁to cante .\n",
            "LBL: O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[96] TOK: ▁O ▁som ▁de ▁risa das ▁é ▁sempre ▁conta gi ante ▁e ▁revi gor ante .\n",
            "LBL: O O O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[97] TOK: ▁Um ▁bom ▁livro ▁pode ▁nos ▁transport ar ▁para ▁mundo s ▁completamente ▁novos .\n",
            "LBL: O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[98] TOK: ▁Che gue i ▁ao ▁37 # ▁20 ▁de ▁dezembro ▁de ▁20 24 ▁e ▁fui ▁para ▁Constant im ▁com ▁o ▁tempo ·\n",
            "LBL: O O O O O O B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O B-ADDRESS I-ADDRESS O O O O\n",
            "TIME: ['▁20▁de▁dezembro▁de▁2024']\n",
            "ADDR: ['▁Constantim']\n",
            "============================================================\n",
            "[99] TOK: ▁A ▁música ▁to cava ▁ao ▁fundo , ▁cria ndo ▁uma ▁atmosfera ▁má gica ▁na ▁sala .\n",
            "LBL: O O O O O O O O O O O O O O O O\n",
            "TIME: []\n",
            "ADDR: []\n",
            "============================================================\n",
            "[100] TOK: ▁As ▁aulas ▁começar am ▁no ▁Pa ços ▁- ▁Rua ▁das ▁Flores ▁23 o ▁dia ▁de ▁janeiro ▁no ▁ca lend ário ▁lunar , ▁e ▁todos ▁estavam ▁anima dos .\n",
            "LBL: O O O O O B-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS I-ADDRESS B-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME I-TIME O O O O O O O\n",
            "TIME: ['▁23o▁dia▁de▁janeiro▁no▁calendário▁lunar']\n",
            "ADDR: ['▁Paços▁-▁Rua▁das▁Flores']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q evaluate seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1ONk_cEsDTF",
        "outputId": "73293b2a-7cb6-400a-9046-336f0c8cbfb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoConfig, TFAutoModelForTokenClassification\n",
        "import numpy as np, tensorflow as tf, torch, os, json\n",
        "\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/checkpoint-1235\"\n",
        "SAVEDMODEL_DIR = \"/content/export_xlmr_tf/saved_model\"\n",
        "TFLITE_PATH    = \"/content/model_int8_full.tflite\"\n",
        "\n",
        "\n",
        "def load_id2label_from_config(model_dir: str):\n",
        "    # 既可以读 checkpoint-1235 下的 config.json，也可以读 SavedModel 导出目录旁边的 config.json\n",
        "    cfg_path = os.path.join(model_dir, \"config.json\")\n",
        "    with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        cfg = json.load(f)\n",
        "    id2label = {int(k): v for k, v in cfg[\"id2label\"].items()}\n",
        "    label2id = {v: int(k) for k, v in id2label.items()}\n",
        "    labels_order = [id2label[i] for i in range(len(id2label))]\n",
        "    return id2label, label2id, labels_order\n",
        "\n",
        "\n",
        "# 读取标签映射（一定用config.json）\n",
        "id2label, label2id, labels_order = load_id2label_from_config(CHECKPOINT_DIR)\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(CHECKPOINT_DIR, use_fast=True)\n",
        "\n",
        "# 1) PyTorch\n",
        "cfg = AutoConfig.from_pretrained(CHECKPOINT_DIR)\n",
        "pt_model = AutoModelForTokenClassification.from_pretrained(CHECKPOINT_DIR, config=cfg).eval()\n",
        "\n",
        "# 2) TF SavedModel\n",
        "tf_model = TFAutoModelForTokenClassification.from_pretrained(CHECKPOINT_DIR, from_pt=True)\n",
        "@tf.function\n",
        "def tf_infer(input_ids, attention_mask):\n",
        "    return tf_model(input_ids=input_ids, attention_mask=attention_mask, training=False).logits\n",
        "\n",
        "# 3) TFLite\n",
        "interpreter = tf.lite.Interpreter(model_path=TFLITE_PATH)\n",
        "interpreter.allocate_tensors()\n",
        "inps = interpreter.get_input_details(); outs = interpreter.get_output_details()\n",
        "# 映射 input_ids / attention_mask\n",
        "name2idx = {d[\"name\"]: i for i,d in enumerate(inps)}\n",
        "idx_ids = [i for i,d in enumerate(inps) if \"input_ids\" in d[\"name\"]][0]\n",
        "idx_mask = [i for i,d in enumerate(inps) if \"attention_mask\" in d[\"name\"]][0]\n",
        "FIXED_LEN = inps[idx_ids][\"shape\"][1]\n",
        "\n",
        "def run_all(text):\n",
        "    enc = tok(text, return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=FIXED_LEN)\n",
        "    # 1) PyTorch\n",
        "    with torch.no_grad():\n",
        "        ipt = {k: torch.tensor(v) for k,v in enc.items()}\n",
        "        logits_pt = pt_model(**ipt).logits[0].cpu().numpy()\n",
        "\n",
        "    # 2) TF\n",
        "    logits_tf = tf_infer(enc[\"input_ids\"], enc[\"attention_mask\"]).numpy()[0]\n",
        "\n",
        "    # 3) TFLite\n",
        "    interpreter.set_tensor(inps[idx_ids][\"index\"], enc[\"input_ids\"].astype(np.int32))\n",
        "    interpreter.set_tensor(inps[idx_mask][\"index\"], enc[\"attention_mask\"].astype(np.int32))\n",
        "    interpreter.invoke()\n",
        "    logits_tfl = interpreter.get_tensor(outs[0][\"index\"])[0]\n",
        "\n",
        "    return logits_pt, logits_tf, logits_tfl, enc\n",
        "\n",
        "def decode_logits(logits, enc):\n",
        "    # 按最后一维 argmax\n",
        "    pred_ids = logits.argmax(-1)  # [seq]\n",
        "    pred_labs = [labels_order[i] for i in pred_ids.tolist()]\n",
        "    toks = tok.convert_ids_to_tokens(enc[\"input_ids\"][0].tolist())\n",
        "    return toks, pred_labs\n",
        "\n",
        "# 跑几条检查三路是否一致\n",
        "texts = [\n",
        "    \"明天上午9:30在上海市徐汇区漕溪北路398号开会。\",\n",
        "    \"Meet me at 221B Baker Street at 7 pm tomorrow.\",\n",
        "    \"Vi mødes kl. 1979-W07-6 ved DK-6000 Kolding, Østergade 16.\"\n",
        "]\n",
        "for s in texts:\n",
        "    l_pt, l_tf, l_tfl, enc = run_all(s)\n",
        "    t1, y1 = decode_logits(l_pt, enc)\n",
        "    t2, y2 = decode_logits(l_tf, enc)\n",
        "    t3, y3 = decode_logits(l_tfl, enc)\n",
        "    print(\"\\nTEXT:\", s)\n",
        "    print(\"PT  :\", \" \".join(y1[:50]))\n",
        "    print(\"TF  :\", \" \".join(y2[:50]))\n",
        "    print(\"TFL :\", \" \".join(y3[:50]))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "9ervVksDINhN",
        "outputId": "4bdfba55-59fd-4cc3-f3f8-db87593a955f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/checkpoint-1235/config.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2441888746.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# 读取标签映射（一定用config.json）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mid2label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_id2label_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECKPOINT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECKPOINT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2441888746.py\u001b[0m in \u001b[0;36mload_id2label_from_config\u001b[0;34m(model_dir)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# 既可以读 checkpoint-1235 下的 config.json，也可以读 SavedModel 导出目录旁边的 config.json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcfg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mid2label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id2label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/checkpoint-1235/config.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M4x1sF2rIPeC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbS7kHSu2zmk"
      },
      "outputs": [],
      "source": [
        "# 训练：XLM-RoBERTa NER (DATE/ADDRESS)\n",
        "import sys, subprocess, os\n",
        "\n",
        "# 依赖\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-U\",\n",
        "                       \"transformers>=4.55.0\", \"datasets>=2.14.0\",\n",
        "                       \"evaluate>=0.4.0\", \"seqeval>=1.2.2\", \"accelerate>=0.21.0\"])\n",
        "\n",
        "import torch, numpy as np\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# 配置\n",
        "@dataclass\n",
        "class CFG:\n",
        "    data_dir: str = \"/content/data\"\n",
        "    model_name: str = \"xlm-roberta-base\"\n",
        "    output_dir: str = \"/content/outputs/xlmr-ner-date-address\"\n",
        "    max_len: int = 128\n",
        "    epochs: int = 10\n",
        "    lr: float = 5e-5\n",
        "    batch_size: int = 128\n",
        "    seed: int = 42\n",
        "    fp16: bool = True\n",
        "    grad_ckpt: bool = True\n",
        "    warmup_ratio: float = 0.1\n",
        "    label_smoothing: float = 0.05\n",
        "    patience: int = 3\n",
        "    eval_steps: int = 200\n",
        "\n",
        "CFG = CFG()\n",
        "os.makedirs(CFG.output_dir, exist_ok=True)\n",
        "os.makedirs(CFG.data_dir, exist_ok=True)\n",
        "\n",
        "# 标签\n",
        "LABEL_LIST = [\"O\", \"B-DATE\", \"I-DATE\", \"B-ADDRESS\", \"I-ADDRESS\"]\n",
        "LABEL2ID = {l: i for i, l in enumerate(LABEL_LIST)}\n",
        "ID2LABEL = {i: l for l, i in LABEL2ID.items()}\n",
        "ALLOWED = set(LABEL_LIST)\n",
        "\n",
        "# 4) demo 数据（若已放好 train/dev/test 就不会覆盖）\n",
        "train_fp = os.path.join(CFG.data_dir, \"train.conll\")\n",
        "dev_fp   = os.path.join(CFG.data_dir, \"dev.conll\")\n",
        "test_fp  = os.path.join(CFG.data_dir, \"test.conll\")\n",
        "\n",
        "def _write_demo(path: str):\n",
        "    demo = \"\"\"明天 O\n",
        "      上午 O\n",
        "      9:30 B-DATE\n",
        "      在 O\n",
        "      上海市 B-ADDRESS\n",
        "      徐汇区 I-ADDRESS\n",
        "      漕溪北路 I-ADDRESS\n",
        "      398号 I-ADDRESS\n",
        "      开会 O\n",
        "\n",
        "      Meet O\n",
        "      me O\n",
        "      at O\n",
        "      221B B-ADDRESS\n",
        "      Baker I-ADDRESS\n",
        "      Street I-ADDRESS\n",
        "      at O\n",
        "      7 B-DATE\n",
        "      pm I-DATE\n",
        "      tomorrow I-DATE\n",
        "      . O\n",
        "      \"\"\".strip()+\"\\n\"\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(demo)\n",
        "\n",
        "for p in [train_fp, dev_fp]:\n",
        "    if not os.path.isfile(p):\n",
        "        _write_demo(p)\n",
        "if not os.path.isfile(test_fp):\n",
        "    with open(test_fp, \"w\", encoding=\"utf-8\") as f_out, open(dev_fp, \"r\", encoding=\"utf-8\") as f_in:\n",
        "        f_out.write(f_in.read())\n",
        "\n",
        "# 读 CoNLL -> HF datasets\n",
        "from typing import List, Tuple\n",
        "def read_conll(path: str) -> Tuple[List[List[str]], List[List[str]]]:\n",
        "    tokens_list, tags_list, tokens, tags = [], [], [], []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if tokens:\n",
        "                    tokens_list.append(tokens); tags_list.append(tags)\n",
        "                    tokens, tags = [], []\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            tok, tag = (parts[0], parts[-1]) if len(parts) > 1 else (parts[0], \"O\")\n",
        "            if tag not in ALLOWED: tag = \"O\"\n",
        "            tokens.append(tok); tags.append(tag)\n",
        "    if tokens:\n",
        "        tokens_list.append(tokens); tags_list.append(tags)\n",
        "    return tokens_list, tags_list\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "def build_dataset_from_conll(train_path: str, dev_path: str, test_path: str) -> DatasetDict:\n",
        "    def to_records(tokens_list, tags_list):\n",
        "        for toks, tags in zip(tokens_list, tags_list):\n",
        "            yield {\"tokens\": toks, \"ner_tags\": tags}\n",
        "    tr_tokens, tr_tags = read_conll(train_fp)\n",
        "    dv_tokens, dv_tags = read_conll(dev_fp)\n",
        "    te_tokens, te_tags = read_conll(test_fp)\n",
        "    return DatasetDict(\n",
        "        train=Dataset.from_list(list(to_records(tr_tokens, tr_tags))),\n",
        "        validation=Dataset.from_list(list(to_records(dv_tokens, dv_tags))),\n",
        "        test=Dataset.from_list(list(to_records(te_tokens, te_tags))),\n",
        "    )\n",
        "ds = build_dataset_from_conll(train_fp, dev_fp, test_fp)\n",
        "\n",
        "# 分词与标签对齐\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
        "\n",
        "def tokenize_and_align_labels(examples, max_length=CFG.max_len):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"tokens\"], is_split_into_words=True,\n",
        "        truncation=True, padding=False, max_length=max_length,\n",
        "    )\n",
        "    aligned_labels = []\n",
        "    for i, labels in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized.word_ids(i)\n",
        "        prev_word_id = None\n",
        "        label_ids = []\n",
        "        for wid in word_ids:\n",
        "            if wid is None:\n",
        "                label_ids.append(-100)\n",
        "            elif wid != prev_word_id:\n",
        "                label_ids.append(LABEL2ID[labels[wid]])\n",
        "            else:\n",
        "                lab = labels[wid]\n",
        "                if lab.startswith(\"B-\"): lab = \"I-\" + lab[2:]\n",
        "                label_ids.append(LABEL2ID[lab])\n",
        "            prev_word_id = wid\n",
        "        aligned_labels.append(label_ids)\n",
        "    tokenized[\"labels\"] = aligned_labels\n",
        "    return tokenized\n",
        "\n",
        "ds_tok = ds.map(tokenize_and_align_labels, batched=True, desc=\"Tokenizing & aligning labels\")\n",
        "\n",
        "#  模型 & 训练器\n",
        "from transformers import (AutoConfig, AutoModelForTokenClassification,\n",
        "                          DataCollatorForTokenClassification, Trainer, TrainingArguments,\n",
        "                          EarlyStoppingCallback, set_seed)\n",
        "import evaluate\n",
        "set_seed(CFG.seed)\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    CFG.model_name, num_labels=len(LABEL_LIST),\n",
        "    id2label=ID2LABEL, label2id=LABEL2ID,\n",
        ")\n",
        "model = AutoModelForTokenClassification.from_pretrained(CFG.model_name, config=config)\n",
        "if CFG.grad_ckpt:\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "seqeval = evaluate.load(\"seqeval\")\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=-1)\n",
        "    true_predictions, true_labels = [], []\n",
        "    for pred, lab in zip(predictions, labels):\n",
        "        cur_pred, cur_lab = [], []\n",
        "        for p_i, l_i in zip(pred, lab):\n",
        "            if l_i == -100: continue\n",
        "            cur_pred.append(ID2LABEL[p_i]); cur_lab.append(ID2LABEL[l_i])\n",
        "        true_predictions.append(cur_pred); true_labels.append(cur_lab)\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=CFG.output_dir,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=CFG.eval_steps,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True, metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=CFG.eval_steps,\n",
        "\n",
        "    learning_rate=CFG.lr, per_device_train_batch_size=CFG.batch_size,\n",
        "    per_device_eval_batch_size=CFG.batch_size, num_train_epochs=CFG.epochs,\n",
        "    warmup_ratio=CFG.warmup_ratio, weight_decay=0.01,\n",
        "    fp16=CFG.fp16 and torch.cuda.is_available(),\n",
        "    logging_steps=50, report_to=\"none\",\n",
        "    label_smoothing_factor=CFG.\n",
        "    label_smoothing,\n",
        "    gradient_accumulation_steps=1,\n",
        "    dataloader_num_workers=2,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=ds_tok[\"train\"],\n",
        "    eval_dataset=ds_tok[\"validation\"],\n",
        "    tokenizer=tokenizer, data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=CFG.patience)],\n",
        ")\n",
        "\n",
        "\n",
        "# 训练 / 评测 / 保存\n",
        "trainer.train()\n",
        "print(\"== Dev metrics ==\", trainer.evaluate(ds_tok[\"validation\"]))\n",
        "if \"test\" in ds_tok and len(ds_tok[\"test\"]) > 0:\n",
        "    print(\"== Test metrics ==\", trainer.evaluate(ds_tok[\"test\"]))\n",
        "trainer.save_model(CFG.output_dir); tokenizer.save_pretrained(CFG.output_dir)\n",
        "print(f\"模型已保存到：{CFG.output_dir}\")\n",
        "\n",
        "# 推理\n",
        "def predict_spans(text: str, max_length: int = CFG.max_len):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        enc = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "        if torch.cuda.is_available():\n",
        "            model.cuda(); enc = {k: v.cuda() for k, v in enc.items()}\n",
        "        logits = model(**enc).logits[0].cpu().numpy()\n",
        "    pred_ids = logits.argmax(-1)\n",
        "    pred_labels = [ID2LABEL[i] for i in pred_ids]\n",
        "    toks = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"][0].cpu().tolist())\n",
        "    spans, cur_type, cur_text = [], None, \"\"\n",
        "    for tok, lab in zip(toks, pred_labels):\n",
        "        if tok in (tokenizer.cls_token, tokenizer.sep_token, \"<s>\", \"</s>\", \"<pad>\"): continue\n",
        "        surface = tok.replace(\"▁\", \" \").strip()\n",
        "        if lab.startswith(\"B-\"):\n",
        "            if cur_type: spans.append((cur_type, cur_text.strip()))\n",
        "            cur_type, cur_text = lab[2:], surface\n",
        "        elif lab.startswith(\"I-\") and cur_type == lab[2:]:\n",
        "            cur_text += surface\n",
        "        else:\n",
        "            if cur_type: spans.append((cur_type, cur_text.strip()))\n",
        "            cur_type, cur_text = None, \"\"\n",
        "    if cur_type: spans.append((cur_type, cur_text.strip()))\n",
        "    return spans\n",
        "\n",
        "print(\"Demo spans:\", predict_spans(\"明天上午9:30在上海市徐汇区漕溪北路398号开会。Meet me at 221B Baker Street at 7 pm tomorrow.\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 推理：读取最新的 checkpoint (ner_xlmr_out 根目录) =====\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import numpy as np\n",
        "\n",
        "# 路径配置\n",
        "MODEL_DIR = \"/content/ner_xlmr_out\"\n",
        "DATA_PATH = \"/content/train_all.conll\"\n",
        "\n",
        "# 标签映射（需和训练时一致）\n",
        "LABELS = [\"O\", \"B-TIME\", \"I-TIME\", \"B-ADDRESS\", \"I-ADDRESS\"]\n",
        "label2id = {l:i for i,l in enumerate(LABELS)}\n",
        "id2label = {i:l for l,i in label2id.items()}\n",
        "\n",
        "# 加载模型和分词器（最新的）\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\n",
        "model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR)\n",
        "\n",
        "#  读取验证集\n",
        "def read_conll(path: str):\n",
        "    sents_tok, sents_lab = [], []\n",
        "    toks, labs = [], []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line=line.strip()\n",
        "            if not line:\n",
        "                if toks:\n",
        "                    sents_tok.append(toks); sents_lab.append(labs)\n",
        "                    toks, labs = [], []\n",
        "                continue\n",
        "            if \" \" in line:\n",
        "                t, y = line.split(\" \", 1)\n",
        "            else:\n",
        "                t, y = line, \"O\"\n",
        "            if y not in label2id: y = \"O\"\n",
        "            toks.append(t); labs.append(y)\n",
        "    if toks:\n",
        "        sents_tok.append(toks); sents_lab.append(labs)\n",
        "    return sents_tok, sents_lab\n",
        "\n",
        "tokens_all, tags_all = read_conll(DATA_PATH)\n",
        "\n",
        "# 随机取 10 条验证样例\n",
        "import random\n",
        "random.seed(42)\n",
        "sample_idx = random.sample(range(len(tokens_all)), 10)\n",
        "\n",
        "#  推理\n",
        "model.eval()\n",
        "for idx in sample_idx:\n",
        "    toks = tokens_all[idx]\n",
        "    labs = tags_all[idx]\n",
        "\n",
        "    enc = tokenizer(toks, is_split_into_words=True, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**enc).logits\n",
        "    preds = logits.argmax(-1).squeeze().tolist()\n",
        "\n",
        "    # 对齐 labels（忽略 special tokens）\n",
        "    word_ids = enc.word_ids()\n",
        "    pred_labels = []\n",
        "    for p, wid in zip(preds, word_ids):\n",
        "        if wid is None: continue\n",
        "        pred_labels.append(id2label[p])\n",
        "\n",
        "    print(\"=\"*40)\n",
        "    print(\"原始 Tokens:\", \" \".join(toks))\n",
        "    print(\"真实 Labels:\", \" \".join(labs))\n",
        "    print(\"预测 Labels:\", \" \".join(pred_labels))\n"
      ],
      "metadata": {
        "id": "065eYl7468xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jjuH7PdV6G35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "templates = [\n",
        "    \"请在{time}到{address}开会。\",\n",
        "    \"We will meet at {address} on {time}.\",\n",
        "]\n",
        "\n",
        "times = [\"明天上午9:30\", \"2025年8月24日15:00\", \"7 pm tomorrow\"]\n",
        "addresses = [\"上海市-徐汇区-漕溪北路-398号\", \"221B Baker Street\", \"北京市东城区景山前街4号\"]\n",
        "\n",
        "def make_sample(template, time, address):\n",
        "    sent = template.format(time=time, address=address)\n",
        "\n",
        "    # span 边界\n",
        "    time_start = sent.index(time)\n",
        "    time_end   = time_start + len(time)\n",
        "    addr_start = sent.index(address)\n",
        "    addr_end   = addr_start + len(address)\n",
        "\n",
        "    # 分词 + offset\n",
        "    enc = tokenizer(sent, return_offsets_mapping=True, add_special_tokens=True)\n",
        "\n",
        "    tokens = enc.tokens()\n",
        "    print(tokens)\n",
        "    offsets = enc[\"offset_mapping\"]\n",
        "    # 兼容单条/批量两种返回形状\n",
        "    if isinstance(offsets[0], tuple):      # 单条：[(s,e), ...]\n",
        "        offsets_seq = offsets\n",
        "    else:                                  # 批量：[[(s,e), ...]]\n",
        "      offsets_seq = offsets[0]\n",
        "\n",
        "    def in_span(s, e, a, b):\n",
        "      # 与 [a,b) 是否有交集（更稳，避免严格包含带来的漏标）\n",
        "      return max(0, min(e, b) - max(s, a)) > 0\n",
        "\n",
        "    labels = []\n",
        "    for tok, (s, e) in zip(tokens, offsets_seq):\n",
        "      if s == e:                # special tokens（CLS/SEP等）offset 常为 (0,0)\n",
        "          labels.append(-100)\n",
        "          continue\n",
        "\n",
        "      if in_span(s, e, time_start, time_end):\n",
        "          # 只要这个 token 覆盖了 time 的起点，就打 B，否则 I\n",
        "          prefix = \"B-\" if (s <= time_start < e) else \"I-\"\n",
        "          labels.append(prefix + \"DATE\")\n",
        "      elif in_span(s, e, addr_start, addr_end):\n",
        "          prefix = \"B-\" if (s <= addr_start < e) else \"I-\"\n",
        "          labels.append(prefix + \"ADDRESS\")\n",
        "      else:\n",
        "          labels.append(\"O\")\n",
        "\n",
        "    return sent, labels\n",
        "\n",
        "print(make_sample(templates[0], times[1], addresses[0]))\n",
        "\n"
      ],
      "metadata": {
        "id": "ABM8Ypct6HfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **文件操作**"
      ],
      "metadata": {
        "id": "JgfKg8IleeAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 清空文件夹(**左侧文件栏中非drive文件夹里的文件删除后无法找回，请谨慎删除！**)"
      ],
      "metadata": {
        "id": "pBZpVrIfn1iF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "dirs_to_clear = [\"/content/drive/MyDrive/NER/current_best\"] # 想清空的文件夹路径\n",
        "\n",
        "for dir_path in dirs_to_clear:\n",
        "    if os.path.exists(dir_path):\n",
        "        shutil.rmtree(dir_path)\n",
        "        print(f\"Cleared directory: {dir_path}\")\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "    print(f\"Recreated directory: {dir_path}\")"
      ],
      "metadata": {
        "id": "jIsw4FFpf1h4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf50331f-8633-4c20-8b2c-be828a8ce4de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleared directory: /content/drive/MyDrive/NER/current_best\n",
            "Recreated directory: /content/drive/MyDrive/NER/current_best\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 将指定文件**移动**（非复制）到谷歌云盘中"
      ],
      "metadata": {
        "id": "6ap8jejEYcN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "source_dirs = [\"/content/ner_xlmr_out/training_args.bin\"] # 想移动的文件原始路径\n",
        "destination_dir = \"/content/drive/MyDrive/NER/CheckPoints/bestCheck\"  # 目标路径（谷歌云盘）\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "for src_dir in source_dirs:\n",
        "    if os.path.exists(src_dir):\n",
        "        try:\n",
        "            shutil.move(src_dir, destination_dir)\n",
        "            print(f\"Moved '{src_dir}' to '{destination_dir}'\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Source directory '{src_dir}' not found.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error moving '{src_dir}': {e}\")\n",
        "    else:\n",
        "        print(f\"Source directory '{src_dir}' does not exist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-0QTmZAnKSr",
        "outputId": "e6359e4c-491e-4960-d388-a8f1cca2d951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moved '/content/ner_xlmr_out/training_args.bin' to '/content/drive/MyDrive/NER/CheckPoints/bestCheck'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 文件改名"
      ],
      "metadata": {
        "id": "KyBR60XxLOcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 文件夹路径\n",
        "folder_path = r\"/content/drive/MyDrive/NER/real_address\"   # 修改为你的文件夹路径\n",
        "\n",
        "# 缩写和语言名的映射关系\n",
        "LANG_MAP = {\n",
        "    \"zh\": \"中文\",\n",
        "    \"en\": \"英语\",\n",
        "    \"da\": \"丹麦语\",\n",
        "    \"ru\": \"俄语\",\n",
        "    \"tr\": \"土耳其语\",\n",
        "    \"de\": \"德语\",\n",
        "    \"it\": \"意大利语\",\n",
        "    \"ja\": \"日语\",\n",
        "    \"fr\": \"法语\",\n",
        "    \"sv\": \"瑞典语\",\n",
        "    \"nl\": \"荷兰语\",\n",
        "    \"pt\": \"葡萄牙语\",\n",
        "    \"es\": \"西班牙语\",\n",
        "    \"ko\": \"韩语\",\n",
        "}\n",
        "\n",
        "def rename_files(folder):\n",
        "    for filename in os.listdir(folder):\n",
        "        old_path = os.path.join(folder, filename)\n",
        "\n",
        "        # 跳过文件夹\n",
        "        if not os.path.isfile(old_path):\n",
        "            continue\n",
        "\n",
        "        # 遍历缩写映射\n",
        "        for abbr, lang in LANG_MAP.items():\n",
        "            if filename.startswith(abbr):  # 文件名前缀是缩写\n",
        "                new_name = filename.replace(abbr, lang, 1)\n",
        "                new_path = os.path.join(folder, new_name)\n",
        "                os.rename(old_path, new_path)\n",
        "                print(f\"重命名: {filename} -> {new_name}\")\n",
        "                break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rename_files(folder_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFfAwqnuLN-f",
        "outputId": "a7afa77b-b7b9-48c1-8808-4e00a36911e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "重命名: en.txt -> 英语.txt\n",
            "重命名: da.txt -> 丹麦语.txt\n",
            "重命名: ru.txt -> 俄语.txt\n",
            "重命名: tr.txt -> 土耳其语.txt\n",
            "重命名: de.txt -> 德语.txt\n",
            "重命名: it.txt -> 意大利语.txt\n",
            "重命名: ja.txt -> 日语.txt\n",
            "重命名: fr.txt -> 法语.txt\n",
            "重命名: sv.txt -> 瑞典语.txt\n",
            "重命名: nl.txt -> 荷兰语.txt\n",
            "重命名: pt.txt -> 葡萄牙语.txt\n",
            "重命名: es.txt -> 西班牙语.txt\n",
            "重命名: ko.txt -> 韩语.txt\n",
            "重命名: zh.txt -> 中文.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 简体转繁体"
      ],
      "metadata": {
        "id": "paRb5CYjNLII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencc-python-reimplemented"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBlTClAyMRpH",
        "outputId": "94f03085-7a88-4cd8-eab0-e2179881c9c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencc-python-reimplemented\n",
            "  Downloading opencc_python_reimplemented-0.1.7-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Downloading opencc_python_reimplemented-0.1.7-py2.py3-none-any.whl (481 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/481.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.7/481.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.8/481.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencc-python-reimplemented\n",
            "Successfully installed opencc-python-reimplemented-0.1.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 安装依赖：\n",
        "# pip install opencc-python-reimplemented\n",
        "\n",
        "import opencc\n",
        "\n",
        "def convert_simplified_to_traditional(input_file, output_file):\n",
        "    # 创建转换器（简体到繁体）\n",
        "    converter = opencc.OpenCC('s2t')\n",
        "\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # 转换文本\n",
        "    converted_text = converter.convert(text)\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(converted_text)\n",
        "\n",
        "    print(f\"转换完成：{input_file} -> {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 修改为你的文件路径\n",
        "    input_path = \"/content/drive/MyDrive/NER/real_time/简体中文.txt\"\n",
        "    output_path = \"/content/drive/MyDrive/NER/real_time/繁体中文.txt\"\n",
        "    convert_simplified_to_traditional(input_path, output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42B2iXviMQCB",
        "outputId": "3bf4edc6-3b77-48b3-81c8-f5979186c39e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "转换完成：/content/drive/MyDrive/NER/negatives/zh/normal.txt -> /content/drive/MyDrive/NER/negatives/tw/normal.txt\n"
          ]
        }
      ]
    }
  ]
}